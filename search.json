[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Allan Farrell",
    "section": "",
    "text": "Whenever I run into a problem I do what everyone does: I google it. Frequently some random blog comes up detailing everything I need to know about solving the problem. So, I figured, if I was already doing the work for my own interest, might as well publish it? Be the blog I want to see in the world, or something like that.\nThat’s where these notebooks come from, they are mostly worked examples of things I’ve either had to do through my work – I’m a chemical engineer by day – or something I stumbled across and just thought was interesting. Hopefully you find them useful and, if you find any mistakes, let me know!"
  },
  {
    "objectID": "about.html#showing-my-work",
    "href": "about.html#showing-my-work",
    "title": "Allan Farrell",
    "section": "",
    "text": "Whenever I run into a problem I do what everyone does: I google it. Frequently some random blog comes up detailing everything I need to know about solving the problem. So, I figured, if I was already doing the work for my own interest, might as well publish it? Be the blog I want to see in the world, or something like that.\nThat’s where these notebooks come from, they are mostly worked examples of things I’ve either had to do through my work – I’m a chemical engineer by day – or something I stumbled across and just thought was interesting. Hopefully you find them useful and, if you find any mistakes, let me know!"
  },
  {
    "objectID": "about.html#caveat-emptor",
    "href": "about.html#caveat-emptor",
    "title": "Allan Farrell",
    "section": "Caveat Emptor",
    "text": "Caveat Emptor\nWhile I may be an engineer, I am not your engineer. You should not blindly follow whatever you read on anyone’s blog, and instead see this as a starting point for your own work and research. I try to provide references, so you can check my work and my assumptions, and these can be very fruitful starting points to read more of the context around what I’m doing. It’s entirely possible that there is a better model out there, in the same reference that I use, for your particular problem! Also I often simplify the example I’m doing to zoom in on details I’m interested in while glossing over pieces that are important to a more fulsome engineering analysis.\nAnd let’s not forget that I can simply be wrong!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Chemical Engineer's Notebook",
    "section": "",
    "text": "The Ooms Plume Model\n\n\n\n\n\nAn integral plume model for buoyant plumes.\n\n\n\n\n\nJun 15, 2025\n\n\nAllan Farrell\n\n38 min\n\n\n\n\n\n\n\n\n\n\n\nLogging data from an Atmotube PRO over Bluetooth\n\n\n\n\n\nHaving fun with data logging.\n\n\n\n\n\nMay 19, 2025\n\n\nAllan Farrell\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\nMapping Pollen Dispersion\n\n\n\n\n\nCalculating how far the wind blows.\n\n\n\n\n\nMay 10, 2025\n\n\nAllan Farrell\n\n26 min\n\n\n\n\n\n\n\n\n\n\n\nVessel Blowdown - Real Gases\n\n\n\n\n\nModelling vessel blowdowns using equations of state.\n\n\n\n\n\nMar 19, 2025\n\n\nAllan Farrell\n\n29 min\n\n\n\n\n\n\n\n\n\n\n\nVessel Blowdown - Ideal Gases\n\n\n\n\n\nEvaluating approaches to ideal gas blowdowns.\n\n\n\n\n\nJan 24, 2025\n\n\nAllan Farrell\n\n25 min\n\n\n\n\n\n\n\n\n\n\n\nRelief Valve Sizing with Real Gases\n\n\n\n\n\nCompressible orifice flow calculations using equations of state.\n\n\n\n\n\nOct 28, 2024\n\n\nAllan Farrell\n\n20 min\n\n\n\n\n\n\n\n\n\n\n\nModelling Hydrogen Releases Using HyRAM+\n\n\n\n\n\nHydrogen plume modelling and indoor accumulation.\n\n\n\n\n\nSep 22, 2024\n\n\nAllan Farrell\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\nPlastics Recycling and Microplastics\n\n\n\n\n\nIs plastic recycling a huge source of microplastics?\n\n\n\n\n\nJul 14, 2024\n\n\nAllan Farrell\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\nEngineering a Cup of Coffee Part Two: Espresso\n\n\n\n\n\nModelling espresso bed extraction.\n\n\n\n\n\nMar 23, 2024\n\n\nAllan Farrell\n\n44 min\n\n\n\n\n\n\n\n\n\n\n\nEstimating the impact of fugitive emissions\n\n\n\n\n\nEvaluating the zero emissions fuel.\n\n\n\n\n\nJan 3, 2024\n\n\nAllan Farrell\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\nImpossible bowling\n\n\n\n\n\nLooking for impossible bowling games.\n\n\n\n\n\nNov 26, 2023\n\n\nAllan Farrell\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\nMessing around with model parameters\n\n\n\n\n\nThe importance of choosing the right references.\n\n\n\n\n\nOct 30, 2023\n\n\nAllan Farrell\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nEngineering a Cup of Coffee\n\n\n\n\n\nBetter coffee through chemical engineering.\n\n\n\n\n\nSep 15, 2023\n\n\nAllan Farrell\n\n24 min\n\n\n\n\n\n\n\n\n\n\n\nMonitoring smoke infiltration\n\n\n\n\n\nBetter indoor air quality through data.\n\n\n\n\n\nMay 22, 2023\n\n\nAllan Farrell\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\nTaking a second look at the Britter-McQuaid model\n\n\n\n\n\nRe-evaluating plume extents and determining the explosive mass\n\n\n\n\n\nMar 12, 2023\n\n\nAllan Farrell\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\nIntegrating a Gaussian puff - mistakes were made\n\n\n\n\n\nSuccessive approximations to … an integrated gaussian puff model.\n\n\n\n\n\nJan 15, 2023\n\n\nAllan Farrell\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\nDynamic Mode Decomposition\n\n\n\n\n\nDynamic mode decomposition of fluid flow problems.\n\n\n\n\n\nDec 18, 2022\n\n\nAllan Farrell\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\nHydrogen Blending\n\n\n\n\n\nBlending hydrogen into natural gas.\n\n\n\n\n\nNov 10, 2022\n\n\nAllan Farrell\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\nAdiabatic Compressible Flow in a Pipe\n\n\n\n\n\nEvaluating different models of adiabatic pipe flow.\n\n\n\n\n\nSep 23, 2022\n\n\nAllan Farrell\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\nBetween a puff and a plume\n\n\n\n\n\nAn integrated Gaussian puff model\n\n\n\n\n\nJun 10, 2022\n\n\nAllan Farrell\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\nMore on Turbulent Jets\n\n\n\n\n\nCalculating concentrations, temperatures, and flow rates.\n\n\n\n\n\nMay 8, 2022\n\n\nAllan Farrell\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\nTurbulent Jets\n\n\n\n\n\nNotes on turbulent jets and velocity profiles.\n\n\n\n\n\nApr 8, 2022\n\n\nAllan Farrell\n\n24 min\n\n\n\n\n\n\n\n\n\n\n\nThe 2021 Canadian Federal Election\n\n\n\n\n\nAn analysis of how exceptionally little changed.\n\n\n\n\n\nSep 22, 2021\n\n\nAllan Farrell\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nSmoke Days\n\n\n\n\n\nFrequency of forest fire smoke events.\n\n\n\n\n\nJul 18, 2021\n\n\nAllan Farrell\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nBuilding Infiltration Example – Chlorine Release\n\n\n\n\n\nSingle zone building infiltration model with an instantaneous release\n\n\n\n\n\nJun 19, 2021\n\n\nAllan Farrell\n\n21 min\n\n\n\n\n\n\n\n\n\n\n\nBuilding Infiltration Example\n\n\n\n\n\nSingle zone building infiltration of forest fire smoke.\n\n\n\n\n\nMay 22, 2021\n\n\nAllan Farrell\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\nTurbulent Jet Example - Acetylene Leak\n\n\n\n\n\nEstimating the explosive mass.\n\n\n\n\n\nApr 10, 2021\n\n\nAllan Farrell\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\nVCE Example - Butane Vapour Cloud\n\n\n\n\n\nUsing the Baker-Strehlow-Tang model for a vapour cloud explosion.\n\n\n\n\n\nJan 9, 2021\n\n\nAllan Farrell\n\n22 min\n\n\n\n\n\n\n\n\n\n\n\nWorst Case Meterological Conditions\n\n\n\n\n\nThe worst case weather conditions for air dispersion modeling.\n\n\n\n\n\nDec 12, 2020\n\n\nAllan Farrell\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\nAir Dispersion Example - Gaussian Dispersion Model of Stack Emissions\n\n\n\n\n\nEstimating the airborne quantity.\n\n\n\n\n\nDec 5, 2020\n\n\nAllan Farrell\n\n18 min\n\n\n\n\n\n\n\n\n\n\n\nCompressible Flow Example - Sizing a Goose Neck Vent\n\n\n\n\n\nCalculating the minimum diameter in incompressible, isothermal, and adiabatic flow situations.\n\n\n\n\n\nNov 28, 2020\n\n\nAllan Farrell\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\nChemical Release Screening Example - Butane leak\n\n\n\n\n\nEstimating the airborne quantity.\n\n\n\n\n\nNov 20, 2020\n\n\nAllan Farrell\n\n20 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/adiabatic-compressible-flow/index.html",
    "href": "posts/adiabatic-compressible-flow/index.html",
    "title": "Adiabatic Compressible Flow in a Pipe",
    "section": "",
    "text": "I was looking through some books and it struck me how strangely inconsistent many standard references are when it comes to adiabatic compressible pipe flow. There are standard methods for incompressible flow and isothermal compressible flow of an ideal gas, but when it comes to adiabatic pipe flow the guidance is very scattershot.\nAs a brief review of some common references: Crane’s1 gives a graphical method for adiabatic flow, which is the easiest to use with a pencil and paper, but doesn’t give a lot of details on how that model was developed. Albright’s2 recommends assuming flow is locally isentropic and gives a model of isentropic flow – that is flow which is both adiabatic and reversible – but with frictional losses also included, which allows for direct calculation if one assumes the friction factor is constant (with respect to the Reynold’s number). Perry’s3 gives the adiabatic irreversible flow model (i.e. Fanno flow), though with only a sketch of how to perform the iterative solution. Hall4 gives the Fanno flow model and, helpfully, a procedure for how to actually do the calculations and example VBA code. Ludwig’s5 gives both the isentropic and Fanno flow models but in a very confused manner: the section labeled “Adiabatic Flow” gives a model of isentropic flow (albeit with a typo in the equation) and suggests that all adiabatic flow is isentropic (which is false) and much later in a section labeled “Other Simplified Compressible Flow Methods” gives the Fanno flow model, though it doesn’t explain what it is, misattributes the derivation, and gives no clues on how to use it. Probably the best reference to sort all of this out is Coulson and Richardson’s6 as it provides easy to follow derivations of both the reversible and irreversible adiabatic flow models (the isentropic and Fanno flow models) and highlights their differences.\nAnother part of this confusion is differences in how the problem is being approached – or what problem, exactly, one is trying to solve. Typically the isothermal and isentropic flow models are presented as ways to solve for the flowrate given the pressure drop between two points, whereas the Fanno flow model is often given in terms of the Mach number and one is solving for the pressure drop. If you have the Mach number, rather obviously, you already know the flow, and it is often left as an exercise for the reader to figure out how to use the Fanno flow model to solve for flow.\nGiven all of that, I thought it may be worthwhile to unpack these various approaches to adiabatic flow, and see how they perform relative to one another."
  },
  {
    "objectID": "posts/adiabatic-compressible-flow/index.html#motivating-example",
    "href": "posts/adiabatic-compressible-flow/index.html#motivating-example",
    "title": "Adiabatic Compressible Flow in a Pipe",
    "section": "Motivating Example",
    "text": "Motivating Example\nTo give us something to work towards, suppose we wish calculate the flowrate of air in a horizontal section of piping – a 20m length of 2in schedule 40 steel pipe. In this case the pipe starts a 100kPag vessel which is at ambient temperature and exits into the air at ambient pressure.\n\n\n\n\n\n\nFigure 1: A sketch of the example system, a long straight section of pipe through which air is flowing.\n\n\n\n\n# Pipe dimensions\nL = 20     # m\nD = 0.0525 # m\nϵ = 0.0457*1e-3 # m\n\nA = 0.25*π*D^2\n\nFor “ambient” conditions I am assuming standard conditions: 1 atmosphere and 15°C\n\nP₂ = 101325 #Pa\nP₁ = P₂ + 100e3\nT₁ = 288.15 #K\n\n\nKey Assumptions\n\nAir is an ideal gas, Z=1\nThe ratio of heat capacities, γ is constant\nHeat loss is negligible, Δq=0\nFlow is steady state, \\(\\dot{m}_{in} = \\dot{m}_{out}\\)\nFlow is turbulent, α=1\nFlow is horizontal, Δz=0\nFriction factor is constant along the length\n\n\n# Universal gas constant \n# to more digits than are at all necessary\nR = 8.31446261815324 # Pa⋅m³/mol/K\n\n# Some useful physical properties of air\nMw = 0.02896 # kg/mol\nγ = 1.4      # Cp/Cv, ideal gas\n\n# density of air, ideal gas law\nρ(P,T) = (P*Mw)/(R*T); # kg/m³\n\n# viscosity of air, from Perry's\nμ(T) = (1.425e-6*T^0.5039)/(1+108.3/T); # Pa⋅s\n\nThe mass velocity, G = ρu, in a pipe with constant cross-sectional area at steady state is constant7, and the Reynold’s number can be written in terms of G as:\n7 This is a consequence of the steady state assumption, \\[\\dot{m}_{in} = G_{in} A = G_{out} A = \\dot{m}_{out}\\]\\[ \\mathrm{Re} = { {G D} \\over \\mu } \\]\nWhere only the viscosity is a function of temperature, and for most gases only weakly so.\n\n# Reynold's number\nRe(G,T) = G*D/μ(T);\n\nThe Darcy friction factor, f, is a function of the Reynolds number and, for ease of calculation, I am assuming the Churchill correlation applies,8 and that it can be taken as a constant at the average temperature (the arithmetic average of T1 and T2)\n8 Tilton, “Fluid and Particle Dynamics,” 6–11.\nfunction churchill(Re; κ=ϵ/D)\n    A = (2.457 * log(1/((7/Re)^0.9 + 0.27*κ)))^16\n    B = (37530/Re)^16\n    return 8*((8/Re)^12 + 1/(A+B)^(3/2))^(1/12)\nend;\n\nK_entrance = 0.5\nK_exit = 1.0\n\nKf(Re) = K_entrance + churchill(Re)*L/D + K_exit;\n\nFor a large Reynolds number approximation I am using the Nikuradse rough pipe law.9\n9 Crane, “TP410M,” 1–10.\nfₙ = (2*log10(3.7*D/ϵ))^-2\n\nKf() = K_entrance + fₙ*L/D + K_exit;\n\n\n\nChoking Flow\nA pitfall of compressible flow calculations is that flow at the exit of the pipe cannot exceed Ma=1, once the exit velocity achieves sonic velocity then the exit pressure will rise and the overall flowrate will remain at a constant, no matter the upstream pressure.\nThe easiest way to check for this is to use the limiting factors in Crane’s.10 Using the estimated Kf &gt; 8 and γ=1.4, we can check:\n10 Crane, A–23.\\[ {\\Delta P \\over P_1} \\lt 0.762 \\]\n\n(P₁ - P₂)/P₁\n\n0.496709300881659\n\n\n\n(P₁ - P₂)/P₁ &lt; 0.762\n\ntrue\n\n\nThere is a fit to the critical pressure ratios, as a function of Kf\n\\[\\log \\left( {\\Delta P} \\over P_1 \\right) = A \\left( \\log K_f \\right)^3 + B \\left( \\log K_f \\right)^2 + C \\left( \\log K_f \\right) + D\\]\nWith the constants for γ=1.4:\n\n\n\nA\n0.0011\n\n\nB\n-0.0302\n\n\nC\n0.238\n\n\nD\n-0.6455\n\n\n\n\nfunction critical_pressure(Kf)\n    @assert Kf &gt; 0\n    x = log(Kf)\n    y = 0.0011*x^3 - 0.0302*x^2 + 0.238*x - 0.6455\n    return exp(y)\nend\n\ncritical_pressure(Kf())\n\n0.7707810480736812\n\n\n\n(P₁ - P₂)/P₁ &lt; critical_pressure(Kf())\n\ntrue\n\n\nFor this problem we are well within the sub-sonic region."
  },
  {
    "objectID": "posts/adiabatic-compressible-flow/index.html#mechanical-energy-balance",
    "href": "posts/adiabatic-compressible-flow/index.html#mechanical-energy-balance",
    "title": "Adiabatic Compressible Flow in a Pipe",
    "section": "Mechanical Energy Balance",
    "text": "Mechanical Energy Balance\nConsider the differential form of the mechanical energy balance:\n\\[ {u \\over \\alpha} du + g dz + v dP + \\delta W_s + \\delta F = 0 \\]\nFrom the assumptions listed above, and noting that in this system there is no shaft work Ws, this can be simplified to:\n\\[ u du + v dP + {u^2 \\over 2} {f \\over D} dl = 0 \\]\nwhere f is the Darcy friction factor.\nFor compressible flow the velocity, u, varies along the length of the pipe while the mass velocity does not, so it is convenient to make the substitution u=G/ρ=Gv\n\\[ G^2 v dv + v dP + G^2 {v^2 \\over 2} {f \\over D} dl = 0 \\]\nDividing through by v2 and integrating gives:\n\\[ G^2 \\log \\left( {v_2 \\over v_1} \\right) + \\int_{P_1}^{P_2} {dP \\over v} + {K_f \\over 2} G^2 = 0 \\]\nwhere Kf is the pipe friction fL/D\nThe integral \\(\\int {dP \\over v}\\) is where the reversible and irreversible models differ, but they both amount to the same thing: integrate over an adiabatic path, and solve the mechanical energy balance for G."
  },
  {
    "objectID": "posts/adiabatic-compressible-flow/index.html#reversible-adiabatic-flow-isentropic-flow",
    "href": "posts/adiabatic-compressible-flow/index.html#reversible-adiabatic-flow-isentropic-flow",
    "title": "Adiabatic Compressible Flow in a Pipe",
    "section": "Reversible Adiabatic Flow (Isentropic Flow)",
    "text": "Reversible Adiabatic Flow (Isentropic Flow)\nTypically the isentropic flow model comes as a consequence of examining non-isothermal flow more generally, where one assumes Pvk is constant with k being a function of heat transfer (for the isothermal case k=1). The adiabatic case is then taken to be when k=γ. I think this is the greatest source of vaguery and confusion in the various sources I’ve looked at. Coulson and Richardson’s11 emphasizes that this is only an approximation as this equates to assuming an isentropic path, but many other sources either don’t make the distinction or only hint at it.\n11 Chhabra and Shankar, Coulson and Richardson’s Chemical Engineering.\\[ Pv^\\gamma = P_1 v_1^\\gamma \\]\n\\[ \\int_{P_1}^{P_2} {dP \\over v} = \\int_{P_1}^{P_2} {1 \\over v_1} \\left( P \\over P_1 \\right)^{1 \\over \\gamma} dP \\\\\n= {\\gamma \\over {\\gamma + 1} } {P_1 \\over v_1} \\left( \\left( P_2 \\over P_1 \\right)^{ {\\gamma+1}\\over\\gamma} - 1 \\right)\\]\nSubstituting this into the mechanical energy balance gives\n\\[ G^2 \\log \\left( {v_2 \\over v_1} \\right) + {\\gamma \\over {\\gamma + 1} } {P_1 \\over v_1} \\left( \\left( P_2 \\over P_1 \\right)^{ {\\gamma+1}\\over\\gamma} - 1 \\right) + {K_f \\over 2} G^2 = 0 \\]\nMaking the substitution \\[ {v_2 \\over v_1} = \\left( P_1 \\over P_2 \\right)^{1\\over\\gamma} \\]\n\\[ \\left( {K_f \\over 2} - {1 \\over \\gamma} \\log \\left( {P_2 \\over P_1} \\right) \\right) G^2 + {\\gamma \\over {\\gamma + 1} } P_1 \\rho_1 \\left( \\left( P_2 \\over P_1 \\right)^{ {\\gamma+1}\\over\\gamma} - 1 \\right) = 0 \\]\nThis form is a convenient objective function for numerical solution, however it can be re-arranged to solve for G, giving:\n\\[ G = \\sqrt{ { {2 \\gamma \\over {\\gamma + 1} } P_1 \\rho_1 \\left( 1 - \\left( P_2 \\over P_1 \\right)^{ {\\gamma+1}\\over\\gamma} \\right) } \\over { K_f - {2 \\over \\gamma} \\log \\left( {P_2 \\over P_1} \\right) } } \\]\nWhich is the form typically given in texts. If one assumes Kf is a constant then the mass velocity can be calculated directly. In practice, however, Kf is a function of the Reynolds number and so this must be solved numerically.\n\nfunction isentropic_flow(P₁, K::Number; T₁=T₁, P₂=P₂, γ=γ)\n    q = P₂/P₁\n    ρ₁ = ρ(P₁,T₁)\n    G = √( ((2γ/(γ+1))*P₁*ρ₁*(1-q^((γ+1)/γ)))/(K - (2/γ)*log(q)) )\n    return G\nend;\n\n\nusing Roots: find_zero\n\nfunction isentropic_flow(P₁, K::Function; T₁=T₁, P₂=P₂, γ=γ) \n    # Initialize Parameters\n    q = P₂/P₁\n    ρ₁ = ρ(P₁,T₁)\n    T₂ = T₁*(q^((γ-1)/γ))\n    Tₐᵥ = (T₁+T₂)/2\n    \n    # Initial guess\n    G_est = isentropic_flow(P₁, K(); T₁=T₁, P₂=P₂, γ=γ)\n    \n    # Numerically solve for G\n    obj(G) = (K(Re(G,Tₐᵥ)) - (2/γ)*log(q))*(G^2) - (2γ/(γ+1))*P₁*ρ₁*(1-q^((γ+1)/γ))\n    G = find_zero(obj, G_est)\n    \n    return G\nend;\n\n\nṁ_i = isentropic_flow(P₁, Kf)*A\n\n0.43138829795543004\n\n\n\n\n\n\n\n\n\n\nFigure 2: The mass flowrate through the example piping system as a function of pressure drop, using an isentropic flow model."
  },
  {
    "objectID": "posts/adiabatic-compressible-flow/index.html#irreversible-adiabatic-flow-fanno-flow",
    "href": "posts/adiabatic-compressible-flow/index.html#irreversible-adiabatic-flow-fanno-flow",
    "title": "Adiabatic Compressible Flow in a Pipe",
    "section": "Irreversible Adiabatic Flow (Fanno Flow)",
    "text": "Irreversible Adiabatic Flow (Fanno Flow)\nThe integration for Fanno flow is decidedly more tedious. As a sketch, start with the invariant (which comes from taking an energy balance for an ideal gas):\n\\[ {1 \\over 2} \\left( Gv \\right)^2 + {\\gamma \\over {\\gamma -1} } Pv = \\textrm{a constant}\\]\nSolve for P, take the derivative to determine dP, substitute and integrate. The result is:\n\\[ \\int_{P_1}^{P_2} {dP \\over v} = { {\\gamma -1} \\over {\\gamma} } G^2 \\left( \\left(v_1 \\over v_2\\right) -1 -2 \\log \\left(v_1 \\over v_2\\right) \\right) - \\frac{1}{2} {P_1 \\over v_1} \\left( 1 - \\left(v_1 \\over v_2\\right)^2 \\right)\\]\nWhich, when substituted into the mechanical energy balance and simplified, becomes:\n\\[ \\left( K - { {\\gamma -1} \\over {2 \\gamma} } \\left( 1 - \\left( { \\rho_2 \\over \\rho_1 } \\right)^2 \\right) - { {\\gamma +1} \\over \\gamma} \\log \\left( { \\rho_2 \\over \\rho_1 } \\right) \\right) G^2 - \\left( 1 - \\left( { \\rho_2 \\over \\rho_1 } \\right)^2 \\right) P_1 \\rho_1 = 0 \\]\nand can be further re-arranged to solve for G\n\\[ G = \\sqrt{ { P_1 \\rho_1 \\left( 1 - \\left( { \\rho_2 \\over \\rho_1 } \\right)^2 \\right) } \\over { K - { {\\gamma -1} \\over {2 \\gamma} } \\left( 1 - \\left( { \\rho_2 \\over \\rho_1 } \\right)^2 \\right) - { {\\gamma +1} \\over \\gamma} \\log \\left( { \\rho_2 \\over \\rho_1 } \\right) } } \\]\nThe obvious complication here is that ρ2 is unknown, so solving this requires simultaneously solving for either ρ2 or T2.\nMost often the Fanno flow model is given in terms of the Mach number, however the equation above is equivalent. This can be shown most easily by starting with the definition of the Fanno parameter, Fa, and the relation K = Fa1 - Fa2,\n\\[ Fa = \\left(\\frac{1 - Ma^2}{\\gamma Ma^2}\\right) + \\left(\\frac{\\gamma + 1}{2\\gamma}\\right)\\log\\left[\\frac{Ma^2}{\\left(\\frac{2}{\\gamma + 1}\\right)\\left(1 + \\frac{\\gamma - 1}{2}Ma^2\\right)}\\right] \\]\n\\[ K = \\left(\\frac{1 - Ma_1^2}{\\gamma Ma_1^2}\\right) - \\left(\\frac{1 - Ma_2^2}{\\gamma Ma_2^2}\\right) + \\left(\\frac{\\gamma + 1}{2\\gamma}\\right)\\log\\left[\\frac{Ma_1^2}{Ma_2^2}\\frac{\\left(1 + \\frac{\\gamma - 1}{2}Ma_2^2\\right)}{\\left(1 + \\frac{\\gamma - 1}{2}Ma_1^2\\right)}\\right] \\]\nThen, making the substitution:\n\\[\\left(v_1 \\over v_2\\right)^2 = { {Ma_1^2 \\left( 1 + { {\\gamma-1}\\over 2} Ma_2^2 \\right)} \\over {Ma_2^2 \\left( 1 + { {\\gamma-1}\\over 2} Ma_1^2 \\right)} }\\]\nwe get:\n\\[ K = \\left( \\frac{1}{\\gamma Ma_1^2} + \\frac{\\gamma-1}{2\\gamma} \\right) \\left( 1 - \\left(v_1 \\over v_2\\right)^2 \\right) + \\frac{\\gamma + 1}{\\gamma} \\log\\left(v_1 \\over v_2\\right) \\]\nThen, using the definition of the Mach number, in terms of G, \\(Ma=\\frac{G}{\\sqrt{\\gamma P \\rho} }\\)\n\\[ K = \\left( \\frac{P_1 \\rho_1}{G^2} + \\frac{\\gamma-1}{2\\gamma} \\right) \\left( 1 - \\left(v_1 \\over v_2\\right)^2 \\right) + \\frac{\\gamma + 1}{\\gamma} \\log\\left(v_1 \\over v_2\\right) \\]\nSolving for G, and making the substitution ρ = 1/v\n\\[ G = \\sqrt{ { P_1 \\rho_1 \\left( 1 - \\left( { \\rho_2 \\over \\rho_1 } \\right)^2 \\right) } \\over { K - { {\\gamma -1} \\over {2 \\gamma} } \\left( 1 - \\left( { \\rho_2 \\over \\rho_1 } \\right)^2 \\right) - { {\\gamma +1} \\over \\gamma} \\log \\left( { \\rho_2 \\over \\rho_1 } \\right) } } \\]\nWhich puts us back where we started.\nWhen it comes to actually using the Fanno flow model, if the goal is to calculate the flowrate for a given pressure drop, working in terms of the specific volume or density is far easier than using the model given in terms of the Mach number.\n\nApproximating Temperature Change\nAn obvious simplifying assumption is to estimate the exit temperature using the relationship for isentropic flow.\n\\[ {T_2 \\over T_1} = \\left( P_2 \\over P_1 \\right)^{ {\\gamma-1} \\over \\gamma} \\]\nIf we were assuming Kf is constant, then using this assumption to estimate the density at the exit allows for a direct calculation of the mass velocity, no numerical methods required. In the more general case, the flow still needs to be calculated iteratively as the friction factor is a function of the flow (Reynolds number).\n\nfunction approx_fanno_flow(P₁, K::Number; T₁=T₁, P₂=P₂, γ=γ)\n    ρ₁ = ρ(P₁, T₁)\n    ρ₂ = ρ₁*(P₂/P₁)^(1/γ)\n    q  = ρ₂/ρ₁\n    G = √( (P₁*ρ₁*(1-q^2))/(K - ((γ-1)/(2γ))*(1-q^2) - ((γ+1)/γ)*log(q)) )\n    return G\nend;\n\n\nusing Roots: find_zero\n\nfunction approx_fanno_flow(P₁, K::Function; T₁=T₁, P₂=P₂, γ=γ)\n    # Initializing some parameters\n    T₂ = T₁*(P₂/P₁)^((γ-1)/γ)\n    ρ₁ = ρ(P₁, T₁)\n    ρ₂ = ρ(P₂, T₂)\n    q  = ρ₂/ρ₁\n    Tₐᵥ  = (T₁+T₂)/2\n    \n    # Initial guess\n    G_est = approx_fanno_flow(P₁, K(); T₁=T₁, P₂=P₂, γ=γ)\n    \n    # Numerically solve for G\n    obj(G) = (K(Re(G,Tₐᵥ)) - ((γ-1)/(2γ))*(1-q^2) - ((γ+1)/γ)*log(q))*(G^2) - (1-q^2)*P₁*ρ₁\n    G = find_zero(obj, G_est)\n    \n    return G\nend;\n\n\nṁ_fa = approx_fanno_flow(P₁, Kf)*A\n\n0.38355173684967075\n\n\n\n\nThe Full Treatment\nActually calculating the exit conditions requires a little more work. I am going to simultaneously calculate the density at exit since it is somewhat simpler to work with than the temperature, though either could be done.\nTo start we note that:\n\\[ {1 \\over 2} \\left( Gv \\right)^2 + {\\gamma \\over {\\gamma -1} } Pv = C = \\textrm{a constant}\\]\nWhich is a quadratic in v, and solving for v:\n\\[ v = {1 \\over G^2} \\left( \\sqrt{ \\left( {\\gamma \\over {\\gamma -1} } P \\right)^2 + 2 G^2 C} - {\\gamma \\over {\\gamma-1} } P \\right) \\]\nWhere C is calculated at the entrance conditions and ρ = 1/v.\nFrom this the temperature at the exit can be backed out using the ideal gas law, and used to update the Reynolds number.\nThis makes the whole calculation somewhat more complicated, and I think that added complication makes the simplification that Kf is constant pointless – that assumption does not make the math easier in any case.\n\nusing Roots: find_zero\n\nfunction fanno_flow(P₁, K::Function; T₁=T₁, P₂=P₂, γ=γ)\n    # Initialize some parameters\n    ρ₁ = ρ(P₁,T₁)\n    \n    # Initial guesses\n    q = (P₂/P₁)^(1/γ) # isentropic\n    G_est = √( (P₁*ρ₁*(1-q^2))/(K() - ((γ-1)/(2γ))*(1-q^2) - ((γ+1)/γ)*log(q)) )\n    \n    function obj(G)\n        # Calculate the downstream density\n        C  = 0.5*(G/ρ₁)^2 + (γ/(γ-1))*(P₁/ρ₁)\n        G²v = √(((γ/(γ-1))*P₂)^2 + 2*(G^2)*C) - (γ/(γ-1))*P₂\n        ρ₂ = (G^2)/G²v\n        @assert ρ₂ &gt; 0\n\n        # Update Temperature dependent parameters\n        T₂ = (P₂*Mw)/(R*ρ₂)\n        Tₐᵥ = (T₁+T₂)/2\n\n        # Calculate the objective value\n        q  = ρ₂/ρ₁\n        return (K(Re(G,Tₐᵥ)) - ((γ-1)/(2γ))*(1-q^2) - ((γ+1)/γ)*log(q))*G^2 - P₁*ρ₁*(1-q^2)\n    end\n    \n    G = find_zero(obj, G_est)\n    \n    return G\nend;\n\n\nfunction fanno_flow(P₁, K::Number; T₁=T₁, P₂=P₂, γ=γ)\n    fn() = K\n    fn(Re) = K\n    return fanno_flow(P₁, fn; T₁=T₁, P₂=P₂, γ=γ)\nend;\n\n\nṁ_f = fanno_flow(P₁, Kf)*A\n\n0.40934309494917254\n\n\n\n\n\n\n\n\n\n\nFigure 3: The mass flowrate through the example piping system as a function of pressure drop, using an adiabatic Fanno flow model.\n\n\n\n\n\nThe approximation produces reasonable results in this case, especially at higher pressure drops, but one should always be cautious when mixing results from different models.12\n12 This is something worth keeping mind more generally, as I have seen the assumption that Fanno flow is approximately isentropic (implicitly) taken for calculating different flow parameters, and it is often a bad assumption. For example, some references use the isentropic choking condition for a nozzle as an estimate for the choking condition in Fanno flow. Unless the pipe is incredibly short this is a terrible approximation – in the current example the pressure drop exceeds the choking flow condition for a nozzle and yet the pipe flow is far from choked."
  },
  {
    "objectID": "posts/adiabatic-compressible-flow/index.html#expansion-factors-y-factors",
    "href": "posts/adiabatic-compressible-flow/index.html#expansion-factors-y-factors",
    "title": "Adiabatic Compressible Flow in a Pipe",
    "section": "Expansion Factors (Y Factors)",
    "text": "Expansion Factors (Y Factors)\nBy far the simplest method is to use a modified Darcy equation with expansion factors (Y factors). This takes the well known Darcy equation for incompressible pipe flow and, in a classic engineering move, tacks on a Y factor to account for all the complexity in adiabatic flow.\n\\[ G = Y \\sqrt{ { 2 \\rho_1 \\Delta P } \\over K } \\]\nWhere the expansion factor, Y, is read off of a chart. This is great if you are working things out by hand, but can present some challenges when calculating things on a computer. Ludwig’s13 provides a complicated series of equations to iteratively calculate the Y curves yourself, but I think if you are expending that level of effort then you really are not saving anything over using the Fanno model above. A much simpler approach is to either interpolate the critical expansion factor, Ycr, and critical pressure ratio, qcr, from the values given in Crane’s or use a correlation for them (that’s what I will use). Though this adds the wrinkle of only being able to use Y factors for gases with the same γ as what is either tabulated or available in a correlation.\n13 Coker, Ludwig’s Applied Process Design for Chemical and Petrochemical Plants.The actual Y value then comes from a simple linear relationship (where \\(q={ {\\Delta P} \\over P_1}\\) )\n\\[ Y = \\left( Y_{cr} -1 \\right) \\left( q \\over q_{cr} \\right) +1 \\]\nThis has the added convenience of telling you when you have crossed into choked flow, it happens when q&gt;qcr.\nOne downside is that this method does not directly produce the exit conditions, so the Reynolds number is typically taken at the entrance conditions only. Since the Reynolds number is only a function of Temperature through the viscosity, this works out fine over ranges where the viscosity is approximately constant.14\n14 The temperature can be worked out by using the method given in the section for Fanno flow, calculating the invariant at entrance conditions (once G is known) and then solving for the exit density.\n# Correlations for γ=1.4\n\nfunction Ycr(K)\n    x = log(K)\n    y = 0.0006*x^3 - 0.0185*x^2 + 0.1141*x - 0.5304\n    return exp(y)\nend;\n\nfunction qcr(K)\n    x = log(K)\n    y = 0.0011*x^3 - 0.0302*x^2 + 0.238*x - 0.6455\n    return exp(y)\nend;\n\n\n\n\n\n\n\n\n\nFigure 4: The correlation curves for critical expansion factor and critical pressure ratio, along with tabulated values from Crane’s.15\n\n15 Crane, “TP410M,” A–23.\n\n\n\nThe correlation curves I am using fit the tabulated values from Crane’s reasonably well, but clearly the fit is not perfect.\n\nfunction Y(K,q)\n    Yc, qc = Ycr(K), qcr(K)\n    if q &lt; qc\n        return (Yc-1)*(q/qc)+1\n    else\n        return Yc\n    end\nend;\n\n\n\n\n\n\n\n\n\nFigure 5: The expansion factor vs pressure ratio, this calculated example falls between the reference curves from Crane’s as expected.16\n\n16 Crane, A–23.\n\n\n\nThe calculated curve is between the bracketing curves in Crane’s and looks plausible.\nIf you are assuming that Kf is constant then the mass velocity can be calculated directly, however if you wish to be more exact you can also iterate.\n\nfunction modified_darcy(P₁, K::Number; T₁=T₁, P₂=P₂, γ=γ)\n    ρ₁ = ρ(P₁,T₁)\n    q = (P₁-P₂)/P₁\n    G = Y(K,q)*√(2*ρ₁*(P₁-P₂)/K)\n    return G\nend;\n\n\nusing Roots: find_zero\n\nfunction modified_darcy(P₁, K::Function; T₁=T₁, P₂=P₂, γ=γ)\n    # Intialize Parameters\n    ρ₁ = ρ(P₁,T₁)\n    q = (P₁-P₂)/P₁\n    \n    # Initial Guess\n    G_est = modified_darcy(P₁, K(); T₁=T₁, P₂=P₂, γ=γ)\n    \n    # Numerically solve for G\n    obj(G) = G - modified_darcy(P₁, K(Re(G,T₁)); T₁=T₁, P₂=P₂, γ=γ) \n    G = find_zero(obj, G_est)\n    \n    return G\nend;\n\n\nṁ_y = modified_darcy(P₁, Kf)*A\n\n0.4049511071122898\n\n\n\n\n\n\n\n\n\n\nFigure 6: The mass flowrate through the example piping system as a function of pressure drop, using the modified Darcy equation."
  },
  {
    "objectID": "posts/adiabatic-compressible-flow/index.html#comparison",
    "href": "posts/adiabatic-compressible-flow/index.html#comparison",
    "title": "Adiabatic Compressible Flow in a Pipe",
    "section": "Comparison",
    "text": "Comparison\nBelow is a plot showing all of the methods examined so far, including assuming the isothermal case (this a common recommendation for a simplifying assumption). The expansion factor method approximates the Fanno flow method, from which it was derived, quite well, to the point where they are essentially indistinguishable. The isothermal model is practically just as good for this particular example, while the isentropic model works well only for low pressure drops, the version of the Fanno flow that approximates the temperature as isentropic is the opposite, being the worst model at low pressure drops and converging towards the Fanno model at higher pressure drops.\n\n\n\n\n\n\n\n\nFigure 7: The mass flowrate through the example piping system as a function of pressure drop, showing all of the discussed models. Note the significant overlap of the Fanno flow, modified Darcy equation, and isothermal flow curves.\n\n\n\n\n\nBut this is just one example, perhaps we can look at a wider range of Kf and pressure drops. Conveniently, Crane’s has a table with Kf ranging from 1 to 100 and calculated pressure drops and expansion factors: the limiting factors. Using the models examined above, the effective expansion factors can be calculated quite easily for each K in Crane’s table (taking the pressure ratios as givens).\n\n\n\n\n\n\n\n\nFigure 8: Calculated expansion factors for flow at the critical K values tabulated in Crane’s, this represents flow at the critical pressure ratio17\n\n17 Crane, A–23.\n\n\n\nNote that this represents the greatest pressure drop for a given Kf, which should correspond to the “worst case” for most models (except the approximated Fanno model). The Fanno model and the correlation I was using to generate Y factors line up quite nicely, though there is a fair amount of scatter with the tabulated Y factors which is interesting. The isentropic model is close, but not in great agreement, over the entire range. What I find more interesting is how rapidly the isothermal model comes into agreement. We would expect, then, at Kf=100 that the isothermal model would basically fall on top of the Fanno model over the entire range of pressure.\n\n\n\n\n\n\n\n\nFigure 9: The mass flowrate for isothermal and Fanno flow models vs pressure drop for high K piping systems. Note that both lines overlap almost entirely for the entire range.\n\n\n\n\n\nThey are basically indistinguishable. However, this is not at all implying that the temperature in the Fanno flow model is remaining constant over these large pressure drops. As one would expect, the adiabatic flow moves further from isothermal as the pressure drop increases. The mass flows just happen to be the same.\n\n\n\n\n\n\n\n\nFigure 10: The exit temperature for isothermal, isentropic, and Fanno flow models vs pressure drop for high K piping systems. Note that while the isothermal and Fanno flow models may give identical mass flowrates, the exit conditions are quite different."
  },
  {
    "objectID": "posts/adiabatic-compressible-flow/index.html#final-thoughts",
    "href": "posts/adiabatic-compressible-flow/index.html#final-thoughts",
    "title": "Adiabatic Compressible Flow in a Pipe",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI think the big take-away is that the isentropic flow model is not a very good approximation of Fanno flow and references that suggest that it is are in error. The other big take-away may be that, at least when calculating mass flow rates, the isothermal model is often better than one would expect: it does well at low pressure drops and also for long lines where Kf is large. In practice, when the flow conditions are within the range of available Y factors, the modified Darcy equation is the easiest to use and gives excellent agreement with the full Fanno model, however when the situation is outside of that range and Y factors have to be calculated it is not a time-saver.\nThe big elephant in the room is that, in practice, no actual gas flow is perfectly ideal or perfectly adiabatic, nor is the friction factor truly a constant. These assumptions play a big role in the overall model error, and being fussy about some of the details of different adiabatic ideal gas models may amount to nothing in practice."
  },
  {
    "objectID": "posts/adiabatic-compressible-flow/index.html#references",
    "href": "posts/adiabatic-compressible-flow/index.html#references",
    "title": "Adiabatic Compressible Flow in a Pipe",
    "section": "References",
    "text": "References\n\n\nAlbright, Lyle F. Albright’s Chemical Engineering Handbook. Boca Raton: CRC Press, 2009.\n\n\nChhabra, R. P., and V. Shankar. Coulson and Richardson’s Chemical Engineering: Fluid Flow: Fundamentals and Applications. 7th ed. Vol. 1A. Amsterdam: Elsevier, 2018.\n\n\nCoker, A. Kayode. Ludwig’s Applied Process Design for Chemical and Petrochemical Plants. 4th ed. Amsterdam: Elsevier, 2007.\n\n\nCrane. “TP410M: Flow of Fluids.” Stamford, CT: Crane, 2013.\n\n\nHall, Stephen M. Rules of Thumb for Chemical Engineers. 6th ed. Amsterdam: Elsevier, 2018.\n\n\nTilton, James N. “Fluid and Particle Dynamics.” In Perry’s Chemical Engineers’ Handbook, edited by Don W. Green, 8th ed. New York: McGraw Hill, 2008."
  },
  {
    "objectID": "posts/intpuff2_successive_approximations/index.html",
    "href": "posts/intpuff2_successive_approximations/index.html",
    "title": "Integrating a Gaussian puff - mistakes were made",
    "section": "",
    "text": "The other day I was working on a project involving Gaussian puff models and I noticed that I had made a significant mistake, a mistake I have made several times without noticing, and one that invalidated a whole bunch of work I that I had done previously, so I thought this would be a good opportunity to examine my mistake and it’s consequences."
  },
  {
    "objectID": "posts/intpuff2_successive_approximations/index.html#the-gaussian-puff-model",
    "href": "posts/intpuff2_successive_approximations/index.html#the-gaussian-puff-model",
    "title": "Integrating a Gaussian puff - mistakes were made",
    "section": "The Gaussian puff model",
    "text": "The Gaussian puff model\nTo re-cap on what a Gaussian puff model even is: for a short duration release (strictly an instantaneous release) of a neutrally buoyant substance at ground-level, the concentration can be modeled as the product of three Gaussian distributions:\n\\[ c \\left(x,y,z,t \\right) = \\dot{m} \\Delta t \\cdot g_x(x, t) \\cdot g_y(y) \\cdot g_z(z) \\]\nwhere\n\\[ g_x(x,t) = {1 \\over \\sqrt{2\\pi} \\sigma_x } \\exp \\left( -\\frac{1}{2} \\left( x-u t \\over \\sigma_x \\right)^2 \\right) \\]\n\\[ g_y(y) = {1 \\over \\sqrt{2\\pi} \\sigma_y } \\exp \\left( -\\frac{1}{2} \\left( y \\over \\sigma_y \\right)^2 \\right) \\]\n\\[ g_z(z) = {2 \\over \\sqrt{2\\pi} \\sigma_z } \\exp \\left( -\\frac{1}{2} \\left( z \\over \\sigma_z \\right)^2 \\right) \\]\nWhere \\(\\dot{m}\\) is the mass emission rate, Δt the duration of the release, and u the ambient windspeed. The coordinates are such that the release point is at the origin, the puff moves in the downwind, x, direction while spreading into the crosswind, y, and vertical, z, directions.\nThe dispersion parameters, σx, σy, σz are all functions of the downwind distance and the atmospheric stability.\n\n# class F puff dispersion\n# x is in meters\nσx(x) = 0.024*x^0.89\nσy(x) = σx(x)\nσz(x) = 0.05*x^0.61"
  },
  {
    "objectID": "posts/intpuff2_successive_approximations/index.html#integrating-the-puff",
    "href": "posts/intpuff2_successive_approximations/index.html#integrating-the-puff",
    "title": "Integrating a Gaussian puff - mistakes were made",
    "section": "Integrating the puff",
    "text": "Integrating the puff\nWhat this generates is an instantaneous release of all of the mass in an infinitesimal point that grows as it moves downwind. This isn’t terribly realistic for releases of any appreciable duration (all of the mass is released instantly in this model), so a common approach is to break up the release into a sequence of n smaller puffs that each capture the mass released over the sub-interval \\({ \\Delta t \\over n }\\). Taking the limit as \\(n \\to \\infty\\) equates to integrating the puff model from t - Δt to t giving a nice solution in terms of the error function erf and … this is where I made the critical mistake.\nThe dispersion parameters are functions of the downwind distance, but critically..to what? Taken as the downwind distance to the point being calculated, the dispersion parameters are constants (with respect to time) and the problem simplifies to integrating the Gaussian \\(g_{x}(x,t)\\) with respect to t, which is what I had assumed. However if the dispersion parameters are actually correlated to the downwind distance of the cloud center, which is \\(x_c = u t\\), they are in fact functions of time and this does not work.\nThis distinction is by no means made obvious in many of the references for chemical hazard analysis. Most are either vague about it or take the dispersion parameters at the downwind distance of the point being calculated. My main reference is the CCPS Guidelines for Consequence Analysis of Chemical Releases and it does this.1 As do several workbooks I have seen. However Lees2 notes that the dispersion parameters for the Pasquill-Gifford puff model (which this is) are given by3\n1 AIChE/CCPS, Guidelines for Consequence Analysis of Chemical Releases., 107–8.2 Lees, Loss Prevention in the Process Industries.3 Lees, 15/112.\\[ \\sigma = { C^2 \\over 2 } \\left( u t \\right)^{2-n} \\]\nwhere C and n are some constants from Sutton, and in general the dispersion correlations are functions of travel time with a lot of discussion in the literature of to what power. The standard correlations for the dispersion parameters come from Slade4 which gives some details on how the measurements were actually taken. It certainly seems to me that the downwind distance was to the cloud center, i.e. the experimenters measured the cloud dimensions at the downwind point to which it had traveled. Which makes the travel time and windspeed implicit.\n4 Slade, Meteorology and Atomic Energy, 117–89.I think it is a reasonable confusion as the dispersion parameters for a continuous release, a Gaussian plume model, are indeed functions of the downwind distance to the point being calculated. It is also frequently the case that examples are given for the concentration at the cloud center, in which case the downwind distance at the point being calculated is the downwind distance to the cloud center."
  },
  {
    "objectID": "posts/intpuff2_successive_approximations/index.html#dispersion-nearly-constants",
    "href": "posts/intpuff2_successive_approximations/index.html#dispersion-nearly-constants",
    "title": "Integrating a Gaussian puff - mistakes were made",
    "section": "Dispersion nearly-constants",
    "text": "Dispersion nearly-constants\nHow critical of a mistake is this? For regions far enough from the origin the dispersion parameters do not vary much in the neighborhood of the plume center. This is shown in the plot below where the difference is taken over the interval \\([ x - \\sigma_x, x + \\sigma_x ]\\). At distances further than a few hundred meters the difference is only a few percent. Suggesting that it might not be an unreasonable approximation to assume the dispersion parameters are constants for the purpose of the integral.\n\n\n\n\n\n\n\n\nFigure 1: The relative change in dispersion parameters, ±1σ, as a function of downwind distance."
  },
  {
    "objectID": "posts/intpuff2_successive_approximations/index.html#different-approaches-to-approximation",
    "href": "posts/intpuff2_successive_approximations/index.html#different-approaches-to-approximation",
    "title": "Integrating a Gaussian puff - mistakes were made",
    "section": "Different approaches to approximation",
    "text": "Different approaches to approximation\nAnother way of approaching this is simply to view it as an approximation instead of an error. On the one hand this is a pretty great rhetorical trick: my answer isn’t wrong, it’s just differently true. But it could be the case that this is a useful simplification, just by eye-balling isopleths and looking at limiting behavior in the previous notebook it certainly looked reasonable.\nTo make life easier, going forward, I am going to define a unit-less time \\[ t = { u t^{\\prime} \\over L } \\]\nand unit-less distances\n\\[ x = {x^{\\prime} \\over L } \\\\ y = {y^{\\prime} \\over L } \\\\ z = {z^{\\prime} \\over L } \\]\nwhere I am abusing notation with the \\(\\prime\\) indicates the variable with units, and no \\(\\prime\\) indicates it is unitless. A characteristic length, \\(L\\), is introduced to make everything unitless and, due to the dispersion correlations \\(L = 1 \\mathrm{m}\\) is the most convenient.\nWe can then explore the performance of different approximations to the integrated puff model by only examining the Gaussian distributions – with no dependence upon \\(\\dot{m}\\) or u.\n\ng(ξ,σ) = exp(-0.5*(ξ/σ)^2)/(√(2π)*σ)\n\ngx(x, t) = g((x-t),σx(t))\ngy(y, t) = g(y,σy(t))\ngz(z, t) = 2*g(z,σz(t))\n\npf(x,y,z,t; Δt) = gx(x,t)*gy(y,t)*gz(z,t)*Δt\n\n\nSum of discrete puffs\nThe first type of approximation is to divide the release interval into n sub-intervals and n Gaussian puffs\n\nfunction Σpf(x,y,z,t; Δt, n)\n    Δt = min(t,Δt)\n    δt = Δt/(n-1)\n    _sum = 0\n    for i in 0:(n-1)\n        t′ = t-i*δt\n        pf_i = t′&gt;0 ? gx(x,t′)*gy(y,t′)*gz(z,t′)*δt : 0\n        _sum += pf_i\n    end\n    return _sum\nend\n\n\n\nIntegrating assuming constant σs\nThe next type of approximation is the one I made in the previous post wherein \\(g_x(x,t)\\) is integrated with respect to time, treating the σs as constants.\nThere is a little sleight of hand as I include the downwind distance dependence of the σs after the integration (they aren’t actually constants)\n\nusing SpecialFunctions: erf\n\nfunction ∫gx(x,t,Δt)\n    Δt = min(t,Δt)\n    a  = (x-(t-Δt))/(√2*σx(t-Δt))\n    b  = (x-t)/(√2*σx(t))\n    return erf(b,a)/2\nend\n\n∫pf_approx(x,y,z,t; Δt) = ∫gx(x,t,Δt)*gy(y,x)*gz(z,x)\n\n\n\nNumerically integrating the full model\nFinally, I take advantage of the QuadGK package to numerically integrate the Gaussian puff model, including the time dependence of the dispersion parameters.\n\nusing QuadGK: quadgk\n\nfunction ∫pf(x,y,z,t; Δt)\n    Δt = min(t,Δt)\n    integral, err = quadgk(τ -&gt; gx(x,τ)*gy(y,τ)*gz(z,τ), t-Δt, t)\n    return integral\nend"
  },
  {
    "objectID": "posts/intpuff2_successive_approximations/index.html#comparing-performance",
    "href": "posts/intpuff2_successive_approximations/index.html#comparing-performance",
    "title": "Integrating a Gaussian puff - mistakes were made",
    "section": "Comparing performance",
    "text": "Comparing performance\n\nModel error\nTo give a sense of how these successive approximations work, lets examine a series of slices through the cloud. The first is at a constant x on the center-line of the release, looking at how the concentration changes with time.\nJust by eye-ball the the approximate integral is very close to the numerical exact(ish) integral, as is a large enough number of puffs. Importantly, I think, the approximate integral error is of the same order of magnitude as a large number of puffs – so this is at least as good in a sense as the discrete sum of puffs method, given that we can vary the number of puffs to always make it a better/worse approximation\n\n\n\n\n\n\n\n\nFigure 2: Top: concentration profile over time, at a fixed location, for discrete sums of puffs, the approximated integral, and the exact integral. Bottom: the relative error of a sum of discrete puffs and the approximate integral.\n\n\n\n\n\nIn the crosswind and vertical directions the sum of discrete puffs approximation works decidedly less well, at least at this slice in the cloud, while the approximate integral still works relatively well. I would say it is still at least as good as a sum of discrete puffs for a suitably large number of puffs.\n\n\n\n\n\n\n\n\nFigure 3: Top: crosswind concentration profile, at a fixed location and time, for discrete sums of puffs, the approximated integral, and the exact integral. Bottom: the relative error of a sum of discrete puffs and the approximate integral.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Top: vertical concentration profile, at a fixed location and time, for discrete sums of puffs, the approximated integral, and the exact integral. Bottom: the relative error of a sum of discrete puffs and the approximate integral.\n\n\n\n\n\nThis is, of course, very particular to that point downwind of the release. As we move closer to the origin the integral approximation gets worse, but then so does the sum of discrete puffs model. Especially for a low number of puffs: they become visibly discrete. I think this reinforces that, at least for class F stability, this approximation is in the same ball park as summing over a set discrete Gaussian puffs.\n\n\n\n\n\n\n\n\nFigure 5: Top: concentration profile over time near the origin, for discrete sums of puffs, the approximated integral, and the exact integral. Bottom: the relative error of a sum of discrete puffs and the approximate integral.\n\n\n\n\n\n\n\nCompute time\nModel error is not the only factor in deciding upon an approximation. Since QuadGK exists we have to ask ourselves, why would we not always use it? We can answer that by benchmarking the three approaches at a particular point of interest (I don’t think the choice of point impacts the calculations at all)\n\nusing BenchmarkTools: @benchmark\n\n# point of interest\nx₁ = 100\ny₁ = σy(x₁)\nz₁ = σz(x₁)\nt₁ = x₁\n\nStarting with the full numerical integration of the model, this is the time to beat. Any approximation that takes longer than ~30μs is literally pointless: it generates worse results and takes longer.\n\n@benchmark ∫pf(x₁,y₁,z₁,t₁; Δt=10)\n\n\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):  28.056 μs … 57.603 μs  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     28.200 μs              ┊ GC (median):    0.00%\n Time  (mean ± σ):   28.570 μs ±  1.565 μs  ┊ GC (mean ± σ):  0.00% ± 0.00%\n  █▆▁  ▃▃   ▁▂▁▁                                              ▁\n  ███▇▅██▇▅▄█████▆▆▆▇▆█▇▆▄▃▁▄▄▄▃▆▅▄▄▃▁▃▁▃▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▄▅▆▇▆ █\n  28.1 μs      Histogram: log(frequency) by time      37.7 μs &lt;\n Memory estimate: 384 bytes, allocs estimate: 3.\n\n\n\nAs we expect, the sequence of discrete puffs is much faster for fewer puffs, and adding an order of magnitude more puffs increases the time by an order of magnitude. At around n=100 we are no longer gaining anything over the full numerical integration. So, if the near-field matters a lot to you, then this is probably not a great approximation as the number of puffs required to approximate the full numerical integration well takes longer than just doing the integration.\n\n@benchmark Σpf(x₁,y₁,z₁,t₁; Δt=10, n=10)\n\n\nBenchmarkTools.Trial: 10000 samples with 8 evaluations.\n Range (min … max):  3.746 μs …  11.498 μs  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     3.768 μs               ┊ GC (median):    0.00%\n Time  (mean ± σ):   3.920 μs ± 450.991 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%\n  █  ▅  ▅▂▁▁▂                                                 ▁\n  ██▄█▇▅██████▇▆▄▄▄▃▅▄▄▂▄▄▅▃▄▅▅▆▇▇▆▆▇▇▆▆▅▅▅▅▆▅▅▅▅▄▃▅▅▄▅▅▄▄▄▂▃ █\n  3.75 μs      Histogram: log(frequency) by time      5.97 μs &lt;\n Memory estimate: 16 bytes, allocs estimate: 1.\n\n\n\n\n@benchmark Σpf(x₁,y₁,z₁,t₁; Δt=10, n=100)\n\n\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):  37.090 μs … 77.023 μs  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     37.199 μs              ┊ GC (median):    0.00%\n Time  (mean ± σ):   37.760 μs ±  2.411 μs  ┊ GC (mean ± σ):  0.00% ± 0.00%\n  █    ▂▁    ▃                                                ▁\n  █▇▁▁▁██▃▃▁▃█▇▇▆▄▄▇█▅▄▄▄▄▄▄▃▄▄▃▁▅▁▄▃▁▁▃▃▃▃▄▇▇█▇▇▆▄▅▅▅▆▄▄▆▄▄▄ █\n  37.1 μs      Histogram: log(frequency) by time      49.3 μs &lt;\n Memory estimate: 16 bytes, allocs estimate: 1.\n\n\n\nFinally we have the integral approximation. This takes ~1/50th the time as the full numerical integration and, by the results above, it potentially performs just as well as the discrete puff approximation. In the examples above it was doing as well as discrete puff approximations that are too large to be worthwhile.\n\n@benchmark ∫pf_approx(x₁,y₁,z₁,t₁; Δt=10)\n\n\nBenchmarkTools.Trial: 10000 samples with 189 evaluations.\n Range (min … max):  534.974 ns … 988.852 ns  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     547.606 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   560.844 ns ±  40.302 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%\n  ▇▃▅█▅▅ ▅▃▄ ▃▄▁▁▁▁▁                                            ▂\n  ██████▇█████████████▇█▇▇▆▆▆▆▅▇▇▆▅▆▇▆▆▅▅▅▅▄▄▆▅▅▅▆▆▄▅▅▂▅▄▅▄▃▅▅▅ █\n  535 ns        Histogram: log(frequency) by time        770 ns &lt;\n Memory estimate: 16 bytes, allocs estimate: 1.\n\n\n\nI also have put no effort into optimizing any of this code, so take this with a grain of salt. Like the examination of the model error this is hardly rigorous, it is more suggestive than anything. It is possible that one could dramatically improve the discrete puff model, or re-write how the models are calculated to be more performant than I have. I prefer to write code that is easy for me to read, and re-uses things, but that does not necessarily translate into fast."
  },
  {
    "objectID": "posts/intpuff2_successive_approximations/index.html#conclusions",
    "href": "posts/intpuff2_successive_approximations/index.html#conclusions",
    "title": "Integrating a Gaussian puff - mistakes were made",
    "section": "Conclusions",
    "text": "Conclusions\nI think it’s worth noting that calculations that take on the order of tens of microseconds, on my crappy old laptop, are fast. To make the various plots required calculating the concentration at hundreds of points and my laptop did it all in the blink of an eye. I would say the first choice, all things being equal, would be simply to use the QuadGK model and call it a day. In terms of lines of code it is certainly short, all the heavy lifting is being done by the library. It also best captures what you are trying to achieve.\nIf you are doing a huge number of calculations, and can tolerate some model error, then the integral approximation is a good choice. It is the fastest and can perform as well as the discrete puff model. That said, there is an elephant in the room: The two integral approaches strictly require that all of the puffs are moving along the same line, at the same speed. For a great many chemical release scenarios that is entirely the set of assumptions being made, so it works great. However, for more complex atmospheric conditions – with variable windspeed and direction – then they don’t work at all. Or, at least, it is not obvious to me how to adapt them to work. A slightly tweaked discrete puff model, tracking each puff’s individual center location and windspeed, would be quite easy to implement, giving a more flexible model overall. This is in fact the how several more complicated atmospheric dispersion modeling tools work."
  },
  {
    "objectID": "posts/intpuff2_successive_approximations/index.html#references",
    "href": "posts/intpuff2_successive_approximations/index.html#references",
    "title": "Integrating a Gaussian puff - mistakes were made",
    "section": "References",
    "text": "References\n\n\nAIChE/CCPS. Guidelines for Consequence Analysis of Chemical Releases. New York: American Institute of Chemical Engineers, 1999.\n\n\nLees, Frank P. Loss Prevention in the Process Industries. 2nd ed. Oxford: Butterworth-Heinemann, 1996.\n\n\nSlade, David H. Meteorology and Atomic Energy. Springfield, VA: National Technical Information Service, 1968. https://doi.org/10.2172/4492043."
  },
  {
    "objectID": "posts/hydrogen_release_modeling/index.html",
    "href": "posts/hydrogen_release_modeling/index.html",
    "title": "Modelling Hydrogen Releases Using HyRAM+",
    "section": "",
    "text": "Continuing on a series of posts on hydrogen of sorts, this post is on modelling hydrogen releases for risk assessment. Industry in many places, and in Alberta in particular, is looking to hydrogen as a key component of the transition to a low carbon future. This means that, suddenly, there will by hydrogen pressure equipment in a lot of process areas where it wasn’t before, as fuel gas.\nHydrogen presents an interesting challenge for hazard analysis as it is lighter than air, rising and accumulating in places a more standard analysis would neglect. In my experience, the typical release modelling tools are for neutrally buoyant or negatively buoyant (heavier than air) gas releases, such as Gaussian plume models or dense gas models like SLAB, DEGADIS, and PHAST. They are not designed for, and may not accurately capture, the dispersion of hydrogen.\nDense gas models typically assume cloud dynamics that are particular to denser than air clouds, with limiting behavior that brings the results in line with a neutrally buoyant release. Denser than air clouds pile up around the source, leading to dispersion upwind of the source, and have sharper cloud fronts than a more neutral cloud. These features are often written into the governing equations for plume dispersion from the outset.\nNeutral and positive buoyancy plume models typically account for buoyancy differences only through temperature as they are generally intended for hot stack gases and not low molecular weight gases. For example, the standard implementation of Brigg’s plume-rise in tools like ISC3 use only the temperature of the source to calculate the buoyant flux – implicitly assuming the molar weight of the gas is similar to that of air. The original Ooms model1 for positively buoyant plumes also only accounts for buoyancy differences due to temperature. These models would erroneously conclude that a stream of cold hydrogen gas would be heavier than air and would thus sink.\nThis leaves a lot of room for integral plume models that better handle the behaviour of low molecular weight gases. One such model is incorporated into HyRAM+, from Sandia National Laboratories in the United States.2 It includes an integral plume model for positively buoyant plumes that accounts for differences in buoyancy by molar weight in addition to temperature, and was designed with hydrogen dispersion in mind.\nHyRAM+ is implemented in python, with a Windows GUI, though I will be using it directly in a jupyter notebook. Partly because I use linux at home, but also I am interested in how one would use the plume dispersion and other tools independently. I’m interested in the use case where this is integrated into an existing process safety management system and what is needed are specific values from a consequence analysis such as the explosive mass."
  },
  {
    "objectID": "posts/hydrogen_release_modeling/index.html#the-scenario",
    "href": "posts/hydrogen_release_modeling/index.html#the-scenario",
    "title": "Modelling Hydrogen Releases Using HyRAM+",
    "section": "The Scenario",
    "text": "The Scenario\nJust for something to play around with, consider the case of a leak from a hydrogen cylinder into the ambient air. Suppose a cylinder containing 50kg of hydrogen at 35MPa has fallen over and the valve has broken, creating a leak from a 1/4 in. hole at essentially ground level and is oriented at 45° upwards. The hydrogen is initially at ambient temperature and the ambient air is at standard conditions and is otherwise quiescent.\n\n\n\n\n\n\nFigure 1: A sketch of the scenario: a hydrogen gas release from a fallen cylinder.\n\n\n\nUsing the HyRAM+ API we can create the ambient air, air, and hydrogen, h2, fluid models at initial conditions.\n\nimport numpy as np\nimport hyram.phys.api as api\nimport hyram.phys as hp\n\n\nTa, Pa = 288.15, 101325\nair = api.create_fluid(\"Air\", Ta, Pa)\n\nTh2, Ph2 = Ta, 35e6\nh2 = api.create_fluid(\"Hydrogen\", Th2, Ph2)\n\nThe broken valve is initialized as an Orifice object, which has a diameter and a discharge coefficient. In this case I assume the discharge coefficient is 0.6.\n\nd_H = 25.4e-3/4 # mm\nc_d = 0.6 # assumed\ntheta = np.pi/4\norifice = hp.Orifice(d_H, c_d)\n\n\nModeling the Jet\nThe jet is modeled, by HyRAM+, as as a steady-state jet consisting of 3 distinct zones:3\n3 Ehrhart, Hecht, and Schroeder, Hydrogen Plus Other Alternative Fuels Risk Assessment Models (HyRAM+) Version 5.1 Technical Reference Manual.\nOrifice flow in which the release occurs isentropically through an orifice. HyRAM+ uses the CoolProp library to perform this calculation for the real fluid (unlike many other models which assume an ideal gas for simplicity). For most situations, such as with this example, the flow will be choked and the jet will enter the atmosphere at sonic velocity (Ma = 1) and under-expanded (i.e. the pressure in the jet is above atmospheric)\nNotional nozzle the under-expanded jet then expands to ambient pressure. HyRAM+ models this as occuring adiabatically and with no entrainment of ambient air. The expansion occurs across what is termed a notional nozzle as it is modeled as a nozzle stepping down the jet to ambient pressure, assuming isentropic expansion. The notional nozzle is assumed to be of negligible size, so this step is really about calculating the initial conditions for the actual dispersion.\nGaussian jet at the end of the notional nozzle, and assuming that no cryogenic effects need to be corrected for, the jet is assumed to follow a self-similar Gaussian profile in both velocity and concentration. It is the same Gaussian model I discussed previously for a turbulent jet, however in that case the jet center-line was simply a straight line. In this case the center-line follows a curve through space which needs to be solved for. This is done using an integral plume model not unlike the Ooms model4 which accounts for entrainment, conservation of momentum, and conservation of mass.\n\n4 Ooms, “A New Method for the Calculation of the Plume Path of Gases Emitted by a Stack”.The jet is created by initializing a Jet object, which solves the zones and integrates the governing equations to determine the plume center-line.\nHyRAM+ has several internal models for solving the notional nozzle, the default is the model by Yüceil and Ötügen and is selected with the keyword parameters nn_conserve_momentum=True and nn_T='solve_energy'\n\njet = hp.Jet(h2, orifice, air, theta0=theta,\n             nn_conserve_momentum=True,\n             nn_T='solve_energy',\n             verbose=True)\n\nsolving for orifice flow... done\nsolving for notional nozzle... done.\nintegrating... done.\n\n\nWith this done, we can retrieve the mass flow-rate (in kg/s)\n\njet.mass_flow_rate\n\n0.4024272255826383\n\n\nWe can compare this to a simple ideal gas model of an adiabatic orifice5\n5 AIChE/CCPS, Guidelines for Consequence Analysis of Chemical Releases.\\[ \\dot{m} = c_d A_h \\sqrt{ \\rho_1 p_1 k \\left( 2 \\over k+1 \\right)^{k+1 \\over k-1} } \\]\n\nA_h = (np.pi/4)*d_H**2\nk = 1.41\n\nideal_gas_jet = c_d*A_h*np.sqrt( h2.rho*h2.P*k*pow(2/(k+1),(k+1)/(k-1)) )\n\nideal_gas_jet\n\n0.37797876222402915\n\n\n\njet.mass_flow_rate/ideal_gas_jet\n\n1.0646821086315916\n\n\nThe HyRAM+ model is estimating a ~6% greater mass flow rate through the orifice than a simple ideal gas jet model. From an end user perspective, this adds a dimension of realism to the model without requiring really anything more from the user. There are probably several opportunities to use more realistic fluid models, elsewhere in the standard literature of hazard analysis, that haven’t been realized more for reasons of tradition and laziness than anything else.\nIn the past, modelling an isentropic nozzle with a real gas from scratch was a pain as there is a lot of overhead in implementing a more realistic equation of state. Especially gathering all of the relevant model parameters. With libraries like CoolProp, it really drops the barrier for incorporating more realistic fluid models into ones calculations.\n\n\nCalculating Downstream Distances\nFor hydrogen, the hazard we are most concerned with is fires and explosions. Conveniently, we can retrieve the lower flammability limit (LFL) for hydrogen without needing to look it up ourselves.\n\nlfl = hp.FuelProperties(h2.species).LFL\n\nand calculate the distance, along the plume center-line, to the LFL\n\nstreamline_dists = jet.get_streamline_distances_to_mole_fractions([lfl])\n\nstreamline_dists[0]\n\n19.655324152591245\n\n\nsimilarly we can retrieve the x-y coordinates of the plume extent, out to the LFL\n\nmole_frac_dists = jet.get_xy_distances_to_mole_fractions([lfl])\n\nmole_frac_dists\n\n{0.04: [(0, 13.734800620965242), (0, 14.201255169630102)]}\n\n\n\n\n\n\n\n\n\nand, finally, calculate the flammable mass in the steady-state jet.\n\njet.m_flammable()\n\n0.2789535809847125\n\n\n\n\nPlotting the Plume Dispersion\nThe next obvious thing we want to do is plot the actual plume dispersion, to do this we retrieve the x-y coordinates and corresponding mass fraction (X), mole fraction (Y), velocity (v) and temperature (T) fields.\n\nx, y, X, Y, v, T = jet._contourdata\n\nwe can use matplotlib to plot the concentrations and highlight the contour corresponding to the LFL\n\n\n\n\n\n\n\n\n\n\n\nDoing it the Easy Way\nAbove I walked through the steps using the physics models included in HyRAM+, but if what you want is just the final plot and some basic parameters for QRA there is a much easier way: use the analyze_plume_jet model in the HyRAM+ API.\n\nplume = api.analyze_jet_plume(air, h2, \n                              orif_diam=d_H,\n                              rel_angle=theta,\n                              dis_coeff=c_d,\n                              nozzle_model='yuce',\n                              contours=lfl,\n                              xmin=0.0,\n                              xmax=60,\n                              ymin=0.0,\n                              ymax=75,\n                              vmin=0,\n                              vmax=2*lfl,\n                              output_dir='figures',\n                              filename='h2_plume_fig.png')\n\n\nBy default this outputs a file, instead of plotting directly into the notebook, and does not allow for as much control of the final figure. But it returns the necessary arrays if you wanted to do your own thing.\nThe mass flow-rate, distance along the streamline to the LFL, and contour of the LFL are also retrievable.\n\nplume['mass_flow_rate']\n\n0.4024272255826383\n\n\n\nplume['streamline_dists'][0]\n\n19.655324152591245\n\n\n\nplume['mole_frac_dists']\n\n{0.04: [(0, 13.734800620965242), (0, 14.201255169630102)]}\n\n\nThis does not return a Jet object, it returns a Dict with just the contours of the plume and some distances, so I am not entirely sure how one would get the explosive mass. This isn’t obviously exposed through the API either.\n\n\nLimitations\nAn important limitation, from a usability standpoint, is that there is no obvious way to retrieve the concentration for a given point. Suppose you have some coordinates x, y, z and you really need to know what the hydrogen concentration will be at that location specifically. HyRAM+ is not really set up to answer that question, or at least that functionality is not obviously exposed to the user. You could take the arrays of x and y points used for generating the plots, do a 2D-interpolation, and work it out from there, but that is kind of clunky.\nAnother limitation of this model, if it is being used for process equipment outdoors, is that there is no accounting for wind. Ambient conditions are always assumed to be quiescent. This leads to less dispersion than you would expect were wind included, and may over-estimate the degree to which the plume will rise and disperse vertically.\nAnother, more major limitation, is that there is no accounting for the ground. For large releases, near ground level, it may not be obvious that mass is being lost through the ground, and not accumulating as you would actually expect. For example, taking the above scenario and setting the release angle to 45° downward, the jet simply disappears into the earth. In reality the hydrogen should accumulate along the ground, or reflect off with some momentum. Releases near ground-level, with shallow release angles relative to horizontal, may have hazardous build-ups in areas, and that is being neglected by this model.\n\nplume = api.analyze_jet_plume(air, h2, \n                              orif_diam=d_H,\n                              rel_angle=-theta,\n                              dis_coeff=c_d,\n                              nozzle_model='yuce',\n                              contours=lfl,\n                              xmin=0,\n                              xmax=60,\n                              ymin=-65,\n                              ymax=10,\n                              vmin=0,\n                              vmax=2*lfl,\n                              output_dir='figures',\n                              filename='h2_downward_plume_fig.png')\n\n\nFor integral plume dispersion models, like this one, it is typical to restrict the plume center-line such that it cannot extend below the ground, i.e. any integration step that would have a center-line with y&lt;0 is rejected and replaced with one with y≥0. It is also common to implement ground reflection where the plume dispersion “bounces off” the ground, perfectly elastically.\nPreventing the plume from passing through the ground is strictly necessary for denser than air models, for example DEGADIS, as the plume naturally falls to ground level and rolls along it. That perhaps explains why HyRAM+ doesn’t implement this, the plume will naturally rise away from the ground due to the relative density of hydrogen. However, since HyRAM+ assumes the plume is on the ground by default, this strikes me as a significant trap for users. Shallow release angles will have non-physical results in the immediate vicinity of the jet."
  },
  {
    "objectID": "posts/hydrogen_release_modeling/index.html#indoor-accumulation",
    "href": "posts/hydrogen_release_modeling/index.html#indoor-accumulation",
    "title": "Modelling Hydrogen Releases Using HyRAM+",
    "section": "Indoor Accumulation",
    "text": "Indoor Accumulation\nAn important feature of this tool is that it allows one to easily model the accumulation of a buoyant layer along the ceiling in an enclosed space. Suppose, to continue the example, this happened in my workroom, which for the sake of simplicity is just a 4m × 4m room with 2.7m (9ft) ceiling. While the explosive mass in the steady state plume is pretty small, the lfl extent of the unconfined plume extends much further than the walls of my room. The hydrogen will hit the far wall and accumulate quite significantly.\n\n\n\n\n\n\nFigure 2: A sketch of the room, the cylinder is supposed to have fallen by one wall. A layer of hydrogen gas accumulates at the ceiling, with the boundary moving downward as more hydrogen accumulates.\n\n\n\n\nCylinder Blowdown\nThe blowdown of the hydrogen cylinder will take some time and the exact blowdown curve is necessary for determining how rapidly the hydrogen will accumulate in the room. Assuming the jet it at the initial steady state mass-rate throughout will be very conservative and the entire contents of the cylinder will be gone within a few seconds.\nHyRAM+ uses the governing equations for adiabatic blow-down.\n\\[ \\frac{dm}{dt} = -c_d A \\rho v \\]\n\\[ \\frac{du}{dt} = \\frac{1}{m} \\frac{dm}{dt} \\left( h - u \\right) \\]\nwhere m is the mass remaining in the tank, u is the specific internal energy, h the specific enthalpy, and v the velocity through an isentropic nozzle. The thermodynamic state variables (P, T) are recovered from the fluid model and the internal energy, u, and the density ρ = m/V.\nFirst the cylinder is defined as a Source with an initial mass of hydrogen.\n\nm_h2 = 50 #kg\ncylinder = hp.Source.fromMass(m_h2,h2)\n\nThen the cylinder can be blown down through the orifice previously defined. This numerically integrates the governing equations and returns the mass, pressure, temperature, and flowrate as functions of time.\nA convenience function can also plot them for us.\n\ncylinder.empty(orifice)\n\ncylinder.plot_time_to_empty()\n\n\n\n\nThe Indoor Release\nAt this point the release is still unconfined. We need to define the room. This also includes defining the location of vents. Since no room is perfectly leak free, and for the sake of an example, I assume a similar leak area for a vent near the ceiling and one near the floor. I also define the cylinder as leaking from ground level essentially at one wall and aimed 45° upwards towards the opposite wall.\n\nceiling_height = 2.7 #m\nfloor_area = 16 #m^2\nrelease_height = 0 #m\n\nceiling_vent = hp.Vent(A_h,2.6)\nfloor_vent = hp.Vent(A_h,0.01)\n\nroom = hp.Enclosure(ceiling_height, floor_area, release_height, ceiling_vent, floor_vent, Xwall=4)\n\nThe release model then estimates the accumulation of a flammable layer starting at the roof and extending downwards.\n\nrelease = hp.IndoorRelease(source=cylinder,\n                           orifice=orifice,\n                           ambient=air,\n                           enclosure=room,\n                           theta0=theta,\n                           nn_conserve_momentum=True,\n                           nn_T='solve_energy',\n                           tmax=30,\n                           verbose=False)\n\nrelease.plot_mass()\n\n\nAt this point I thought, initially, that there was something wrong with the code. Why does the flammable mass suddenly disappear? Where does it go? Nowhere. HyRAM+ by default assumes the flammable mass is between the LFL and UFL. At around 15 seconds the room is essentially saturated with hydrogen and above the UFL, hence why it suddenly disappears. This can be seen by plotting the flammable layer at the ceiling.\n\nrelease.plot_layer()\n\n\nBy 15 seconds the layer reaches the floor and the entire room is above the UFL. Some recommend not cutting off at the UFL, this room is still quite hazardous, say if someone opened a door there would be an explosive mixture right in the door-frame that could explode and that explosion would mix with the rest of the gas and the whole mass of released hydrogen could explode.\nTo consider the flammable mass to be the mixed area above the LFL and include areas that exceed the UFL, the X_lean and X_rich keyword arguments must be used.\n\nfull_release = hp.IndoorRelease(source=cylinder,\n                                orifice=orifice,\n                                ambient=air,\n                                enclosure=room,\n                                theta0=theta,        \n                                X_lean=lfl,\n                                X_rich=1.0,\n                                nn_conserve_momentum=True,\n                                nn_T='solve_energy',\n                                tmax=30,\n                                verbose=False)\n\nfull_release.plot_mass()\n\n\n\n\nUsing the HyRAM+ API\nThis whole analysis can also be accomplished using the API and the analyze_accumulation function. It outputs a series of plots into a given folder, including calculating the jet trajectories for a series of user selected times.\n\napi.analyze_accumulation(amb_fluid=air,\n                         rel_fluid=h2,\n                         tank_volume=cylinder.V, \n                         orif_diam=d_H, \n                         rel_height=release_height,\n                         enclos_height=ceiling_height, \n                         floor_ceil_area=floor_area,\n                         ceil_vent_xarea=A_h, \n                         ceil_vent_height=2.6,\n                         floor_vent_xarea=A_h, \n                         floor_vent_height=0.01,\n                         dist_rel_to_wall=4.0,\n                         tmax=30,\n                         times=[1,5,15,20,25,30],\n                         orif_dis_coeff=c_d,\n                         rel_angle=theta,\n                         nozzle_key='yuce',\n                         output_dir=\"figures/accumulation\")\n\n{'status': 1,\n 'pressures_per_time': array([ 388375.73527918, 3092893.44785494,       0.        ,\n              0.        ,       0.        ,       0.        ]),\n 'depths': array([1.34306015, 2.41226448, 2.45803334, 2.45809352, 2.4631503 ,\n        2.46359848]),\n 'concentrations': array([18.58049761, 43.85233797, 81.37521187, 89.24254417, 93.32644083,\n        95.79735125]),\n 'overpressure': 8136855.548411997,\n 'time_of_overp': 11.970055170861283,\n 'mass_flow_rates': array([0.3978773 , 0.38039983, 0.3417355 , 0.32307435, 0.30734882,\n        0.29251744]),\n 'pres_plot_filepath': 'figures/accumulation/pressure_plot_20240921-132132.png',\n 'mass_plot_filepath': 'figures/accumulation/flam_mass_plot_20240921-132132.png',\n 'layer_plot_filepath': 'figures/accumulation/layer_plot_20240921-132132.png',\n 'trajectory_plot_filepath': 'figures/accumulation/trajectory_plot_20240921-132132.png',\n 'mass_flow_plot_filepath': 'figures/accumulation/time-to-empty_20240921-132132.png'}\n\n\n\n\n\n\n\n\n\nLimitations\nOne limitation that stuck out to me, in the blowdown model, is that the blowdown is either at a constant heat flux or adiabatic (which is constant at zero). Blowdown models where the vessel is isothermal are fairly typical, especially for large (un-insulated) vessels blowing down through a small valve. For small vessels, assuming an adiabatic blowdown is reasonable, but this limits the model as the vessels get larger."
  },
  {
    "objectID": "posts/hydrogen_release_modeling/index.html#final-thoughts",
    "href": "posts/hydrogen_release_modeling/index.html#final-thoughts",
    "title": "Modelling Hydrogen Releases Using HyRAM+",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nIf you are looking for a quick and easy-to-use tool for performing hazard analysis of hydrogen releases, HyRAM+ is worth checking out. I haven’t gone into it here, but the tool allows you to continue the analysis into blast over-pressure and a much more fully featured QRA. They python library allows you to split out each piece of the model, allowing you to really explore what it is doing, but also making it easier to pull out relevant pieces for comparison to other hazard analysis tools one might be using in a plant setting.\nThe indoor accumulation model is worth exploring, for sites that are transitioning to hydrogen, as most “screening level” indoor accumulation models I have seen consider either a heavier-than-air layer along the ground or the entire indoor space (or zone, if it divides the area into zones) having one, fully-mixed, concentration. So it is possible that the standard plant tools for, e.g., LOPA may have blind spots for the unique hazards that hydrogen can present (e.g. accumulating as a flammable layer along the ceiling).\nFor chemical plants that also operate a cogen, they may already have hydrogen venting into the air in an indoor space: large turbines (&gt;60MW) typically use hydrogen coolant. It might be worthwhile running a HyRAM+ model for that venting just to confirm that the small quantities vented into the turbine hall are not an issue. While this may be a well known and well understood aspect of turbine operations, for people in the power business, process safety engineers tend to gasp and clutch their chests when told of routine venting of flammable and explosive gases into enclosed spaces."
  },
  {
    "objectID": "posts/hydrogen_release_modeling/index.html#references",
    "href": "posts/hydrogen_release_modeling/index.html#references",
    "title": "Modelling Hydrogen Releases Using HyRAM+",
    "section": "References",
    "text": "References\n\n\nAIChE/CCPS. Guidelines for Consequence Analysis of Chemical Releases. New York: American Institute of Chemical Engineers, 1999.\n\n\nEhrhart, Brian D., Ethan S. Hecht, and Benjamin B. Schroeder. Hydrogen Plus Other Alternative Fuels Risk Assessment Models (HyRAM+) Version 5.1 Technical Reference Manual, 2023. https://doi.org/10.2172/2369637.\n\n\nEhrhart, Brian D., Cianan Sims, Ethan S. Hecht, Benjamin B. Schroeder, Benjamin R. Liu, Katrina M. Groth, John T. Reynolds, and Gregory W. Walkup. “HyRAM+ (Hydrogen Plus Other Alternative Fuels Risk Assessment Models).” Sandia National Laboratories, February 8, 2024. https://hyram.sandia.gov.\n\n\nOoms, G. “A New Method for the Calculation of the Plume Path of Gases Emitted by a Stack.” Atmospheric Environment (1967) 6, no. 12 (1972): 899–909. https://doi.org/10.1016/0004-6981(72)90098-4."
  },
  {
    "objectID": "posts/smoke_days/index.html",
    "href": "posts/smoke_days/index.html",
    "title": "Smoke Days",
    "section": "",
    "text": "Recently wildfire smoke has returned and blanketed the city in haze, causing the air quality health index (AQHI) to sky rocket, and as a result I’ve been spending a lot more time inside. This feels like it is a lot more common event than it used to be, but I’m not sure if that’s true or if it merely feels true because I’m looking out at a hazy skyline.\nI would like to look into this more using air quality data and see if this truly is a recent change, or maybe Edmonton has always been like this and I’ve simply forgotten."
  },
  {
    "objectID": "posts/smoke_days/index.html#particulates-as-a-proxy",
    "href": "posts/smoke_days/index.html#particulates-as-a-proxy",
    "title": "Smoke Days",
    "section": "Particulates as a proxy",
    "text": "Particulates as a proxy\nAlberta has a series of air quality monitoring stations set up around the province and I can pull a data-set from the Edmonton Central station (the closest one to me) and look at airborne particulates (pm2.5) as a proxy for wildfire smoke. Though the smoke itself is more than just particulates &lt;2.5μm in diameter, it is those particulates that cause the AQHI to rise significantly.\nHowever there are more sources of pm2.5 than just wildfires, vehicles are a major source for one, and in the winter atmospheric inversions can lead to really poor air quality during which time the pm2.5 concentration rises. Additionally farmers around the city often burn stubble and other stuff in the fall, leading to smoke days that have nothing to do with wildfires.\nSo this is a proxy for wildfire smoke, but not a great one.\n\nAmbient Air Data\nI downloaded just the pm2.5 measurements for Edmonton Central from October 2000, the earliest reported values, through to the end of December 2020, the latest values in the database at this time, from Alberta’s Ambient Air Data Warehouse. This is a csv with 177,072 rows of data and several columns each corresponding to, I’m guessing, a different instrument. Over time the station has swapped out instruments for measuring pm2.5s and those are recorded as a different measurement type.\n\nusing CSV, DataFrames, Dates, Pipe, Plots\n\n\ndata_file = \"data/Long Term pm2.5 Edmonton Central.csv\"\n\nambient_data = @pipe data_file |&gt;\n    CSV.File( _ ; \n             dateformat=\"mm/dd/yyyy HH:MM:SS\", \n             types=[DateTime, DateTime, Float64, Float64, Float64, Float64], \n             header=16, silencewarnings=true) |&gt;\n    DataFrame(_);\n\n\n\n\n177072×6 DataFrame\n\n6×6 DataFrame\n\n Row │ IntervalStart        IntervalEnd          MeasurementValue  MeasurementValue_1  MeasurementValue_2  MeasurementValue_3 \n\n     │ DateTime             DateTime             Float64?          Float64?            Float64?            Float64?           \n\n─────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n   1 │ 2000-10-20T00:00:00  2000-10-20T00:59:00           missing             missing             missing                 2.3\n\n   2 │ 2000-10-20T01:00:00  2000-10-20T01:59:00           missing             missing             missing                 1.8\n\n   3 │ 2000-10-20T02:00:00  2000-10-20T02:59:00           missing             missing             missing                 1.8\n\n   4 │ 2000-10-20T03:00:00  2000-10-20T03:59:00           missing             missing             missing                 1.0\n\n   5 │ 2000-10-20T04:00:00  2000-10-20T04:59:00           missing             missing             missing                 1.8\n\n   6 │ 2000-10-20T05:00:00  2000-10-20T05:59:00           missing             missing             missing                 2.0\n\n\n\n\n\nParsing the Data\nWhat I want to know is when smoke events happened and how long they were. To estimate that I am going to assume a smoke event is a period in which the hourly pm2.5 exceed the ambient air quality guideline for pm2.5s. The event starts at the first hour greater than that limit and ends on the first hour less than that limit. This has the obvious weakness that sometimes the clouds of wildfire smoke has breaks in it, so what feels like a week long smoke event would end up as a series of smaller events as the pm2.5 count might dip overnight or something. But this is a start.\nOne complication is that the dataset has four columns of pm2.5 data that are full of mostly missing values since they each only correspond to the period in which the given instrument is running. So I first need to collect those measurement values, drop the missing values, and take the mean of what remains. I assume there is no overlap and so it’s the mean of one number, but I haven’t checked to see if that’s true and the mean seems like the most sensible thing to do if there is overlap.\nIf there is a missing hour entirely, i.e. no instrument has a reading, then I skip it. That neither counts as the start nor the end of a smoke event and I move to the next row.\n\nusing Statistics\n\nlim_1h = 80.0  #μg/m³ 1-hr limit\nlim_24h = 29.0 #μg/m³ 24-hr limit\n\nfunction exceedences(df; limit=lim_1h)\n\n    results = DataFrame(start_date = DateTime[], end_date = DateTime[], month = Int64[], year = Int64[], duration = Float64[], max_conc = Float64[])\n\n    flag = false\n    start_date = nothing\n    end_date = nothing\n    max_conc = 0.0\n\n    for r in eachrow(ambient_data)\n        measurements = [r[:MeasurementValue], r[:MeasurementValue_1], r[:MeasurementValue_2], r[:MeasurementValue_3]]\n        measurements = collect( skipmissing(measurements) )\n        conc = if (sizeof(measurements)&gt;0) mean(measurements) else missing end\n\n        if typeof(conc) == Missing\n            # ignore missing data\n        elseif conc &gt; limit\n            if flag == true            # we are already in a sequence\n                end_date = r[:IntervalEnd]\n                max_conc = max(conc, max_conc)\n            else                       # we are starting a sequence\n                flag = true\n                start_date = r[:IntervalStart]\n                end_date = r[:IntervalEnd]\n                max_conc = max(conc, max_conc)\n            end\n        else\n            if flag == true           # we are ending a sequence\n                flag = false\n                duration = Dates.value.(end_date - start_date)/3.6e6\n                push!(results, [start_date, end_date, month(start_date), year(start_date), duration, max_conc])\n                max_conc = 0.0\n            end\n        end\n    end\n    \n    return results\nend\n\n\nresult_1hr = exceedences(ambient_data, limit=lim_1h)\n\n\n\n\nResults: 53×6 DataFrame\n\n\n\nSummary: \n\n6×7 DataFrame\n\n Row │ variable    mean     min                  median   max                  ⋯\n\n     │ Symbol      Union…   Any                  Union…   Any                  ⋯\n\n─────┼──────────────────────────────────────────────────────────────────────────\n\n   1 │ start_date           2001-05-24T11:00:00           2019-05-31T15:00:00  ⋯\n\n   2 │ end_date             2001-05-24T12:59:00           2019-05-31T15:59:00\n\n   3 │ month       5.84906  1                    7.0      12\n\n   4 │ year        2011.15  2001                 2010.0   2019\n\n   5 │ duration    3.85126  0.983333             1.98333  26.9833              ⋯\n\n   6 │ max_conc    135.9    80.3                 93.9     867.0\n\n                                                               2 columns omitted\n\n\n\nOver the past 20yrs there were 53 periods with the pm2.5 concentration above the limit, these range from 1hr to 27hrs long and a max concentration observed of 867μg/m³"
  },
  {
    "objectID": "posts/smoke_days/index.html#results",
    "href": "posts/smoke_days/index.html#results",
    "title": "Smoke Days",
    "section": "Results",
    "text": "Results\nA plot of the results, showing each period in excess of the hourly limit and the duration of that period, is very suggestive that these are becoming more frequent events. If we also plot the maximum hourly concentration observed it appears that the extreme smoke days are a more recent phenomenon. Though with the big caveat that the data only goes back 20 years, it could be that the period between 2000 and 2010 was an abnormally smoke-less period.\n\n\n\n\n\n\n\n\nFigure 1: Duration of AAQO exceedances in Edmonton from 2001 through 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Observed concentrations for AAQO exceedances in Edmonton, 2001 through 2019\n\n\n\n\n\nThe plots below aggregate the events by year, and it certainly seems like smoke events are becoming more frequent and lasting longer, with more time spent in haze than in the early 2000s. But there are notable years such as 2010 and 2018 which could simply be outliers. Interestingly the most notable, in the news, years for wildfires are not obvious ones here – the Slave Lake fire of 2011 and Ft. McMurray fire of 2016. I think it is often the case that the wildfire smoke in Alberta has less to do with fires in Alberta itself and more to do with smoke being carried in from neighbouring states and provinces. That is certainly true now when the major wildfires are in BC, northern Saskatchewan, and northern Manitoba.\n\n\n\n\n\n\n\n\nFigure 3: Frequency of smoke days in Edmonton, 2001 through 2019\n\n\n\n\n\nOne thing these plots may mask is that while the median duration perhaps hasn’t changed much, it’s clear from the scatter plots that the outlier periods are more common in the last decade than the one preceding it.\n\n\n\n\n\n\n\n\nFigure 4: Frequency of smoke days exceeding 5 hours in duration, Edmonton 2001-2019.\n\n\n\n\n\nWhen grouped by month, we can see winter months have notable representation, which is likely those atmospheric inversions trapping pollutants near ground level, but the summer months appears to be when the hazy periods are longest and that likely corresponds to wildfire smoke.\n\n\n\n\n\n\n\n\nFigure 5: Frequency of smoke events by month, Edmonton 2001-2019\n\n\n\n\n\nFiltering out only the extended periods, with a duration &gt;5 hrs, we see that prolonged periods of excess pm2.5 appears to be a summer phenomena, especially August. Which is certainly consistent with my experience of noticeable smokey days, corresponding with wildfire season.\n\n\n\n\n\n\n\n\nFigure 6: Frequency of smoke events exceeding 5 hours in duration by month, Edmonton 2001-2019"
  },
  {
    "objectID": "posts/smoke_days/index.html#limitations-and-opportunities",
    "href": "posts/smoke_days/index.html#limitations-and-opportunities",
    "title": "Smoke Days",
    "section": "Limitations and Opportunities",
    "text": "Limitations and Opportunities\nAn opportunity for further analysis would be to look for correlations between pm2.5 and other pollutants, say NOx, to allow one to exclude pm2.5s from vehicle emissions. That would still leave road dust, construction dust, and just farmers burning stubble, but I imagine that would go pretty far in terms of removing unrelated bad air quality days from the dataset. If one was only concerned with the most extreme cases, when the sky turns orange and visibility drops to only a few blocks, well that’s visible from space and could presumably be pulled out a dataset of satellite images, taking care to distinguish smoke from cloud cover.\nI would like to see a longer dataset. The dataset I was looking at was relatively short, only twenty years, which doesn’t allow me to answer the question of whether or not extended periods of wildfire smoke is truly a recent phenomenon versus a “return to normal”, i.e. it could be that 2000-2010 were the abnormal years and that is equally consistent with this dataset. Just looking around the air data warehouse it doesn’t look like this kind of air analysis was routine before the 2000s, but I could simply be ignorant of some other studies or datasets.\nFinally I picked pm2.5s since that is the variable responsible for the high risk AQHI levels, but there could be much better proxies for wildfire smoke that have better datasets. I can’t think of anything off the top of my head, but I’m a chemical engineer not an air quality expert."
  },
  {
    "objectID": "posts/Britter-McQuaid/index.html",
    "href": "posts/Britter-McQuaid/index.html",
    "title": "Taking a second look at the Britter-McQuaid model",
    "section": "",
    "text": "I recently spent some time looking in detail at the Britter-McQuaid workbook model for dense gas dispersion and I thought the plume model deserved some extra attention. Firstly because I believe there is an error in the plume dimensions, and secondly because I think an important feature of top-hat models is often neglected and the Britter-McQuaid workbook model should be used more.\nAs a re-cap the Britter-McQuaid model1 is a series of correlations for the dispersion of denser than air gases. These are given as a series of correlation curves and the typical procedure is to interpolate the downwind distance to the concentration of interest, for example to the Lower Flammability Limit (LFL). The model also gives some equations for estimating the plume horizontal and vertical dimensions, where conventional practice is to assume the plume has a rectangular cross-section and a uniform concentration."
  },
  {
    "objectID": "posts/Britter-McQuaid/index.html#a-motivating-example",
    "href": "posts/Britter-McQuaid/index.html#a-motivating-example",
    "title": "Taking a second look at the Britter-McQuaid model",
    "section": "A motivating example",
    "text": "A motivating example\nJust to have some numbers to look at, I am going to use a scenario adapted from the Burro series of trials of LNG dispersion.2 The release conditions are:\n2 AIChE/CCPS, Guidelines for Consequence Analysis of Chemical Releases., 122.\nrelease temperature: -162°C\nrelease rate: 0.23 m³/s (liquid)\nrelease duration: 174 s\nwindspeed at 10m: 10.9 m/s\nLNG liquid density (at release conditions): 425.6 kg/m³\nLNG gas density (at release conditions): 1.76 kg/m³\n\nThe goal is to find the distance to the lower flammability limit (LFL) which is 5%(v/v) and ultimately work out the extent of the plume and total explosive mass.\n\nusing Unitful\n\nTₐ  = 288.15u\"K\"      # ambient air temperature \nρₐ  = 1.225u\"kg/m^3\"  # density of air at 15°C and 1atm\nu₁₀ = 10.9u\"m/s\"      # windspeed at 10m \n\nρₗ  = 425.6u\"kg/m^3\"  # liquid density of LNG, given\nρᵥ  = 1.76u\"kg/m^3\"   # vapour density of LNG, given\nṁ   = ρₗ*0.23u\"m^3/s\" # mass release rate\nTᵣ  = (273.15 - 162)u\"K\" # boiling point of LNG, given\nLFL = 0.05 # lower flammability limit, volume fraction\n\nQₒ = ṁ/ρᵥ # gas volumetric flowrate: mass flowrate divided by gas density\n\nFirst calculate the critical length, D, and the dimensionless parameter α for the model\n\nD = √(Qₒ/u₁₀)\n\ng = 9.806u\"m/s^2\"\ngₒ = g * (ρᵥ - ρₐ )/ ρₐ\nα = 0.2*log10(gₒ^2 * Qₒ / u₁₀^5)\n\nThen, using digitized curves,3 work out the points for the linear interpolation in terms of \\(\\beta = \\log_{10}(x/D)\\)\n3 AIChE/CCPS, 118.\nCs = [ 0.1,         0.05,        0.02,        0.01,        0.005,       0.002]\nβs = [ 0.24*α+1.88, 0.36*α+2.16, 0.45*α+2.39, 0.49*α+2.59, 0.59*α+2.80, 0.39*α+2.87]\n\nThese points only cover the middle region of the concentration curve, where the concentration ratio, \\({ c_m \\over c_0 }\\), is between 0.1 and 0.002, there is a near-field correlation that needs to be connected for concentration ratios &gt;0.1\n\nfunction Cm_nf(x′)\n    if x′ &gt; 0\n        return 306/(306 + x′^2)\n    else\n        return 1.0\n    end\nend\n\nxnf = 30\nβnf = log10(xnf)\nCnf = Cm_nf(xnf)\n\nAnd a far field correlation for when the concentration ratio is &lt;0.002 which is basically just continuing the curve from the last point but such that the concentration decays with 1/x2\n\nxff = 10^(maximum(βs))\nA = minimum(Cs)*xff^2\n\nfunction Cm_ff(x′; A=A)\n    return A/x′^2\nend\n\nFinally, putting together the pieces: near field correlation, a linear interpolation for the middle of the concentration curve, and a far field correlation, to form the complete concentration function, along with a correction for non-isothermal releases (of which this is an example)\n\nusing Interpolations\n\nitp = interpolate( ([βnf; βs],), [Cnf; Cs], Gridded(Linear()) )\n\n\nfunction Cm(x::Quantity; xnf=xnf, xff=xff, D=D, T′=Tᵣ/Tₐ)\n    x′ = x/D\n    c′ = if x′ &lt; xnf\n        Cm_nf(x′)\n    elseif xnf ≤ x′ &lt; xff\n        itp(log10(x′))\n    else\n        Cm_ff(x′)\n    end\n    \n    c = c′ / (c′ + (1 - c′)*T′)\n    \n    return c\nend\n\n\n\n\n\n\n\n\n\nFigure 1: The concentration profile for the Britter-McQuaid dense gas model, with the LFL shown for reference.\n\n\n\n\n\nIf all one needs is the distance to the LFL there is an easier way of doing this: interpolate the concentrations to find the β corresponding to the LFL (after applying the non-isothermal correction). However, if one also requires the plume dimensions the concentration profile is required.\nFrom the concentration profile calculating the downwind distance to the LFL is very straight-forward.\n\nusing Roots\n\nxn = find_zero((x) -&gt; Cm(x) - LFL, (300,400).*1u\"m\", Roots.Brent())\n\n354.5630187009715 m"
  },
  {
    "objectID": "posts/Britter-McQuaid/index.html#looking-again-at-plume-dimensions",
    "href": "posts/Britter-McQuaid/index.html#looking-again-at-plume-dimensions",
    "title": "Taking a second look at the Britter-McQuaid model",
    "section": "Looking again at plume dimensions",
    "text": "Looking again at plume dimensions\nAt first glance the workbook seems to be giving the user everything they need to workout the size of the plume, giving the following diagram\n\n\n\n\n\n\nFigure 2: Dense plume concentration contour.4\n\n4 Britter and McQuaid, “Workbook on the Dispersion of Dense Gases,” fig. 10.\n\nand the following relations for the labeled distances\n\\[ L_U = {D \\over 2} + 2 l_b \\]\n\\[ L_{Ho} = D + 8 l_b \\]\n\\[ L_H = L_{Ho} + 2.5 \\sqrt[3]{ l_b x^2 } \\]\nwith the buoyancy scale lb defined as \\[ l_b = { { g_o Q_o } \\over u_{ref}^{3} } \\]\n\nlb = (gₒ*Qₒ)/u₁₀^3\n\n0.18392758812310803 m\n\n\n\nLᵤ  = D/2 + 2lb\n\n1.4973003373658906 m\n\n\n\nLₕₒ = D + 8lb\n\n3.7303110272242135 m\n\n\n\nLₕ(x) = Lₕₒ + 2.5∛(lb*x^2)\n\n\nUpwind region\nThe curve given for LH for x &gt; 0 is not the curve for x &lt; 0, the upwind extent of the plume. This is the blue curve in the figure below. The orange curve is slightly adjusting LH such that for x &lt; 0 the second term is subtracted (so the curve actually converges to zero instead of blowing up to +∞ as x → -∞). The black dots are points taken from the diagram given by Britter and McQuaid, using a graph digitizer and scaling to the actual LHo and LU. Clearly the given curve for LH is not at all what is shown in the diagram for the upwind region.\nA conservative approach to estimating the size of the upwind extent is to assume LH = LHo for LU &lt; x &lt; 0, i.e. making the upwind region a rectangle of width LHo and length LU.5 This is the green curve in the figure below.\n5 Bakkum and Duijm., “Vapour Cloud Dispersion”.Alternatively one could “fit” a curve to hit the end points while also having the same power of x: \\(L_H = L_{Ho} \\left( {x + L_U} \\over L_U \\right)^{2/3}\\) where LU &lt; x &lt; 0, this at least retains the same general shape and is the red curve in the figure below. I think this should be taken with the giant caveat that I don’t know if insisting on the same power law is truly justified.\n\n\n\n\n\n\n\n\nFigure 3: The various approaches to estimating the upwind plume extent, black dots are a digitization of the corresponding diagram from Britter and McQuaid shown for reference.\n\n\n\n\n\nFor most typical cases I would think the upwind region would be a small component of the overall plume and taking the conservative, rectangle, approach would be a small error.\n\n\nVertical extent\nThe vertical extent is not given on the diagram, but an equation is given in the text, with the note that this comes from continuity, however I think this is incorrect.\n\\[ L_V = {Q_o \\over {u_{ref} L_H} } = { D^2 \\over L_H } \\]\nSuppose a steady state plume with a system boundary such that the plume is sliced along the y-z plane at some downwind distance x. All of the mass entering the plume, from the source, exits the plume through this plane\n\n\n\nimage.png\n\n\nConsider the steady state mass balance\n\\[ \\textrm{mass in} = \\textrm{mass out} \\]\n\\[ c_o Q_o = \\iint_A c u \\,dA = \\int_{0}^{\\infty} \\int_{-\\infty}^{\\infty} c(x,y,z) u(x,y,z) \\,dy \\,dz \\]\nBy the nature of a top-hat model the plume cross section is a rectangle with half-width LH and height LV and the concentration everywhere inside the rectangle is cm. Assuming a constant advection velocity, u, the integral can be simplified to\n\\[ \\iint_A c u \\,dA = c_m u \\int_{0}^{L_V} \\int_{-L_H}^{L_H} \\,dy \\,dz = 2 c_m u L_H L_V \\]\nThe steady state mass balance is then\n\\[ c_o Q_o = 2 c_m u L_H L_V \\]\nand the vertical extent can be solved for with some simple re-arrangement\n\\[ L_V = { { c_o Q_o } \\over { 2 c_m u L_H } } = {1 \\over 2}{ c_o \\over c_m } { Q_o \\over {u L_H} } \\]\nSetting the advection velocity of the plume to the reference windspeed gives\n\\[ L_V = {1 \\over 2}{ c_o \\over c_m } { Q_o \\over {u_{ref} L_H} } = {1 \\over 2} { c_o \\over c_m } { D^2 \\over L_H }\\]\n\nLᵥ(x) = D^2/(2*Cm(x)*Lₕ(x))\n\nThis is definitely similar to what is given by Britter and McQuaid but with two big differences:\n\nit depends upon the concentration\nit is divided by two\n\nThe last point could equally be a mistake in the diagram (I have no real way of checking) as while the diagram shows LH as the plume half-width, the text simply refers to it as the “lateral plume extent”, which is ambiguous – do they mean the entire lateral extent or from the center-line of the plume?\nThe TNO Yellow Book gives a different equation6 for the vertical extent:\n6 Bakkum and Duijm. equation 4.104.\\[ L_V = {1 \\over 2} { Q_o \\over {u_{ref} L_H} } = {1 \\over 2} { D^2 \\over L_H }\\]\nWhich clearly follows from assuming LH is the half-width, and the corresponding figure is labeled as such (using the same equation for LH as Britter and McQuaid). But it doesn’t depend upon concentration.\nI think the vertical extent has to depend upon the concentration as otherwise mass will simply disappear from the plume as it extends downwind. There is also the obvious problem that since the plume lateral extent monotonically increases, and the vertical extent is inversely related to it, the vertical extent is monotonically decreasing. In fact it becomes vanishingly small quite quickly. This entirely the opposite of what is observed with actual dense plume dispersion.\nThis can be seen most clearly in the following figure in which the vertical extent is shown as a function of downwind distance along with the mass flowrate in the plume (i.e. \\(c_m u A\\) )\n\n\n\n\n\n\n\n\nFigure 4: Approaches to plume height estimation (top) and the corresponding conservation of mass (bottom).\n\n\n\n\n\nI think it is fairly obvious that both the Britter-McQuaid and TNO models give silly answers for the vertical extent. Though the corrected curve, the green curve, clearly has problems too: it has an odd bumpiness, as a result of the linear interpolation, and it is also too small due to both assuming the concentration everywhere is equal to the ground level concentration and due to an overly large advection velocity (the windspeed at 10m is quite a bit higher than the windspeed at ~1m).\nAn alternative approach to using the reference windspeed as the advection velocity is to assume the advection velocity is some constant fraction of the reference velocity, e.g. \\(u = 0.4 u_{ref}\\), which is what Britter and McQuaid use for the instantaneous model.\nAnother alternative might be to use an average windspeed, ū over cross-section of the plume as the advection velocity, assuming windspeed is only a function of height.\n\\[ \\bar{u} = { { \\iint_A u \\,dA } \\over A } =  { { \\int_{0}^{L_V} u(z) \\,dz } \\over L_V } \\]\nAssuming the windspeed follows a powerlaw distribution \\(u = u_{ref} \\left( z \\over z_{ref} \\right)^p\\) gives\n\\[ \\bar{u} = { { \\int_{0}^{L_V} u(z) \\,dz } \\over L_V } \\]\n\\[ = {1 \\over L_V} \\int_{0}^{L_V} u_{ref} \\left( z \\over z_{ref} \\right)^p \\,dz \\]\n\\[ = { u_{ref} \\over {p+1} } \\left( L_V \\over z_{ref} \\right)^p \\]\nplugging it into the simple mass balance\n\\[ c_o Q_o = c_m \\bar{u} A \\]\n\\[ = c_m {u_{ref} \\over {p+1} } \\left( L_V \\over z_{ref} \\right)^p { 2 L_H L_V } \\]\nre-arranging to solve for LV\n\\[ L_V = \\left( { {p+1} \\over 2 } { c_o \\over c_m } z_{ref}^p {Q_o \\over {u_{ref} L_H} } \\right)^{1 \\over {p+1} } \\]\n\\[ = \\left( { {p+1} \\over 2 } { c_o \\over c_m } z_{ref}^p {D^2 \\over L_H } \\right)^{1 \\over {p+1} }\\]\nThe red curve in the figure above is this model, using p = 0.15.7\n7 AIChE/CCPS, Guidelines for Consequence Analysis of Chemical Releases., 83.This could also be done using the logarithmic windspeed curve \\(u = {u_{\\star} \\over \\kappa} \\log \\left( z \\over z_) \\right)\\) where \\(u_{\\star}\\) is the friction velocity and z0 is the roughness length. Though I don’t imagine the expression would work out as nicely.\n\n\nRecommendations\nFor the upwind region, assuming a simple rectangular prism with length LU, width 2LHo, height LVo and uniform concentration co is a conservative approach. Likely the plume downwind of the source will be much larger than the upwind area and so this will be a small overestimate.\nThe simple mass balance approach to calculating the plume height is a reasonable approach if one simply wants to reference Britter and McQuaid and not have to justify additional assumptions. It is not what is given in the text, but it is what is described in the text. The other models for plume height may be more realistic, in the sense that they represent more realistic advection velocities, and will give larger explosive masses for the plume, however they have not been validated against any actual data. That validation may be a worthwhile exercise but is well beyond the scope of this blog post."
  },
  {
    "objectID": "posts/Britter-McQuaid/index.html#calculating-the-explosive-mass",
    "href": "posts/Britter-McQuaid/index.html#calculating-the-explosive-mass",
    "title": "Taking a second look at the Britter-McQuaid model",
    "section": "Calculating the explosive mass",
    "text": "Calculating the explosive mass\nThe explosive mass in the cloud is the given by the volume integral\n\\[ m_e = \\iiint_V c dV \\]\nwhere V is defined as the region where c ≥ LFL.8\n8 Some sources recommend 1/2 LFL.Using the concentration profile and the plume extents, we could work out the function c(x,y,z) such that the concentration is returned if we are:\n\nwithin the plume, and\nthe concentration is ≥ LFL\n\nTo determine the explosive mass in the downwind region this might be done by the following\n\ncₒ = ustrip(u\"kg/m^3\", ṁ/Qₒ)\n\nLₕ(x::Number) = ustrip(u\"m\", Lₕ(x*1u\"m\"))\nLᵥ(x::Number) = ustrip(u\"m\", D)^2/(2*Cm(x)*Lₕ(x))\n\nfunction c(x,y,z; lim=LFL)\n    c_ = Cm(x)\n    \n    if c_ ≥ lim\n        if (abs(y) ≤ Lₕ(x)) && (z ≤ Lᵥ(x))\n            return cₒ*c_\n        else\n            return 0.0\n        end\n    else\n        return 0.0\n    end\nend\n\nusing HCubature: hcubature\n\nx_min, x_max = 0, xn\ny_min, y_max = -Lₕ(xn), Lₕ(xn)\nz_min, z_max = 0, Lᵥ(xn)\n\nm_e, err = hcubature( c, [x_min, y_min, z_min], [x_max, y_max, z_max])\nThis is a pretty tedious integration, is very inefficient, and doesn’t take into account any of the structure of the model and it turns out that a top-hat model has some pretty convenient structure.\n\nA nice property of top hat models\nReturning to the integral for the explosive mass, the plume can be divided into an upwind region (x &lt; 0) and a downwind region (x ≥ 0)\n\\[ m_e = \\iiint_V c \\,dV = m_{e,u} + m_{e,d} \\]\nwith the explosive mass of the downwind region being\n\\[ m_{e,d} = \\int_0^{x_n} \\iint_A c \\,dA \\,dx \\]\nFor a top-hat model, since the concentration at a given downwind distance is constant everywhere within the plume cross-section \\(\\iint_A c dA = c_m A\\), and, from a mass balance on the plume\n\\[ c_m A u = c_o Q_o \\]\n\\[ c_m A = { {c_o Q_o} \\over u} \\]\nwhich is a constant, thus\n\\[ m_{e,d} = \\int_0^{x_n} c_m A \\,dx \\]\n\\[ = \\int_0^{x_n} { {c_o Q_o} \\over u} \\,dx \\]\n\\[ = { {c_o Q_o} \\over u} x_n \\]\nFor the explosive mass of the upwind region a simple box model gives \\(m_{e,u} = 2 c_o L_U L_{Ho} L_{Vo}\\). Putting everything together9\n9 This is not specific to the Britter-McQuaid model, it works for any top hat model.\\[ m_e = 2 c_o L_U L_{Ho} L_{Vo} + { {c_o Q_o} \\over u} x_n \\]\nThis can be simplified greatly by setting the advection velocity to uref\n\\[ m_e = 2 c_o L_U L_{Ho} L_{Vo} + { {c_o Q_o} \\over u_{ref} } x_n \\]\n\\[ = 2 c_o L_U L_{Ho} {1 \\over 2}{D^2 \\over L_{Ho} } + c_o D^2 x_n \\]\n\\[ = c_o D^2 \\left( L_U + x_n \\right)\\]\n\ncₒ = ṁ/Qₒ\n\nmₑ = cₒ*D^2*(Lᵤ+xn)\n\n3197.617661470163 kg\n\n\nThis very simple expression is the obvious strength of a top-hat model: it makes calculating the explosive mass incredibly easy.10 It also retroactively justifies why the Britter McQuaid model is oriented around calculating xn: that’s all you actually need.11\n10 Woodward, Estimating the Flammable Mass of a Vapour Cloud.11 Some sources recommend calculating the explosive mass as the region of the plume with the concentration LFL ≤ c ≤ UFL, in which case \\(m_e = c_o D^2 \\left( x_{n,LFL} - x_{n,UFL} \\right)\\)If this seems too good to be true, the integration can be performed numerically by taking\n\\[ \\iint_A c \\,dA = c_m \\cdot 2L_H \\cdot L_V \\]\n\nusing QuadGK: quadgk\n\nfunction ∫∫cdA(x)\n    if Cm(x) ≥ LFL\n        return cₒ*Cm(x)*(2Lₕ(x))*Lᵥ(x)\n    else\n        return 0.0u\"kg/m\"\n    end\nend\n\nm_ed, err = quadgk(∫∫cdA, 0u\"m\", xn)\n\nm_eu = 2*cₒ*Lᵤ*Lₕₒ*Lᵥ(0u\"m\")\n\nm_eu + m_ed\n\n3197.617661470163 kg\n\n\nWhich is exactly the same.\nAbove I claimed the upwind region was “small” relative to the downwind region, this can be shown easily as the mass in each region is directly proportional to the length.\n\nLᵤ/(Lᵤ+xn)\n\n0.004205187316042018\n\n\nSince the mass in the upwind region is &lt;0.5% of the total mass in the cloud, I think the simple box model is justified.\n\n\nAdded complications\nAccording to Britter and McQuaid the top-hat model generates an overly conservative plume extent and they recommend using given the lateral extent curve up to 2/3 xn and after which connecting to xn using straight lines, as shown in the plume diagram. This makes the integration for explosive mass a little more complicated.\nFor simplicity the plume can be divided into three regions, the upwind region (x &lt; 0), the downwind region up to the cutoff (0 ≤ x &lt; 2/3 xn), and the downwind cutoff region (2/3 xn ≤ x &lt; xn )\n\\[ m_{e, \\textrm{cut off} } = m_{e,u} + m_{e,d1} + m_{e,d2} \\]\nThe upwind region, me,u, and the first downwind region me,d1 are already known, they are the same as above up to 2/3 xn. What is left to determine is the explosive mass in the cutoff region.\n\\[ m_{e,d2} = \\int_{2/3 x_n}^{x_n} \\iint_A c \\,dA \\,dx \\]\nThe integral can be re-written to take advantage of cmA being an invariant for a top-hat model,\n\\[ m_{e,d2} = \\int_{2/3 x_n}^{x_n} \\iint_A c \\,dA \\,dx \\]\n\\[ = c_m A_{\\textrm{original} } \\int_{2/3 x_n}^{x_n} { A_{\\textrm{cut off} } \\over A_{\\textrm{original} } } \\,dx \\]\nAssuming the vertical extent remains unchanged in this operation, the ratio of areas is the same as the ratio of horizontal extents\n\\[ { A_{\\textrm{cut off} } \\over A_{\\textrm{original} } } = { L_{H, \\textrm{cut off} } \\over L_{H, \\textrm{original} } } \\]\nFrom some simple geometry, the horizontal extent is\n\\[ L_{H, \\textrm{cut off} } = 3 L_{H, 2/3 x_n}  { {x_n - x} \\over x_n }\\]\nWhich then leads to\n\\[ m_{e,d2} = 3 c_o D^2 \\int_{2/3 x_n}^{x_n} { L_{H, 2/3 x_n} \\over L_H } { {x_n - x} \\over x_n } \\,dx \\]\nThere is probably a closed form for this integral but it is just as easy to integrate that numerically.\n\\[ m_{e, \\textrm{cut off} } = c_o D^2 \\left( L_U + \\frac{2}{3}x_n + 3 L_{H, 2/3 x_n} \\int_{2/3 x_n}^{x_n} { 1 \\over L_H(x) } { {x_n - x} \\over x_n } \\,dx   \\right)\\]\n\nmₑ_cutoff = cₒ*D^2*(Lᵤ + (2/3)*xn \n    + 3*Lₕ((2/3)*xn)*quadgk( (x) -&gt; (xn - x)/(xn*Lₕ(x)), (2/3)*xn, xn)[1] )\n\n2620.489605856347 kg\n\n\nThis works out to be about 20% less than the original explosive mass.\n\nmₑ_cutoff/mₑ\n\n0.8195131136008078"
  },
  {
    "objectID": "posts/Britter-McQuaid/index.html#final-thoughts",
    "href": "posts/Britter-McQuaid/index.html#final-thoughts",
    "title": "Taking a second look at the Britter-McQuaid model",
    "section": "Final thoughts",
    "text": "Final thoughts\nI think the error in the vertical extent may have limited the apparent utility of the Britter-McQuaid model. Most references I have do use the Britter-McQuaid model, noting that it is “reasonably simple to apply, and produces results which appear to be as good as more sophisticated models”,12 however they either claim that it is only good for calculating xn or gloss over how it could be used for anything else. The CCPS references seem consistent in neglecting to mention at all that the model can also estimate the plume extent. So, while I can’t imagine I’m the first person to have noticed that the given equation for LV doesn’t work, I have yet to encounter anyone actually admitting it.\n12 AIChE/CCPS, Guidelines for Consequence Analysis of Chemical Releases., 122.13 Lees, Loss Prevention in the Process Industries; Casal, Evaluation of the Effects of Consequences of Major Accidents in Industrial Plants.14 Bakkum and Duijm., “Vapour Cloud Dispersion”.That said, the correction also seems obvious to me: one simply follows what is described in the text which is exactly how Britter and McQuaid calculated the cloud height for the instantaneous model (which is correct) in the same workbook. That the incorrect equation for LV is repeated in other references,13 with only the TNO Yellow Book14 making a correction, while still repeating a critical mistake, strikes me as very odd.\nThe Britter-McQuaid model would seem to be the perfect fit for screening models, which are often only order of magnitude estimates at best anyways. It gives reasonable concentrations, plausible plume extents, and the explosive mass is ridiculously easy to calculate (slightly more tedious if you are using the 2/3 cut-off region but nothing that couldn’t be worked out in advance if this was going to be incorporated into a routine calculation tool)."
  },
  {
    "objectID": "posts/Britter-McQuaid/index.html#references",
    "href": "posts/Britter-McQuaid/index.html#references",
    "title": "Taking a second look at the Britter-McQuaid model",
    "section": "References",
    "text": "References\n\n\nAIChE/CCPS. Guidelines for Consequence Analysis of Chemical Releases. New York: American Institute of Chemical Engineers, 1999.\n\n\nBakkum, E. A., and N. J. Duijm. “Vapour Cloud Dispersion.” In Methods for the Calculation of Physical Effects, CPR 14E, edited by C. J. H. van den Bosch and R. A. P. M. Weterings, 3rd ed. The Hague: TNO, 2005.\n\n\nBritter, Rex E., and J. McQuaid. “Workbook on the Dispersion of Dense Gases. HSE Contract Research Report No. 17/1988,” 1988.\n\n\nCasal, Joachim. Evaluation of the Effects of Consequences of Major Accidents in Industrial Plants. 2nd ed. Amsterdam: Elsevier, 2018.\n\n\nLees, Frank P. Loss Prevention in the Process Industries. 2nd ed. Oxford: Butterworth-Heinemann, 1996.\n\n\nWoodward, John L. Estimating the Flammable Mass of a Vapour Cloud. New York: American Institute of Chemical Engineers, 1998."
  },
  {
    "objectID": "posts/dispersion_parameter_sensitivity/index.html",
    "href": "posts/dispersion_parameter_sensitivity/index.html",
    "title": "Messing around with model parameters",
    "section": "",
    "text": "Recently I added some alternative correlations to GasDispersion.jl, the julia package I put together for basic chemical release modeling, and I thought it would be worthwhile to circle back and look at some of those in more depth.\nTypically, when evaluating various release scenarios, key pieces of the model are specified in advance and each scenario uses the same set of assumptions: comparing apples to apples. For a Gaussian plume dispersion model there are really three key correlations used for the model parameters: the wind-speed profile, crosswind dispersion, and vertical dispersion. Correlations for each of these are given in the standard references and there is not, to my mind, any deep reason to prefer one reference over the another. Besides maintaining consistency with other modeling or perhaps with industry practice in a particular area.\nThis raises the obvious question: how much does it matter which reference you use? Usually one takes the results of a Gaussian plume model with a fair grain of salt, these are “order of magnitude” estimates really. That’s what I’m going to look at here."
  },
  {
    "objectID": "posts/dispersion_parameter_sensitivity/index.html#windspeed",
    "href": "posts/dispersion_parameter_sensitivity/index.html#windspeed",
    "title": "Messing around with model parameters",
    "section": "Windspeed",
    "text": "Windspeed\nThe windspeed correlations I am looking at here are the basic power law\n\\[ u = u_R \\left( z \\over z_R \\right)^p \\]\nwhere uR is the known windspeed at a reference height zR and p is a parameter that depends upon the Pasquill stability class. There are more complex models that incorporate the surface roughness, Monin-Obukhov mixing length, and other measures of stability, they are beyond this analysis.\nThere are three different standard references used in GasDispersion.jl for windspeed: the default which comes from Spicer and Havens,1 the correlations used by the EPA Industrial Source Complex (ISC3)2 dispersion models, and the correlations given in the various CCPS guidance documents3\n1 EPA-450/4-89-019.2 EPA-454/b-95-003b.3 AIChE/CCPS, Guidelines for Consequence Analysis of Chemical Releases.4 Handbook on Atmospheric Diffusion.The ISC3 and CCPS correlations are divided into urban and rural terrain and are exactly the same correlations for the unstable classes. They appear to be the correlations given in Hanna, Briggs, and Hosker.4 They also bracket the default correlation. Clearly whether or not the terrain is urban is significant, it can lead to a 20-30% difference in estimated windspeed (depending upon elevation).\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Windspeed correlations for class A, B, C, and D stability.\n\n\n\n\nFor the stable atmospheres the ISC3 and CCPS rural correlations are the same. However they are very different for urban terrain and they no longer bracket the default correlation. The CCPS urban correlations are the same as Hanna, Briggs, and Hosker,5 the ISC3 correlations use the parameter p = 0.30 and no reference is given in the model specification so I don’t know why.\n5 Hanna, Briggs, and Hosker.For an urban release scenario, whether or not one choses the default, the ISC3 urban, or the CCPS urban correlation can lead to a 300% difference in windspeed (for class F stability, depending on elevation). Which is a pretty large difference.\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Windspeed correlations for class E and F stability."
  },
  {
    "objectID": "posts/dispersion_parameter_sensitivity/index.html#plume-dispersion",
    "href": "posts/dispersion_parameter_sensitivity/index.html#plume-dispersion",
    "title": "Messing around with model parameters",
    "section": "Plume Dispersion",
    "text": "Plume Dispersion\nThe more diverse sets of correlations are for the plume dispersion parameters, the crosswind and vertical dispersion. To some extent this is because the early Turner6 presented the dispersion parameters graphically and many subsequent authors generated their own curves to fit these plots.\n6 Workbook of Atmospheric Dispersion Estimates.\nCrosswind Dispersion\nCrosswind dispersion can be divided into the various attempts at fitting the curves presented graphically by Turner and those based on Briggs’ urban and rural correlations7\n7 Briggs, “Diffusion Estimation for Small Emissions. Preliminary Report” page 38; Note that the correlations are given with respect to half-width/half-depthThe default correlation is a simple set of correlations of the form\n\\[ \\sigma_y = a x^b \\]\nwhich attempts to fit the Turner curves.\nThe CCPS correlations are from Briggs8 and the ISC3 urban correlations are from Briggs as well, the ISC3 rural correlations are something else entirely but I suspect are intended to fit the Turner9 curves. The correlations from the TNO yellow book10 are also a different attempt at fitting the Turner curves. What GasDispersion,jl gives as “Turner” is the fit to the Turner curves given in Lees.11\n8 Briggs.9 Turner, Workbook of Atmospheric Dispersion Estimates.10 Bakkum and Duijm., “Vapour Cloud Dispersion”.11 Lees, Loss Prevention in the Process Industries.\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Crosswind dispersion correlations.\n\n\n\n\nZooming in on the class F curves is illustrative of the lot: most of the lines overlap and hew pretty close to the curve-fit for Turner12 with the exception of the Briggs’ urban/rural correlations. The biggest impact on these model parameters is whether or not a rural/urban terrain is used or not. Note these are log-log plots.\n12 Turner, Workbook of Atmospheric Dispersion Estimates.\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Crosswind dispersion correlations, class F stability.\n\n\n\n\n\n\nVertical Dispersion\nThe vertical dispersion correlations are decidedly more varied. Varied enough that I’m just going to show them all at full scale13\n13 The correlations given in AIChE/CCPS, Guidelines for Consequence Analysis of Chemical Releases for urban conditions has typos in the class A, B and D correlations, I have corrected them here to match the Briggs correlations on which they are supposed to be based.\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Vertical dispersion correlations.\n\n\n\nFor some of these there is an order of magnitude spread in vertical dispersion, depending on which model happens to be used. Even when looking only at the correlations that are “universal”, i.e. are not for either urban or rural terrains. From this alone one would expect that the concentration profiles would vary by a large amount, depending on which set of correlations one used to model a given scenario."
  },
  {
    "objectID": "posts/dispersion_parameter_sensitivity/index.html#an-example",
    "href": "posts/dispersion_parameter_sensitivity/index.html#an-example",
    "title": "Messing around with model parameters",
    "section": "An Example",
    "text": "An Example\nJust to give an example of how this works out, lets look at the emissions from a large stack. I happened to have picked the stack for a large power plant in the Edmonton area: TransAlta’s Sundance station. This power plant is on the shores of Lake Wabamun and is pretty rural, it has several stacks but let’s consider only Stack 2 and examine the dispersion of SO2 emissions.\nFrom Alberta’s AEIR Air Emission Rates dataset we can pull the mass emission rates for SO2 as well as the relevant stack dimensions. Note this dataset is from 2018 and thus may not represent the current operations at Sundance.\n\n# TransAlta Sundance - Stack 2\nm = 3200/3600 # mass emission rate: 3200kg/h in kg/s\nh = 155.5 # stack height, m\nd = 7.3   # stack diameter, m\nv = 35.6  # stack exit velocity, m/s\nT = 439.7 # stack exit temperature, K\n\nFor the sake of modeling let’s assume a class D atmospheric stability with a windspeed at 10m of 2m/s. The atmosphere is otherwise at standard state.\n\n# assumed weather conditions\nuᵣ  = 2  # windspeed, m/s\nzᵣ = 10 # windspeed elevation, m\nstability = ClassD\n\n# standard state\nPₛ = 101325 # Pa\nTₛ = 273.15 # K\n\nWe can construct the relevant scenario for GasDispersion.jl directly.\n\nr = VerticalJet(m, Inf, d, v, h, Pₛ, T, 0.0)\n\na = SimpleAtmosphere(pressure=Pₛ, temperature=Tₛ, windspeed=uᵣ, windspeed_height=zᵣ, stability=stability)\n\n# a dummy substance, since I know a gaussian plume doesn't require any material\n# properties I have just left them as NaNs\nSO2 = Substance(name=:SulfurDioxide,molar_weight=0.064066,liquid_density=1,boiling_temp=1,\n                latent_heat=1,gas_heat_capacity=1,liquid_heat_capacity=1)\n\nscn = Scenario(SO2,r,a)\n\nSubstance: SulfurDioxide \n    MW: 0.064066 kg/mol \n    P_v: GasDispersion.Antoine{Float64}(0.007705368698167287, 0.007705368698167287, 0.0) Pa \n    ρ_g: 2.7095140841291006 kg/m^3 \n    ρ_l: 1 kg/m^3 \n    T_ref: 288.15 K \n    P_ref: 101325.0 Pa \n    k: 1.4  \n    T_b: 1.0 K \n    Δh_v: 1 J/kg \n    Cp_g: 1 J/kg/K \n    Cp_l: 1 J/kg/K \nVerticalJet release:\n    ṁ: 0.8888888888888888 kg/s \n    Δt: Inf s \n    d: 7.3 m \n    u: 35.6 m/s \n    h: 155.5 m \n    P: 101325.0 Pa \n    T: 439.7 K \n    f_l: 0.0  \nSimpleAtmosphere atmosphere:\n    P: 101325.0 Pa \n    T: 273.15 K \n    u: 2.0 m/s \n    h: 10.0 m \n    rh: 0.0 % \n    stability: ClassD  \n\n\nThe Gaussian plume model is then given by the following, neglecting the effect of plume rise.\n\nconc = plume(scn, GaussianPlume; plumerise=false);\n\nPlotted below are the results for every equation set, at near ground level (at basically “my head” level). Clearly the urban/rural choice is quite important, leading to a ~4× greater maximum concentration. The TNO correlations, which uses the default correlation for windspeed and the TNO correlations for the crosswind and vertical dispersion, leads to less dispersion and thus a greater maximum concentration relative to the rest.\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Downwind concentration of sulfur dioxide, at 2m elevation, as predicted by the default, CCPS, ISC3, TNO, and Turner correlations, neglecting plume rise.\n\n\n\n\nPlotted below is the 172ppbv isopleth, the 1-hr Ambient Air Quality Objective (AAQO) for SO2 in Alberta. As we would expect, the correlations that lead to a higher maximum concentration correspond to less overall dispersion and the isopleth is quite a bit smaller for the urban versus rural case and the TNO versus the remaining cases. The scale is in kilometers so this is quite a large difference in area.\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Concentration isopleths for sulfur dioxide, at 2m elevation, as predicted by the default, CCPS, ISC3, TNO, and Turner correlations, neglecting plume rise.\n\n\n\n\nThe above was assuming no plume rise, however the relative differences are much more pronounced when plume rise is included.\n\nconc = plume(scn, GaussianPlume; plumerise=true);\n\nPlotted below is the same downwind concentration plot as above, but incorporating the Briggs’ plume rise model. Since this leads to a greater overall dispersion, the concentration is much smaller (everything is well below the AAQO at ground level, which is good news). However this adds another dimension along which the models can vary: plume rise is a function of windspeed, and overall dispersion is a function of plume rise. These different sets of correlations lead to the plume rising to a different elevation, and also dispersing to a differing degree, magnifying the differences between them. In this case there is up to a ~30× difference between the max concentrations predicted between the urban and rural case.\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Downwind concentration of sulfur dioxide, at 2m elevation, as predicted by the default, CCPS, ISC3, TNO, and Turner correlations, using Briggs’ plume rise correlations."
  },
  {
    "objectID": "posts/dispersion_parameter_sensitivity/index.html#final-thoughts",
    "href": "posts/dispersion_parameter_sensitivity/index.html#final-thoughts",
    "title": "Messing around with model parameters",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI think the above illustrates the necessity of picking a standard set of correlations for use when screening scenarios at a particular plant (e.g. using either the CCPS urban or rural correlations as appropriate for the area around the plant) and being careful to keep these consistent. It also shows how seriously one should take the exact values generated by the models: not very. The dispersion model results are highly sensitive to the choice of correlations, and they are also quite sensitive to the other assumptions that go into a release scenario (e.g. atmospheric stability, wind-speed, mass emission rate). The results are really order of magnitude at best.\nIt is often the case that chemical plants are situated at the periphery of cities, in areas that blur the line between “urban” and “rural”. Also, cities grow and industrial areas fill in. A plant that was essentially rural may, overtime, fill in such that the urban correlations better represent the area. I think it is worth comparing the urban/rural models for a range of plausible results and considering whether assumptions made in the past about the area around the plant are still valid given changes in the area.\nThere are other correlations, for wind-speed and for dispersion, that take into account the local surface roughness which could be used instead and the sensitivity to the models to assumptions about surface roughness could be evaluated. This would likely lead to a smaller range of values, and give a path for updating the screening model as the area around the plant changes (update the assumed surface roughness and re-run)."
  },
  {
    "objectID": "posts/dispersion_parameter_sensitivity/index.html#references",
    "href": "posts/dispersion_parameter_sensitivity/index.html#references",
    "title": "Messing around with model parameters",
    "section": "References",
    "text": "References\n\n\nAIChE/CCPS. Guidelines for Consequence Analysis of Chemical Releases. New York: American Institute of Chemical Engineers, 1999.\n\n\nBakkum, E. A., and N. J. Duijm. “Vapour Cloud Dispersion.” In Methods for the Calculation of Physical Effects, CPR 14E, edited by C. J. H. van den Bosch and R. A. P. M. Weterings, 3rd ed. The Hague: TNO, 2005.\n\n\nBriggs, Gary A. “Diffusion Estimation for Small Emissions. Preliminary Report.” Oak Ridge, TN: Air Resources Atmospheric Turbulence; Diffusion Laboratory, National Oceanic; Atmospheric Administration, 1973. https://doi.org/10.2172/5118833.\n\n\nEPA-454/b-95-003b: User’s Guide for the ISC3 Dispersion Models. Vol. 2. Environmental Protection Agency, 1995.\n\n\nHanna, Steven R., Gary A. Briggs, and Rayford P. Hosker Jr. Handbook on Atmospheric Diffusion. Springfield, VA: National Technical Information Service, 1982. https://doi.org/10.2172/5591108.\n\n\nLees, Frank P. Loss Prevention in the Process Industries. 2nd ed. Oxford: Butterworth-Heinemann, 1996.\n\n\nSpicer, Thomas O., and Jerry A. Havens. EPA-450/4-89-019: User’s Guide for the DEGADIS 2.1 Dense Gas Dispersion Model. Research Triangle Park, NC: Office of Air Quality Planning; Standards, United States Environmental Protection Agency, 1989. https://nepis.epa.gov/Exe/ZyNET.exe/2000J5GU.txt.\n\n\nTurner, D. Bruce. Workbook of Atmospheric Dispersion Estimates. Research Triangle Park, NC: Office of Air Programs, United States Environmental Protection Agency, 1989."
  },
  {
    "objectID": "posts/relief_valve_sizing/index.html",
    "href": "posts/relief_valve_sizing/index.html",
    "title": "Relief Valve Sizing with Real Gases",
    "section": "",
    "text": "Very often, in chemical engineering, the line between problems one can solve one’s self and problems that are solved with a piece of commercial software is when ideal fluid assumptions break down. Relief valve sizing is a typical example: if the fluid is (approximately) an ideal gas then sizing is simple and often done in a spreadsheet. When this isn’t the case, if the compressiblity is &lt;0.8 or &gt;1.1,1 then one typically has to turn to some commercial software. Models of real fluids are complicated and extracting the relevant thermodynamic properties from them can be quite tedious when doing it all from scratch.\nClapeyron.jl comes to the rescue here with a wide array of equations of state for real fluids. Combined with julia’s robust ecosystem of libraries for integration and optimization, solving real fluid problems becomes simple. This post walks through how to size a relief device, in gas service, starting from an ideal gas and working through various methods for real gases using equations of state."
  },
  {
    "objectID": "posts/relief_valve_sizing/index.html#sizing-a-pressure-relief-valve",
    "href": "posts/relief_valve_sizing/index.html#sizing-a-pressure-relief-valve",
    "title": "Relief Valve Sizing with Real Gases",
    "section": "Sizing a Pressure Relief Valve",
    "text": "Sizing a Pressure Relief Valve\nThe general idea for sizing a relief valve is to determine the minimum area required such that the mass flow through the valve equals the mass flow required for the governing release case via the relation2\n2 API, Standard 520, Sizing, Selection, and Installation of Pressure-Relieving Devices, Part i - Sizing and Selection, 10th Ed equation B.5.\\[ W = K A G \\]\nwhere \\(W\\) is the required mass flow-rate, \\(K\\) is a capacity correction, \\(A\\) is the theoretical flow area, and \\(G\\) the frictionless) mass flux through the valve. Valves are sized based on the theoretical flow area.\nThe general process is as follows:\n\nDetermine the governing release rate, \\(W\\)\nDetermine the capacity correction, \\(K\\)\nCalculate the mass flux through the valve, \\(G\\)\nCalculate the theoretical flow area \\(A = \\frac{W}{K G}\\)\nSelect the appropriate valve with a flow area \\(&gt;A\\).\n\nStandards, such as API-2020, give equations that combine steps 3 and 4 and absorb unit-conversions into the constants, so that the equation is in a more convenient form, but this is what is happening under the hood.\nThe complications creep in through calculating \\(G\\), it is path-dependent and is a function of the equation of state for the fluid. For gas releases the relief device is typically treated as an isentropic nozzle, the assumption being that the flow-rate through the valve is typically large enough that any heat transfer can be neglected.\n\n\n\n\n\n\nFigure 1: A hypothetical pressure relief device, connected to a pressure reservoir (1) and discharging into the atmosphere (2).\n\n\n\nConsider the differential form of the mechanical energy balance, along a streamline from the stagnation point, in the vessel, through the valve and out into the atmosphere, assuming no elevation change and no friction\n\\[ dP + \\rho u du = 0 \\]\n\\[ u du = -\\frac{dP}{\\rho} = - vdP \\]\nIntegrating from the stagnation point to the throat of the nozzle gives\n\\[ \\frac{1}{2} u_t^2 = - \\int_{P_1}^{P_t} v dP \\]\nWhere the velocity at the stagnation point, \\(u_1=0\\). Putting this in terms of the mass flux \\(u = v G\\)\n\\[ \\frac{1}{2} v_t^2 G_t^2 = - \\int_{P_1}^{P_t} v dP \\]\n\\[ G_t = \\frac{1}{v_t} \\sqrt{-2 \\int_{P_1}^{P_t} v dP} = \\rho_t \\sqrt{2 \\int_{P_t}^{P_1} v dP} \\]\nThis integral cannot be solved directly at this point as the conditions at the throat of the nozzle are not known. Solving this requires simultaneously solving for the nozzle conditions, \\(P_t, T_t\\).\nIf we specify that the streamline follows an isentropic path, then we can construct a constrained maximization problem: the nozzle conditions are the \\(P_t\\) and \\(T_t\\) which maximizes \\(G_t\\) where the integration is taken along an isentropic path.\n\nChoked Flow\nIn the case where flow is choked, i.e. the flow in the nozzle reaches sonic velocity, the maximum \\(G_t\\) occurs at the sonic velocity with a pressure \\(P_t &gt; P_2\\). This can allow for the direct calculation of the mass flux as \\(G_t = \\rho_t c_t\\), where \\(c_t\\) is the sonic velocity at the throat. No integration required."
  },
  {
    "objectID": "posts/relief_valve_sizing/index.html#a-motivating-example",
    "href": "posts/relief_valve_sizing/index.html#a-motivating-example",
    "title": "Relief Valve Sizing with Real Gases",
    "section": "A Motivating Example",
    "text": "A Motivating Example\nConsider the release of ethane from a vessel at 200 bar and 400 K, for the sake of simplicity assume the release is directly into the atmosphere at 1 bar and 288.15 K (15°C) (the flow is going to be choked, so this doesn’t actually matter).\nusing Unitful\nbegin\n# the vessel properties\n    P₁ = 200u\"bar\"\n    T₁ = 400u\"K\"\n\n# the ambient properties\n    P₂ = 1u\"bar\"\n    T₂ = 288.15u\"K\"\nend\nWe can use Clapeyron.jl to initialize a few example equations of state for ethane. In this case I’m going to use an ideal gas model (ReidIdeal is an ideal gas model that also includes correlations for the ideal gas heat capacity), a cubic equation of state (volume translated Peng Robinson), and an empirical Helmholtz model (GERG-2008).\nusing Clapeyron\nbegin\n# assorted equations of state for ethane\n    ig_ethane = ReidIdeal([\"ethane\"])\n    vtpr_ethane = VTPR([\"ethane\"]; idealmodel = ReidIdeal)\n    gerg_ethane = GERG2008([\"ethane\"])\nend\n# this is a hack, ideal models in Clapeyron do not return a \n# molar weight and so cannot return a mass density\nClapeyron.mw(model::IdealModel) = Clapeyron.mw(vtpr_ethane)\nAt system conditions ethane is a super critical fluid, with the temperature and pressure above the critical point, which can be modelled as a dense gas."
  },
  {
    "objectID": "posts/relief_valve_sizing/index.html#the-ideal-gas-case",
    "href": "posts/relief_valve_sizing/index.html#the-ideal-gas-case",
    "title": "Relief Valve Sizing with Real Gases",
    "section": "The Ideal Gas Case",
    "text": "The Ideal Gas Case\nConsidering the choked flow case, we know that \\(G_t = \\frac{c_t}{v_t}\\) and, for an ideal gas, the sonic velocity is given by3\n3 Tilton, “Fluid and Particle Dynamics” equation 6-113.\\[ c = \\sqrt{ {k R T} \\over M} = \\sqrt{k P v} \\]\nCombining these we have\n\\[ G_t = \\frac{c_t}{v_t} = { \\sqrt{ k P_t v_t } \\over v_t } = \\sqrt{ k P_t \\over v_t } \\]\nIt can be shown that, along an isentropic path defined by \\(P v^k = \\mathrm{const}\\), the critical pressure ratio is4\n4 Tilton equation 6-119.\\[ {P_t \\over P_1} = { P_{chk} \\over P_1 } = \\left(2 \\over {k+1} \\right)^{k \\over {k-1} } \\]\nWhich allows us to write\n\\[ P_t = P_1 {P_t \\over P_1} = P_1 \\left(2 \\over {k+1} \\right)^{k \\over {k-1} } \\]\nand (using \\(P_1 v_1^k = P_t v_t^k\\))\n\\[ v_t = v_1 \\left(P_1 \\over P_t \\right)^{1 \\over k} = v_1 \\left(2 \\over {k+1} \\right)^{-1 \\over {k-1} } \\]\nSubstituting back into the equation for \\(G_t\\)5\n5 API, Standard 520, Sizing, Selection, and Installation of Pressure-Relieving Devices, Part i - Sizing and Selection, 10th Ed equation B.21.\\[ G_t = \\sqrt{ k \\frac{P_1}{v_1} \\left(2 \\over {k+1} \\right)^{k+1 \\over {k-1} } } \\]\nor, to put it in terms of density\n\\[ G_t = \\sqrt{ k P_1 \\rho_1 \\left(2 \\over {k+1} \\right)^{k+1 \\over {k-1} } } \\]\nwhere \\(k\\), the isentropic expansion factor for an ideal gas, is the ratio of heat capacities\n\\[ k = { c_{p,ig} \\over c_{v,ig} } \\]\n\n\n\n\n\n\nNote\n\n\n\nThis is the basis of API 520 Part 1 equation 9 where the following substitutions is made:\n\\[ \\rho = { {P M} \\over {Z R T} } \\]\nand the constant \\(R\\) and some unit conversions are rolled up into the constant 0.03948 in the expression for \\(C\\)\n\\[ R = 8.314 { {\\mathrm{m^3} \\cdot \\mathrm{Pa} } \\over {\\mathrm{mol} \\cdot \\mathrm{K} } } = 8,314 { {\\mathrm{m^3} \\cdot \\mathrm{Pa} } \\over {\\mathrm{kmol} \\cdot \\mathrm{K} } } = 8,314 { {\\mathrm{kg} \\cdot \\mathrm{m^2} } \\over { \\mathrm{kmol} \\cdot \\mathrm{s^2} \\cdot \\mathrm{K} } } \\]\n\\[ {1 \\over \\sqrt{8,314} } \\left[ { \\sqrt{ \\mathrm{kmol} \\cdot \\mathrm{K} } \\cdot \\mathrm{s} } \\over { \\sqrt{\\mathrm{kg} } \\cdot \\mathrm{m} } \\right] \\times 3600 \\left[ \\mathrm{s} \\over \\mathrm{h} \\right] \\times 10^{-6} \\left[ \\mathrm{m^2} \\over \\mathrm{mm^2} \\right] \\times 10^3 \\left[ \\mathrm{Pa} \\over \\mathrm{kPa} \\right] \\]\n\\[ = 0.03948 \\left[ \\sqrt{\\mathrm{kmol} \\cdot \\mathrm{kg} \\cdot \\mathrm{K} } \\over { \\mathrm{h} \\cdot \\mathrm{mm^2} \\cdot \\mathrm{kPa} } \\right] \\]\n\n\nWe can use Clapeyron.jl to calculate \\(k\\) at any given temperature, using correlations for the ideal gas heat capacity.\nfunction isentropic_expansion_factor(model::IdealModel, P, T; z=[1.0])\n    cₚ_ig = isobaric_heat_capacity(model, P, T; phase=:vapor)\n    cᵥ_ig = isochoric_heat_capacity(model, P, T; phase=:vapor)\n    return cₚ_ig/cᵥ_ig\nend\nFrom which we calculate k= 1.146.\nWe can check our work by comparing with the tabulated values. At 15°C and 1 atm we calculate k= 1.193 which is the same as the tabulated value of 1.19 (given at 15°C and 1 atm).6\n6 API, 70.function mass_flux_choked(model, P, T; z=[1.0])\n    k = isentropic_expansion_factor(model, P, T; z=z)\n    ρ = mass_density(model, P, T, z; phase=:vapor)\n    Gₜ² = k*P*ρ*(2/(k+1))^((k+1)/(k-1))\n    return √(Gₜ²)\nend\nThe theoretical mass flux for the ideal gas is then 38359 kg m^-2 s^-1\nThe ideal gas model, when the flow is choked, calculates the mass flux directly without needing to calculate the actual conditions at the nozzle. These can be calculated easily as well.7\n7 Tilton, “Fluid and Particle Dynamics” equations 6-119 and 6-120.nozzle_pressure_ideal(P, T, k) = P*(2/(k+1))^(k/(k-1))\nnozzle_temperature_ideal(P, T, k) = T*(2/(k+1))\nThe pressure at the nozzle is 115 bar the temperature at the nozzle is 373 K, which is above the critical point. The fluid supercritical and choked when leaving the PSV."
  },
  {
    "objectID": "posts/relief_valve_sizing/index.html#the-isentropic-expansion-factor",
    "href": "posts/relief_valve_sizing/index.html#the-isentropic-expansion-factor",
    "title": "Relief Valve Sizing with Real Gases",
    "section": "The Isentropic Expansion Factor",
    "text": "The Isentropic Expansion Factor\nAt the vessel conditions, the VTPR model of ethane gives a compressibility factor of 0.672 (GERG-2008 model gives a similar value of 0.69), well below 0.8 and therefore outside the range where the ideal gas model is expected to work well.\nAn alternative method is to calculate what the effective isentropic expansion factor would be, for the real gas, assuming that the real fluid obeys\n\\[ P_1 v_1^n = P_t v_t^n \\]\nwhere \\(n\\) is a constant.\nThe derivation of \\(n\\) follows from the definition of the speed of sound in a gas8\n8 Tilton, 6–22; Gmehling et al., Chemical Thermodynamics for Process Simulation, 113.\\[ c = \\sqrt{ \\left( {\\partial P} \\over {\\partial \\rho} \\right)_S} =\\sqrt{ -v^2 \\left( {\\partial P} \\over {\\partial v} \\right)_S} \\]\nThe constant entropy partial derivative can be re-written to eliminate entropy9\n9 Gmehling et al., Chemical Thermodynamics for Process Simulation, 660.\\[ \\left( {\\partial P} \\over {\\partial v} \\right)_S = { { \\left( {\\partial S} \\over {\\partial T} \\right)_P \\left( {\\partial P} \\over {\\partial T} \\right)_v } \\over { \\left( {\\partial S} \\over {\\partial T} \\right)_v \\left( {\\partial v} \\over {\\partial T} \\right)_P } } \\]\nUsing the relations10\n10 Gmehling et al., Chemical Thermodynamics for Process Simulation equations C.21 and C.8 (respectively).\\[ c_p = T \\left( {\\partial S} \\over {\\partial T} \\right)_P \\]\nand\n\\[ c_v =  T \\left( {\\partial S} \\over {\\partial T} \\right)_V \\]\nwe get\n\\[ \\left( {\\partial P} \\over {\\partial v} \\right)_S = {c_p \\over c_v} \\left( {\\partial P} \\over {\\partial v} \\right)_T \\]\nand the sonic velocity is then\n\\[ c = \\sqrt{ -v^2 {c_p \\over c_v} \\left( {\\partial P} \\over {\\partial v} \\right)_T } \\]\nequating this to the ideal gas case, \\(c = \\sqrt{n P v}\\), and solving for \\(n\\) gives11\n11 API, Standard 520, Sizing, Selection, and Installation of Pressure-Relieving Devices, Part i - Sizing and Selection, 10th Ed equation B.13.\\[ n = -\\frac{v}{P} {c_p \\over c_v} \\left( {\\partial P} \\over {\\partial v} \\right)_T \\]\nwhere \\(n\\) has been used to distinguish it from \\(k\\) (the ideal gas case). This is the version of \\(n\\) presented in most references, such as API 520. The derivation, however, hints at a useful shortcut to calculating \\(n\\) that does not require digging into the internals of Clapeyron.jl to retrieve partial derivatives:\n\\[ n = { c^2 \\over {P v} } = { {\\rho c^2} \\over P } \\]\nThe remainder of the calculations are identical as the ideal gas case, simply substituting \\(n\\) wherever \\(k\\) appears. Unfortunately \\(n\\) is not actually constant and depends on the temperature and pressure, which are not actually known in the nozzle, so the temperature and pressure at the stagnation point are often used instead.\nfunction isentropic_expansion_factor(model, P, T; z=[1.0])\n    ρ = mass_density(model, P, T, z)\n    c = speed_of_sound(model, P, T, z)\n    n = ρ*c^2/P\n    return n\nend\nUsing effective isentropic expansion factors from the VTPR equation of state, the theoretical mass flux is 57811 kg m^-2 s^-1 ( 59321 kg m^-2 s^-1 from GERG-2008 ). This is quite a bit larger than the ideal case, indicating that the ideal gas law leads to a significantly over-sized PRV, 51.0% larger.\n\n\n\n\n\n\nFigure 2: The isentropic expansion factor for ethane at 400K, calculated for a range of stagnation pressures.\n\n\n\nThe isentropic expansion factor method works best when \\(n\\) is approximately constant over the isentropic path. As the above figure shows, this breaks down in ethane for pressures greater than ~100 bar. It also shows that the different equations of state start to diverge greatly further into the supercritical regime."
  },
  {
    "objectID": "posts/relief_valve_sizing/index.html#solving-the-choked-flow-energy-balance",
    "href": "posts/relief_valve_sizing/index.html#solving-the-choked-flow-energy-balance",
    "title": "Relief Valve Sizing with Real Gases",
    "section": "Solving the Choked Flow Energy Balance",
    "text": "Solving the Choked Flow Energy Balance\nAnother approach, and one I have seen more often in older references, is to perform an energy balance over the isentropic path and, assuming the flow is choked, solve for sonic velocity in the nozzle.12 Consider an energy balance starting at the stagnation point, (1), and following an isentropic path to immediately after the throat of the nozzle (t).\n12 Crowl et al., “Process Safety.” 23–55; Gmehling et al., Chemical Thermodynamics for Process Simulation, 603.\\[ h_1 = h_t + \\frac{1}{2} c_t^2 \\]\nWhere \\(c_t\\) is the speed of sound at the nozzle, a function of \\(P_t\\) and \\(T_t\\). The procedure is then to solve the system of equations given by the energy balance and the entropy balance, \\(s_1 = s_t\\), for \\(P_t\\) and \\(T_t\\), then the theoretical mass flux is given by\n\\[ G_t = \\rho_t c_t \\]\nThere are a few ways this could be done, a straight-forward way is to divide the problem into two: 1. Define the isentropic path, i.e. find the isentropic temperature for a given pressure P 2. Use the energy balance to solve for the pressure, following the isentropic path.\nA more direct way is to solve for \\(P_t\\) and \\(T_t\\) simultaneously. This is what I do next, using NonlinearSolve.jl\n# Clapeyron does not expose this by default\nmolecular_weight(model,z) = Clapeyron.molecular_weight(model,z)\nfunction nozzle_balance(y, prms)\n    P, T = y\n\n    # stagnation point\n    s₁ = prms.entropy\n    h₁ = prms.enthalpy\n\n    # at throat conditions\n    s₂ = entropy(prms.model, P, T, prms.z)\n    h₂ = enthalpy(prms.model, P, T, prms.z)/prms.Mw\n    c² = speed_of_sound(prms.model, P, T, prms.z)^2\n    \n    return [ s₁ - s₂\n             h₁ - h₂ - 0.5*c² ]\nend\nfunction mass_flux_choked_energy_balance(model, P, T; z=[1.0])\n    # calculate the entropy and specific enthalpy at \n    # initial conditions\n    Mw = molecular_weight(model, z)\n    s₁ = entropy(model, P, T)\n    h₁ = enthalpy(model, P, T)/Mw\n\n    # solve the choked flow energy balance for\n    # an isentropic nozzle\n    params = (model=model, entropy=s₁, enthalpy=h₁, z=z, Mw=Mw)\n    y₀ = [P; T]\n    prob = NonlinearProblem(nozzle_balance, y₀, params)\n    sol = solve(prob, NewtonRaphson())\n    Pₜ, Tₜ = sol.u\n\n    # velocity is the sonic velocity at nozzle conditions\n    ρₜ = mass_density(model, Pₜ, Tₜ, z)\n    cₜ = speed_of_sound(model, Pₜ, Tₜ, z)\n    \n    return ρₜ*cₜ\nend\nfunction mass_flux_choked_energy_balance(model, P::Quantity, T::Quantity; z=[1.0])\n    P = ustrip(u\"Pa\", P)\n    T = ustrip(u\"K\", T)\n    return mass_flux_choked_energy_balance(model, P, T; z=z)*1u\"kg*m^-2*s^-1\"\nend\nSolving the choked flow energy balance, using VTPR equation of state, the theoretical mass flux is 54629 kg m^-2 s^-1 ( 54353 kg m^-2 s^-1 from GERG-2008 ). This is also quite a bit larger than the ideal case, 42.0% larger. Though the values for the two equations of state are closer, indicating that this method is less sensitive to model choice.\n\n\n\n\n\n\nFigure 3: The isentropic paths for the ideal gas, effective isentropic factor, and true isentropic path methods.\n\n\n\nIn this case the ideal gas method and the isentropic expansion factor method bracket the more exact method of solving the energy balance directly.\nAs it is written, this method would need to be modified to allow for non-choked flow. This is done by eliminating the assumption \\(u_t = c_t\\) and instead finding the conditions which maximize \\(G_t\\) (subject to the constraints of the entropy balance and the enthalpy balance). This will arrive at the same solution, in the case of choked flow, but with a little more effort."
  },
  {
    "objectID": "posts/relief_valve_sizing/index.html#direct-integration",
    "href": "posts/relief_valve_sizing/index.html#direct-integration",
    "title": "Relief Valve Sizing with Real Gases",
    "section": "Direct Integration",
    "text": "Direct Integration\nDirect integration is the method most commonly recommended today, as it is entirely general. It can be used to solve all flow conditions from liquids to gases as well as two-phase mixtures. As a reminder, this method constitutes finding the \\(P_t\\) and \\(T_t\\) that maximize the mass flux given by\n\\[ G_t = \\rho_t \\sqrt{2 \\int_{P_t}^{P_1} v dP} \\]\nFirst introduce the change of variables \\(\\Delta P = P_1 - P\\) such that the integration is from \\(0\\) to \\(P_1 - P_2\\).\n\\[ \\int_{P_2}^{P_1} v dP = - \\int_{0}^{\\Delta P} v\\left( P_1 - \\Delta P \\right)_{s = s_1} d\\left(\\Delta P \\right) \\]\nThis allows us to write the corresponding differential equation\n\\[ { {d} \\over {d \\left(\\Delta P \\right)} } I = v\\left(P₁ - ΔP\\right) \\]\nsubject to the constraint\n\\[ s(P_1 - \\Delta P, T) = s(P_1, T_1) \\]\nWhich can be implemented as a differential algebraic equation using DifferentialEquations.jl\nusing DifferentialEquations\nfunction rhs(u, params, ΔP)\n    ∫vdP, T = u\n    model, P₁, s₁, z, Mw = params\n    P = P₁ - ΔP\n    return [ volume(model, P, T, z)/Mw\n             s₁ - entropy(model, P, T) ]\nend\nBut we want to stop the integration when \\({ {\\partial G} \\over {\\partial \\left( \\Delta P \\right) } } = 0\\) or, equivalently, when the velocity is sonic. We can show that these are the same by finding the stationary points of \\(G^2\\)\n\\[ { {\\partial G^2} \\over {\\partial \\left( \\Delta P \\right) } } = { {\\partial } \\over {\\partial \\left( \\Delta P \\right) } } \\left( 2 \\rho_t^2 \\int_0^{\\Delta P_t} v d \\left( \\Delta P \\right) \\right) = 0 \\]\nby applying the chain rule and cancelling \\(\\rho\\) we get\n\\[ 2 \\left( {\\partial \\rho} \\over { \\partial P } \\right)_S \\int_0^{\\Delta P_t} v d \\left( \\Delta P \\right) - 1 = 0 \\]\nrecalling the definition of the speed of sound (above)\n\\[ \\left( {\\partial \\rho} \\over { \\partial P } \\right)_S = \\frac{1}{c^2} \\]\nwe have\n\\[ 2 \\int_0^{\\Delta P_t} v d \\left( \\Delta P \\right) - c^2 = 0 \\]\nwhich is simply restating \\(u_t = c_t\\).\nfunction ∂G²_callback(u, ΔP, integrator)\n    ∫vdP, Tₜ = u\n    model, P₁, s₁, z, Mw = integrator.p\n    Pₜ = P₁ - ΔP\n    c = speed_of_sound(model, Pₜ, Tₜ, z)\n    return 2∫vdP - c^2\nend\nfunction mass_flux_direct_integration(model, P₁, T₁, P₂; \n                                      z=[1.0], solver=Rodas5P())\n    s₁ = entropy(model, P₁, T₁, z)\n    Mw = molecular_weight(model, z)\n\n    # defining the ODEFunction\n    M = [ 1 0\n          0 0 ]\n    f = ODEFunction(rhs, mass_matrix = M)\n\n    # defining the ODEProblem\n    u0 = [0.0; T₁]\n    params = (model, P₁, s₁, z, Mw)\n    ΔP_span = (0.0, P₁ - P₂)\n    prob = ODEProblem(f, u0, ΔP_span, params)\n    cb = ContinuousCallback(∂G²_callback, terminate!)\n\n    # solving the DAE\n    sol = solve(prob, solver, callback=cb)\n\n    # unpacking the solution\n    ΔPₜ = sol.t[end]\n    ∫vdP, Tₜ = sol.u[end]\n    ρₜ = mass_density(model, P₁-ΔPₜ, Tₜ, z)\n    G = ρₜ*√(2*∫vdP)\nend\nfunction mass_flux_direct_integration(model, P₁::Quantity, T₁::Quantity,\n                          P₂::Quantity; z=[1.0])\n    P_1 = ustrip(u\"Pa\", P₁)\n    P_2 = ustrip(u\"Pa\", P₂)\n    T_1 = ustrip(u\"K\", T₁)\n    return mass_flux_direct_integration(model, P_1, T_1, P_2; z=z)*1u\"kg*m^-2*s^-1\"\nend\nDirect integration of the VTPR equation of state gives a theoretical mass flux of 54629 kg m^-2 s^-1 ( 54353 kg m^-2 s^-1 from GERG-2008 ). Which is exactly the same as from solving the choked flow energy balance, as expected.\n\n\n\n\n\n\nFigure 4: The mass flux as a function of nozzle pressure drop, showing the intermediate steps until a maximum was found.\n\n\n\nWriting this as a differential algebraic equation was largely necessary because Clapeyron.jl does not expose any routines to calculate the volume as a function of pressure and entropy. Some libraries like CoolProps do, in which case the code could be simplified to be a one dimensional ode.\nThis method could be extended to include liquid and two-phase flows however, as it is currently implemented, it only handles gases. Unlike the energy balance method, though, the flow does not have to be choked. If the flow is not choked, the maximum will occur once the nozzle pressure reaches \\(P_2\\). This result will simply pop out without any extra effort."
  },
  {
    "objectID": "posts/relief_valve_sizing/index.html#comparing-the-results",
    "href": "posts/relief_valve_sizing/index.html#comparing-the-results",
    "title": "Relief Valve Sizing with Real Gases",
    "section": "Comparing the Results",
    "text": "Comparing the Results\nFor the sake of completeness, there are two other methods that should be looked at, which are really special cases: 1. the ideal gas case, but using the real compressibility, \\(Z\\), at stagnation conditions, this is the API 520 standard approach for gases 2. using the isentropic expansion factor, n factor, method but calculating n at the average of the stagnation and nozzle conditions\nThese two approaches do better than the basic methods I presented, but I don’t think they add enough value on their own. Given a model of the gas which can generate the compressibility, using either the energy balance method or the direct integration method produces superior results than correcting the ideal gas case. Once a viable equation of state is in hand, the simplifications are not saving any actual engineer doing their job time, they are saving fractions of a second of compute time.\nI think the choice between the first law energy balance and the direct integration technique is more a matter of taste, at least in the case of choked flow. The direct integration method is in the relevant engineering codes/standards, and that is a strong justification for using it.\n\n\n\n\n\n\nFigure 5: A comparison of calculated theoretical mass flux for the six methods. The results from the first law energy balance and direct integration are identical.\n\n\n\nIn this case the choice of equation of state did not matter strongly, just for fun I have included a few other common cubic equations of state, they all perform reasonably. However this example is for a single compound that is not strongly associating, it is the type of example where cubic equations of state should work well. The choice of equation of state will be far more important with mixtures and strongly associating substances."
  },
  {
    "objectID": "posts/relief_valve_sizing/index.html#final-thoughts",
    "href": "posts/relief_valve_sizing/index.html#final-thoughts",
    "title": "Relief Valve Sizing with Real Gases",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI have long been an advocate for engineering to move out of using spreadsheets for everything and to use scripting languages and notebooks like Jupyter and Pluto far more. There are large classes of problems that are easy to solve with code and hard to solve with a spreadsheet. I think almost any calculation using equations of state fit into that category. We end up beholden to commercial software suppliers for calculations that, in my view, engineers should be doing themselves.\nPresumably you could do the calculations I laid out above in Excel, at enormous effort, and making liberal use of the solver. Julia, however, has a robust ecosystem for doing all the complicated math, it only needed to be connected up. What remains, for the engineer, is assessing the physical system and picking the appropriate methods and thermodynamic models."
  },
  {
    "objectID": "posts/relief_valve_sizing/index.html#references",
    "href": "posts/relief_valve_sizing/index.html#references",
    "title": "Relief Valve Sizing with Real Gases",
    "section": "References",
    "text": "References\n\n\nAPI. Standard 520, Sizing, Selection, and Installation of Pressure-Relieving Devices, Part i - Sizing and Selection, 10th Ed. Washington, DC: American Petroleum Institute, 2020.\n\n\nChemical Process Safety, Center for. Guidelines for Pressure Relief and Effluent Handling Systems. Hoboken, NJ: John Wiley & Sons, 2017.\n\n\nCrowl, Daniel A., Lawrence G. Britton, Walter L. Frank, Stanley Grossel, Dennis Hendershot, W. G. High, Robert W. Johnson, et al. “Process Safety.” In Perry’s Chemical Engineers’ Handbook, edited by Don W. Green. New York: McGraw Hill, 2008.\n\n\nGmehling, Jürgen, Bärbel Kolbe, Michael Kleiber, and Jürgen Rary. Chemical Thermodynamics for Process Simulation. Weinheim, DE: Wiley-VCH Verlag & Co., 2012.\n\n\nGreen, Don W., ed. Perry’s Chemical Engineers’ Handbook. New York: McGraw Hill, 2008.\n\n\nTilton, James N. “Fluid and Particle Dynamics.” In Perry’s Chemical Engineers’ Handbook, edited by Don W. Green. New York: McGraw Hill, 2008."
  },
  {
    "objectID": "posts/worst_case_weather/index.html",
    "href": "posts/worst_case_weather/index.html",
    "title": "Worst Case Meterological Conditions",
    "section": "",
    "text": "In a previous post I modeled an example of a plume from an elevated stack. In that example I assumed very stable conditions and a low windspeed – pasquill stability class F and a windspeed of 1.5m/s – as the worst case. This was an error!\nFor neutrally buoyant releases at or near ground-level that is a common “worst case”, for example when considering the potential impact due to a vapour cloud explosion. But for elevated stacks, releasing a buoyant plume, a class D stability with a moderate windspeed is often recommended. I thought it would be interesting to explore how the maximum concentration at the point of interest – an elevated work area downwind of the stack – varies with stability class and windspeed.\nI am not going to repeat all of the assumptions and working out of the previous notebook, the important results are in the source code for this post. I have also re-defined some of the functions to be a little more re-useable and to represent other stability cases not covered in the original notebook."
  },
  {
    "objectID": "posts/worst_case_weather/index.html#pasquill-stability",
    "href": "posts/worst_case_weather/index.html#pasquill-stability",
    "title": "Worst Case Meterological Conditions",
    "section": "Pasquill Stability",
    "text": "Pasquill Stability\nAs a refresher, Pasquill stability classes are a qualitative way of describing the atmospheric stability – the tendency of the atmosphere to resist or enhance vertical motion. Stability is itself related to the temperature gradient with height, wind speed, and various other things. For a simple model such as this the key model parameters are tabulated with respect to the Pasquill stability, which is why it is relevant to this discussion.\n\nPasquill Stability Classes\n\n\n\nStability Class\nDescription\n\n\n\n\nA\nExtremely unstable\n\n\nB\nUnstable\n\n\nC\nSlightly unstable\n\n\nD\nNeutral\n\n\nE\nSlightly stable\n\n\nF\nStable to extremely stable\n\n\n\nIn general the more stable the class the less dispersion, and thus the higher the concentration within the plume. Which is why class F is typically used for a ground level, neutrally buoyant, cloud. However plume rise is also a function of stability and, in general, more stable plumes rise without as much dispersion and thus the ground level concentration is lower than if the plume dispersed more. Furthermore the plume rise is a function of windspeed, the greater the windspeed the less the plume rises before leveling off and, again, the greater the ground level concentration.\nWe can visualize the relationship between windspeed, stability class, and the concentration at the point of interest with the following plot which includes plume rise relationships for unstable, neutral, and stable atmospheres.\n\n\n\n\n\n\n\n\n\nWe note that, as we expect, lower stability (e.g. A or B) corresponds to a higher groundlevel concentration at low windspeed, but at high windspeed higher stability leads to a greater groundlevel concentration.\nAt first blush it would appear that class F is still the worst case, however this plot naively assumes atmospheric stability is unrelated to windspeed. This is not true and roughly speaking the stability transitions towards classes C and D as the windspeed increases.\n\n\nPasquill Stability and Windspeed\n\nPasquill Stability vs Incoming Solar Radiation\n\n\n\nWindspeed (m/s)\nStrong\nModerate\nSlight\n\n\n\n\n&lt; 2\nA\nA - B\nB\n\n\n2 - 3\nA - B\nB\nC\n\n\n3 - 5\nB\nB - C\nC\n\n\n5 - 6\nC\nC - D\nD\n\n\n&gt; 6\nC\nD\nD\n\n\n\n\n\nPasquill Stability vs Nighttime Cloud Cover\n\n\n\nWindspeed (m/s)\n&gt; 4/8 cloud\n&lt; 3/8 cloud\n\n\n\n\n&lt; 2\n\n\n\n\n2 - 3\nE\nF\n\n\n3 - 5\nD\nE\n\n\n5 - 6\nD\nD\n\n\n&gt; 6\nD\nD\n\n\n\nTo represent this crudely, the plots can be chopped off at the windspeed limits from the tables above. So, for example, the class F plot would end at 3m/s.\nThe plots also present an obvious way of finding the worst case for a particular scenario: find the extremal point for the worst stability class. This is found rather simply by setting the derivative to zero. Something that would be complicated to do analytically but is quite straight forward when using the ForwardDiff library for automatic differentiation.\n\n# ForwardDiff doesn't play nicely with unitful\n# so values have to be stripped of units first\n\nFb′ = ustrip(Fb)\nx₁′ = ustrip(x₁)\nh₁′ = ustrip(h₁)\nQ′  = ustrip(Q)\nhₛ′ = ustrip(hₛ)\n\n∂Cᵤ(u) = ForwardDiff.derivative(u -&gt; C(u, x=x₁′, \n                                          y=0.0, \n                                          z=h₁′, \n                                          Q=Q′, \n                                          h=hₛ′, \n                                          Δh=(x,u) -&gt; Δhᵣ(x, u, Fb=Fb′, stable=false), \n                                          σy=σy(\"D\"), \n                                          σz=σz(\"D\")), float(u))\n\n∂Cᵤ (generic function with 1 method)\n\n\n\n# Find the point where ∂C/∂u = 0\n# Initial guess of 25 just by eye-ball\n\nu_worst = find_zero(∂Cᵤ, 25)\n\nC_worst = C(x₁, 0.0u\"m\", h₁, \n            u=u_worst*1u\"m/s\", \n            Q=Q, \n            h=hₛ, \n            Δh=(x,u) -&gt; Δhᵣ(x, u, Fb=Fb, stable=false), \n            σy=σy(\"D\"), \n            σz=σz(\"D\"))\n\nuconvert(u\"mg/m^3\", C_worst)\n\n0.2753490886768969 mg m^-3\n\n\n\n\n\n\n\n\n\n\n\nWe find that the worst case is indeed class D but with quite a high windspeed, ~26.4m/s or 95kph, which would be considered a 10 on the Beaufort scale with trees being uprooted and considerable structural damage. It’s unlikely that workers would still be on the platform and it may not even be the case that the scaffolding would still be standing!\nRegardless we can look at the contour plots at the work platform elevation and vertically, along the centerline.\nNote the colours are scaled to 4mg/m^3, one tenth the occupational limit of 40mg/m^3.\n\n\n\n\n\n\n\n\n\nAnother interesting impact of elevated releases like this is that the worst concentration for an observer on the ground is often a significant distance downwind of the stack. Because the plume must disperse downwards.\nBelow is a contour plot showing the downwind concentration at a 2m elevation – the height of a reasonably tall person. Note the scale is set to even lower concentrations. The maximum of the colour bar is 1000x lower than the occupational limit."
  },
  {
    "objectID": "posts/worst_case_weather/index.html#multiple-concentrations",
    "href": "posts/worst_case_weather/index.html#multiple-concentrations",
    "title": "Worst Case Meterological Conditions",
    "section": "Multiple Concentrations",
    "text": "Multiple Concentrations\nPreviously, in the discussion of the occupational exposure limit I noted that, in general, one would have to account for the impact of multiple substances in the flue gas, though in that particular example I was only modeling carbon monoxide and I just moved on. I think I left the impression that one would have to model each substance separately and, at least with this simple gaussian dispersion model, that is very much not the case.\nConsider for some substance i being released with in-stack concentration \\(C_{s,i}\\), we can define a dimensionless “dilution” \\(\\chi\\) as\n\\[ \\chi \\left( x, y, z \\right) = { C_i \\left( x, y, z \\right) \\over C_{s,i} } \\]\nAssuming the in-stack concentration to be simply the mass emission rate of i divided by the volumetric flow-rate1\n1 At standard state, because the concentrations given for the occupational exposure limits are given in terms of a volume at standard state. This is also a potential error in the original model as it does not correct the concentrations back to standard state, nor does it really track temperature to make that even possible, especially near the stack.\\[ C_{s,i} = { Q_i \\over V_s^o } \\]\nand recalling the concentration function from the gaussian dispersion model\n\\[ C_i \\left( x, y, z \\right) = {Q_i \\over 2 \\pi u \\sigma_{ye} \\sigma_{ze} } f \\left( x, y, z \\right) \\]\nwhere $ f ( x, y, z ) $ is the products of the exponentials, and is a function of x, y, z only. Putting all that together we get an expression for the dilution that does not depend upon the substance being released\n\\[ \\chi \\left( x, y, z \\right) = {V_s^o \\over 2 \\pi u \\sigma_{ye} \\sigma_{ze} } f \\left( x, y, z \\right) \\]\nIf you have already done the modeling for a particular substance then calculate \\(\\chi\\) for the points of interest by dividing the concentrations by the in stack concentration, otherwise substitute the formula for \\(\\chi\\) given above for the concentration and model that instead.\nThen, when evaluating multiple substances, the test2\n2 From CCOHS\\[ \\sum_i {C_i \\left( x, y, z \\right) \\over T_i } \\lt 1 \\]\nbecomes\n\\[ \\chi \\left( x, y, z \\right) \\cdot \\sum_i {C_{s,i} \\over T_i } \\lt 1 \\]\nwhere \\(T_i\\) is the relevant occupational exposure limit.\nTo recap, instead of calculating the concentrations at the points of interest using a gaussian dispersion model multiple times, calculate a dimensionless dilution at the points of interest and apply that to the in stack concentrations of all of the substances of interest. Then combine those as per the relevant rules for occupational hygiene.\nBelow are a series of contour plots showing the dilution \\(\\chi\\), where colours are are from 0-5% – i.e. the concentration within the yellow region is ≥ 5% the in-stack concentration.\nNote: This is backwards to the usual way of defining dilution, where a \\(\\chi\\) of 5% would be a 95% dilution."
  },
  {
    "objectID": "posts/worst_case_weather/index.html#thoughts-on-code-and-reusability",
    "href": "posts/worst_case_weather/index.html#thoughts-on-code-and-reusability",
    "title": "Worst Case Meterological Conditions",
    "section": "Thoughts on Code and Reusability",
    "text": "Thoughts on Code and Reusability\nA simple way of taking the work from a previous notebook and adding to it is just to import the notebook. This loads the results into the current notebook including any function definitions and such.\nFor example\n\nusing NBInclude\n@nbinclude(\"2020-12-05-gaussian_dispersion_example.ipynb\")\nI didn’t do that here for two reasons:\n\nI want these notebooks to be independent and stand on their own\nI didn’t write the previous notebook in a very extendable or reusable way\n\nThe second point is worth going into if one wants to build a library of worked out, generic, models as notebooks. This way an engineer can import previously defined models as needed for a particular analysis while also keeping the documentation for the models in the model. In the previous notebook I left most things in the global namespace and defined functions that used those global variables. Which is fine for that particular notebook but it means that the current workspace gets very cluttered when importing things and also those functions are not very re-usable as they were defined for a very particular example.\nIt’s better, I think, to write functions that use keyword arguments for any important parameters that can then be passed as needed, instead of defining those parameters in the global namespace. Unless they are truly constants, like g the acceleration due to gravity or R the universal gas constant.\nAnother point is on the use of the library Unitful. It is convenient, and a good check, to have units propagate through calculations, however Unitful does not play nicely with all libraries. This is especially the case with Plots but it can also be real hassle to use with correlations that have lots of parameters. I think this is a good opportunity to take advantage of julia’s multiple dispatch.\nFor example, suppose a correlation of the form \\(f \\left( x \\right) = a \\cdot x^b\\), this can be written in julia very simply (supposing a and b are parameters)\n\nf(x; a, b) = a*x^b\nBut if we pass x with some units and don’t pass the matching units with the parameter a this will throw an error. We could tediously work out the units for each set of parameters a and b to make the units cancel out properly, or we could use multiple dispatch to manage this for us\n\nfunction f(x::Quantity; a, b)::Quantity\n    x′ = ustrip(u\"expected input unit\", x)\n    return f(x′, a=a, b=b)*1u\"correlation output unit\"\nend\nWhere we convert the input to the expected units, whatever they may be, evaluate the function in a unitless way, then tack on the expected output units at the end. Now when we use f(x) in contexts without units, for example when plotting f(x), it works as expected and if we pass a value of x with units attached we get the unit conversion/checking that we want from Unitful."
  },
  {
    "objectID": "posts/engineering_a_cup_of_coffee_part-2/index.html",
    "href": "posts/engineering_a_cup_of_coffee_part-2/index.html",
    "title": "Engineering a Cup of Coffee Part Two: Espresso",
    "section": "",
    "text": "In a previous post I thought about how one might approach making coffee, in a French press, as a chemical engineering problem. The obvious next step is to look at percolation methods like espresso, which is what I am exploring here.\nThe same basic principles of extraction and diffusion apply to an espresso maker as apply to a French press, and we expect the same basic parameters to be relevant: the particle size, solid phase diffusivity, etc. The major difference is that the liquid phase, water, is moving through a fixed bed of coffee particles and this significantly changes the mass transfer problem.\nIn the case of the French press I made the simplifying assumption that the system was well mixed, and so I could generally ignore the flow of the fluid. In the case of the espresso maker, it is more complicated than that."
  },
  {
    "objectID": "posts/engineering_a_cup_of_coffee_part-2/index.html#packed-bed-leaching",
    "href": "posts/engineering_a_cup_of_coffee_part-2/index.html#packed-bed-leaching",
    "title": "Engineering a Cup of Coffee Part Two: Espresso",
    "section": "Packed Bed Leaching",
    "text": "Packed Bed Leaching\nMaking espresso involves packing coffee grounds into a cylindrical puck within a portafilter then passing hot water through the grounds under pressure, up to 10 bar.1 The specific unit operation that corresponds to making espresso is packed bed leaching, in which the coffee solubles are being leached from the coffee grounds. In a lot of standard undergraduate texts on unit operations and separations, I find, this is not well explored. Especially the non-equilibrium case, which is exactly the situation when pulling a shot of espresso: one doesn’t typically fully extract the beans and stops at some point in the transient regime. That said, the basic system is the same as desorption and so a good reference for what follows is often a text on chromatography and adsorption/desorption processes.\n1 Cameron et al., “Systematically Improving Espresso,” 631.\nSetting up the Governing Equations\nThe espresso puck can be modelled as a perfectly cylindrical packed bed of ground coffee. Meaning both that the bed is a perfect cylinder and that the grounds are perfectly evenly distributed, i.e. has a constant porosity \\(\\varepsilon\\). A lot of technique goes into preparing the puck, to ensure the grounds are evenly distributed in the packed bed and the distribution of water through the bed is even, so this is a reasonable assumption.\n\n\n\n\n\n\nFigure 1: The problem domain, a cylindrical puck of coffee.\n\n\n\nIn practice the pressure can vary over the course of the shot, but for simplicity I am only considering the case where the pressure is constant and, consequently, the flow rate of water is constant. We can simplify the flow condition more by assuming plug-flow, i.e. the velocity is a constant throughout. One final simplifying assumption is that the espresso is pulled at a constant temperature, this ensures the physical properties of water are also constant, e.g. constant density and viscosity.\nAll of these assumptions, at their core, are to eliminate various partial derivatives and narrow down the governing equations to only core variables.\n\nLiquid Phase\nZooming in on a thin slice of the packed bed with depth, Δz, we can take a mass balance of the liquid phase.\n\n\n\n\n\n\nFigure 2: Mass transfer in the thin slice of the column.\n\n\n\nThe mass flow into the liquid phase can be due to:\n\nadvection, Qc, where c is the liquid phase concentration entering the slice and Q is the volumetric flow rate\naxial diffusion, \\(\\varepsilon A J_z\\), where Jz is the mass flux at the top of the slice and \\(\\varepsilon A\\) is the porous area of the top of the slice, i.e. the area available to liquid flux.\nmass transfer from the coffee grounds (the solid phase), \\(A_s J_s\\), where Js is the mass flux of solubles from the grounds into the liquid phase and As is the surface area of the grounds\n\nSimilarly the mass flow out of the liquid phase can be due to advection or axial diffusion out the bottom of the slice. Putting that together into a mass balance we have\n\\[ { {d m_l} \\over {d t} } = Q c_z - Q c_{z + \\Delta z} + \\varepsilon A J_z - \\varepsilon A J_{z+\\Delta z} + A_s J_s \\]\n\\[ V_l { {d c} \\over {d t} } = - Q \\Delta c - \\varepsilon A \\Delta J_z + a_s V_s J_s \\]\nwhere as is the specific area of the coffee grounds, the surface area per unit volume. By writing the volume of the liquid phase and solid phase in terms of porosity and cross sectional area, A, we can cancel out some terms.\n\\[ \\varepsilon A \\Delta z { {d c} \\over {d t} } = - \\varepsilon A v \\Delta c - \\varepsilon A \\Delta J_z + \\left( 1 - \\varepsilon \\right) A {\\Delta z} a_s J_s\\]\n\\[ { {d c} \\over {d t} } = - v { {\\Delta c} \\over {\\Delta z} } - { {\\Delta J_z} \\over {\\Delta z} } + \\left( {1 - \\varepsilon} \\over \\varepsilon \\right) a_s J_s \\]\nIn the limit Δz → 0 this becomes\n\\[ { {\\partial c} \\over {\\partial t} } = - v { {\\partial c} \\over {\\partial z} } - { {\\partial J_z} \\over {\\partial z} } + \\left({ 1 - \\varepsilon } \\over \\varepsilon \\right) a_s J_s \\]\nAssuming axial diffusion is Fickian\n\\[ { {\\partial c} \\over {\\partial t} } = \\mathscr{D}_l { {\\partial^2 c} \\over {\\partial z^2} } - v { {\\partial c} \\over {\\partial z} } + \\left({ 1 - \\varepsilon } \\over \\varepsilon \\right) a_s J_s \\]\n\n\nSolid Phase\nFor the solid phase, I am assuming a uniform bed of spherical particles, all with the same radius b. Actual coffee grounds, beyond being non-spherical, are also composed of multiple phases: the solid phase, the coffee oils, and the liquid water phase within the micro-porous structure of the bean. To simplify things greatly, I am assuming an effective solid phase diffusion, wherein diffusion within the particle of coffee follows Fick’s law with an effective diffusion coefficient that combines all of that complexity into a single parameter.\n\\[ { {\\partial q} \\over {\\partial t} } = \\mathscr{D}_s \\nabla^2 q\\]\nIn spherical coordinates, this becomes\n\\[ { {\\partial q} \\over {\\partial t} } = \\mathscr{D}_s \\frac{1}{r^2} {\\partial \\over {\\partial r} } \\left( r^2 { {\\partial q} \\over {\\partial r} } \\right)\\]\n\\[ { {\\partial q} \\over {\\partial t} } = \\mathscr{D}_s \\left( { {\\partial^2 q} \\over {\\partial r^2} }  + {2 \\over r} { {\\partial q} \\over {\\partial r} }  \\right) \\]\nWhere the solid phase concentration, q, is in units of mass per unit volume.\n\n\nThin Film\nConnecting the two phases, the liquid coffee and the solid coffee grounds, is a thin film. This is where the inter-phase mass transfer occurs, and I assume it follows a linear mass transfer relation.\n\\[ J_s = h \\left( c_{s} -  c \\right) \\]\nWhere h is the mass transfer coefficient and cs is the liquid concentration immediately at the solid surface. I am making the additional assumption that this concentration is in equilibrium with the solid phase concentration at the surface, and that equilibrium is linear, i.e.\n\\[ K = c_{s}/q_{s} = \\mathrm{constant} \\]\n\n\n\nInitial Conditions\nThe initial conditions for espresso are complicated. The bed is initially full of air and the first phase of making espresso, the pre-infusion, is to saturate the bed with hot water at a lower pressure than is used during the main extraction phase. This initial step has a very complicated multi-phase flow and mass transfer which dramatically complicates the model and most papers I’ve read avoid this by making one of two simplifying assumptions:\n\nThe bed is initially full of water, but that water has no coffee extracted into it.\nThe bed is initially full of water, and that water is in equilibrium with the coffee grounds (e.g. fully saturated).\n\nBasically everyone ignores pre-infusion and focuses on the main extraction phase, after the bed has been filled with water. I am going to make the second assumption, that after pre-infusion the bed is full of water that is in equilibrium with the solids. This is in part because it is a convenient initial condition for solving the partial differential equations2 and in part because the actual volume of water in the bed is small and the pre-infusion step, which can take between 5-10s, is sufficiently long enough that the water will have extracted some coffee.\n2 Schwartzberg, “Leaching – Organic Materials,” 559.For my model, the initial conditions are:\n\nthe solid concentration in the grounds is the saturation concentration\nthe liquid concentration in the bulk is, initially, in equilibrium and also saturated\nthe boundary condition is that the water entering the system has a concentration 0 mg/m3 coffee solubles\n\nSeveral recent papers that numerically integrate the pde use some variation on the first assumption3 and this will be the major difference between my approach and some of the published literature.\n3 Cameron et al., “Systematically Improving Espresso” page 635; Moroney et al., “Modelling of Coffee Extraction During Brewing Using Multiscale Methods” page 225; Vaca Guerra et al., “Modeling the Extraction of Espresso Components as Dispersed Flow Through a Packed Bed” page 5"
  },
  {
    "objectID": "posts/engineering_a_cup_of_coffee_part-2/index.html#determining-the-parameters-of-the-system",
    "href": "posts/engineering_a_cup_of_coffee_part-2/index.html#determining-the-parameters-of-the-system",
    "title": "Engineering a Cup of Coffee Part Two: Espresso",
    "section": "Determining the Parameters of the System",
    "text": "Determining the Parameters of the System\nI am assuming the water and portafilter are isothermal and the liquid phase has the same physical properties of pure water throughout. This is not entirely true in that the system is not perfectly isothermal but also the process of extracting coffee changes the density and viscosity of the coffee. I am assuming this effect is small and can be ignored.\n\nusing Unitful\n\n# physical properties coffee\n# assumed to be water at 90C and 10bar\nρ = 965.34u\"kg/m^3\"\nμ = 0.282*0.001u\"Pa*s\"\nν = μ/ρ\nν = upreferred(ν)\n\n\nParameters of the Packed Bed\nThe most significant factors for both modelling the flow and also determining the mass transfer parameters are the features of the espresso bed itself. Primarily the porosity and the particle size. For simplicity I am taking both of these from the literature for actual espresso shots,4 but in practice these are probably the most difficult to derive for someone making espresso at home, with the sorts of tools available in a kitchen.\n4 Cameron et al., “Systematically Improving Espresso” supplemental materials.The porosity will vary from espresso shot to espresso shot, as it is a function of the particle size distribution, the distribution within the portafilter, and also the degree of tamping. Furthermore the porosity of the dry bed will not be the same as the porosity of the bed once fully saturated with water. The coffee grounds will swell somewhat and any liquid within a particle is already accounted for in the effective solid phase mass transfer, including it in an estimate of the porosity would be double counting. One could try measuring the mass and volume of the spent puck and calculate it directly, but I’m not sure how one would account for the volume of water absorbed into the grounds while discarding the water that is only in the void space between coffee particles.\nThe particle size distribution for a given coffee, grinder, and grind setting can be measured in a variety of ways, including with an app on one’s phone5 where in this case we are interested in the Sauter mean radius as we are assuming a bed of uniform spherical particles. Camera based approaches have one main weakness in that particles of coffee have microscopic pores that increase the apparent surface area but are too small to be resolved by a typical camera. Laboratory methods tend to measure the adsorption of a neutral substance, like nitrogen, to measure this. Not something one is going to be doing in the kitchen.\n5 Gagné, The Physics of Filter Coffee, 199.The flow rate through the bed is also an important factor, I am simply taking the total volume of the shot divided by the time taken to pull a shot as the flow rate, but this is only a rough estimate. The pre-infusion phase adds water at a lower flow rate and it is only the flow after the pre-infusion has ended that is relevant to the problem. This can be measured with some higher end coffee scales that can output the time series of mass measurements during a shot. I don’t have one of these, but it’s not out of the question that I could just write down the mass and time at several points. Or take a video of my coffee scale’s screen during the actual shot and extract the data very tediously that way.\n\n# Cameron et al, \"Systematically Improving Espresso,\" supplemental materials.\n\n# porosity and particle size\nε = 1 - 0.8272\nb = 12e-6u\"m\"\n\n# bed size\nR_pb = 29.2e-3u\"m\"\nL_pb = 18.7e-3u\"m\"\nA_pb = π*R_pb^2\n\n# shot size\nM_shot = 0.04u\"kg\" # the mass of the espresso shot\nt_shot = 20u\"s\"\nQ_shot = (M_shot/ρ)/t_shot\n\nv_s = Q_shot/A_pb # superficial velocity, m/s\nv = v_s/ε\n\nFrom the dimensions of the packed bed, assumed porosity, and assumed flow rate, I can calculate the time for the water to traverse the bed.\n\n(ε*A_pb*L_pb)/Q_shot\n\n4.177834449522944 s\n\n\n\n\nMass Transfer Parameters\nIn practice, for a lot of chemical engineering mass transfer problems, accurate mass transfer coefficients and diffusivities are hard to come by. This is equally true for the espresso system. I am going to be using a literature value for the effective solid phase diffusion, but then estimating the remainder from correlations.\n\n# effective solid phase diffusivity\n# Cameron et al, \"Systematically Improving Espresso,\" 11.\n𝒟ₛ = 6.25e-10u\"m^2/s\"\n\n# (stagnant) liquid diffusivity\n# Schwartzberg, “Leaching – Organic Materials,” 557.\nD = 5*𝒟ₛ\n\nThe thin film mass transfer coefficient is typically estimated from the Sherwood number which is a function of the Reynolds number and Schmidt number\n\n# Reynolds number\nRe = v_s*(2b)/ν\n\n\n# Schmidt number\nSc = ν/D\n\nThe Sherwood number can be estimated using the Wilson-Geankopolis correlation for packed bed flow\n\n# Wilson-Geankopolis correlation\n# Hottel et al, \"Heat and Mass Transfer,\" 5-77.\nSh = (1.09/ε)*∛(Re*Sc)\n\nGiving the thin film mass transfer coefficient\n\nh = Sh*D/(2b)\n\n0.0014874874803418145 m s^-1\n\n\nThe axial diffusion can be calculated using the Edwards-Richardson correlation\n\n# Edwards-Richardson correlation\n# LeVan and Carta, \"Adsoprtion and Ion Exchange,\" 16-22.\nγ₁ = 0.45 + 0.55*ε\nγ₂ = 0.5*(1 + 13γ₁*ε/(Re*Sc))^-1\nPe = ( γ₁*ε/(Re*Sc) + γ₂ )^-1\n\n\n𝒟ₗ = v*(2b)/Pe\n\n4.623616336378663e-8 m^2 s^-1\n\n\n\n\nEquilibrium Constant\nI am using literature values for the saturated concentration of solubles both in the bean and in the coffee, and calculating an equilibrium constant from that.\n\n# saturation concentrations\n# Cameron et al, \"Systematically Improving Espresso,\" 11.\nq_sat = 118.0u\"kg/m^3\"\nc_sat = 212.4u\"kg/m^3\"\nK = q_sat/c_sat\n\n0.5555555555555556\n\n\n\n\nA Packed Bed Data Structure\nLooking forward a little bit, I know that I will be using multiple approaches to the packed bed model and keeping track of all of the model parameters can be tricky. Especially in a notebook where everything is in the global name space. Which is why I think it is prudent to define a PackedBed data structure to contain all of the model parameters.\n\nstruct PackedBed\n    q₀\n    K\n    𝒟ₛ\n    𝒟ₗ\n    h\n    ε\n    b\n    c₀\n    v\nend\n\n\n# initial concentration\nc₀ = 0.0u\"kg/m^3\"\n\n\npb = PackedBed(q_sat, K, 𝒟ₛ, 𝒟ₗ, h, ε, b, c₀, v);"
  },
  {
    "objectID": "posts/engineering_a_cup_of_coffee_part-2/index.html#anzelius-integral-solution",
    "href": "posts/engineering_a_cup_of_coffee_part-2/index.html#anzelius-integral-solution",
    "title": "Engineering a Cup of Coffee Part Two: Espresso",
    "section": "Anzelius’ Integral Solution",
    "text": "Anzelius’ Integral Solution\nA good first approach to solving the pde is to try simplifying the mass transfer problem by eliminating some of the diffusion terms. Making the following simplifications:\n\nthe rate of mass transfer across the thin film dominates, and thus the solid phase diffusion can be neglected\nthe mass flow into a given slice of the packed bed is dominated by advection, and the axial dispersion can be neglected\n\nThe governing equations can be reduced to\n\\[ { {\\partial c} \\over {\\partial t} } = - v { {\\partial c} \\over {\\partial z} } + \\left({ 1 - \\varepsilon } \\over \\varepsilon \\right) a_s J_s \\]\n\\[ { {\\partial q} \\over {\\partial t} } = - a_s J_s \\]\nwith\n\\[ J_s = h \\left( c_{s} -  c \\right) \\]\nThis is a dramatically simpler model, eliminating much of the real complexity of the mass transfer. However an effective mass transfer coefficient, h, can be fit from measured data that combines the solid diffusion and thin film mass transfer.6 Similarly an effective mass transfer coefficient can be calculated by addition of linear mass transfer resistances. Essentially this is shifting some of the complexity out of the governing equations and into the parameters. This is a fairly common model for packed beds and was first solved, for the equivalent heat transfer case, by Anzelius and independently by Schumann.7 What follows is a general sketch of a solution.\n6 See Moroney, “Analysing Extraction Uniformity from Porous Coffee Beds Using Mathematical Modelling and Computational Fluid Dynamics Approaches” for an example of this model being used for espresso extraction.7 Anzelius, “Über Erwärmung Vermittels Durchströmender Medien”; Schumann, “Heat Transfer”.8 Setting the pde in dimensionless form follows Bird, Stewart, and Lightfoot, Transport Phenomena pages 753-755The first step is to transform this pde into dimensionless form8, first by introducing a dimensionless time \\(\\tau = \\frac{h a_s}{K} \\left( t - \\frac{z}{v} \\right)\\). Which, when substituted into the equation for the solid phase, becomes\n\\[ {{\\partial q} \\over {\\partial \\tau}} = K \\left( c -  c_{s} \\right) \\]\nFurther introducing a dimensionless space \\(\\xi = \\frac{h a_s}{m v} z\\) where \\(m = \\left({ 1 - \\varepsilon } \\over \\varepsilon \\right)\\) transforms the equation for the liquid phase into\n\\[ {{\\partial c} \\over {\\partial \\xi}} = c_s - c \\]\nBy defining a dimensionless liquid phase concentration\n\\[ u = {{c - \\frac{q_0}{K}} \\over {c_0 - \\frac{q_0}{K}}} \\]\nwhere q0 is the initial concentration of the solid phase and c0 is the concentration in the water at z=0. We can re-write the equation for the liquid phase as\n\\[ {{\\partial u} \\over {\\partial \\xi}} = {{q - q_0} \\over {K c_0 - q_0}} - u \\]\nLetting\n\\[ y = { {q - q_0} \\over {K c_0 - q_0} } \\]\nthe final system of equations is then\n\\[ { {\\partial u} \\over {\\partial \\xi} } = y - u \\]\n\\[ { {\\partial y} \\over {\\partial \\tau} } = u - y \\]\nWhich, at this point, is just something that you can look up in Carslaw and Jaeger.9 The solution follows directly from taking the Laplace transform of \\({ {\\partial y} \\over {\\partial \\tau} }\\), with respect to τ, which gives\n9 Carslaw and Jaeger, Conduction of Heat in Solids, 393.\\[ sY = U - Y\\]\n\\[ Y = \\frac{1}{s+1}U \\]\nthen taking the Laplace transform of \\({ {\\partial u} \\over {\\partial \\xi} }\\), with respect to τ gives\n\\[ { {d U} \\over {d \\xi} } = Y - U = \\frac{-s}{s+1}U  \\]\nwhich is a differential equation that can be easily solved using the initial condition u(0) = 1 or, in the Laplace domain, U(0) = 1/s\n\\[ U = \\frac{1}{s} \\exp\\left( \\frac{-s}{s+1} \\xi \\right) \\]\nInverting this requires a little work, though not as much as it may seem. I am departing from Bird10 since I find their approach mystifying. It is clearly designed to reverse engineer a particular form of the answer as opposed to arriving at it naturally. The approach in Carslaw and Jaeger is more intuitive11 and is what I am following here.\n10 Bird, Stewart, and Lightfoot, Transport Phenomena, 762–63.11 Carslaw and Jaeger, Conduction of Heat in Solids pages 393-394; and also Goldstein, “On the Mathematics of Exchange Processes in Fixed Columns i. Mathematical Solutions and Asymptotic Expansions” pages 153-158.First, recognize that\n\\[ \\frac{1}{s} \\exp\\left( \\frac{-s}{s+1} \\xi \\right) = \\exp\\left( -\\xi \\right) \\frac{1}{s} \\exp\\left( \\frac{1}{s+1} \\xi \\right) \\]\nThen, looking at a table of Laplace transforms we find12\n12 With the caveat that you need a good table of Laplace transforms, most undergraduate textbooks have a very brief one. The tables in Carslaw and Jaeger are extensive and Perry’s is also a good reference.\\[ \\mathscr{L}^{-1} \\left\\{ \\frac{1}{s} \\exp\\left( \\frac{1}{s} x \\right) \\right\\} = I_0 \\left( 2 \\sqrt{xt} \\right)\\]\nwhere I0 is the modified Bessel function of the first kind. A basic property of Laplace transforms is that\n\\[ \\mathscr{L}^{-1} \\left\\{ F(s+a) \\right\\} = \\exp(-at)f(t) \\]\nfrom which it follows that\n\\[ \\mathscr{L}^{-1} \\left\\{ \\frac{1}{s+1} \\exp\\left( \\frac{1}{s+1} x \\right) \\right\\} = \\exp\\left( -t \\right) I_0 \\left( 2 \\sqrt{xt} \\right)\\]\nAnother property of Laplace transforms is that\n\\[ \\mathscr{L}^{-1} \\left\\{ \\frac{1}{s} F(s) \\right\\} = \\int_0^t f\\left( \\lambda \\right) d\\lambda \\]\nwhich gives\n\\[ \\mathscr{L}^{-1} \\left\\{ \\frac{1}{s} \\frac{1}{s+1} \\exp\\left( \\frac{1}{s+1} x \\right) \\right\\} = \\int_0^t \\exp\\left( -\\lambda \\right) I_0 \\left( 2 \\sqrt{x \\lambda} \\right) d\\lambda\\]\nWhich is laying the groundwork for the observation that since\n\\[ \\frac{1}{s} - \\frac{1}{s+1} = \\frac{1}{s}\\frac{1}{s+1} \\]\nthen\n\\[ \\frac{1}{s} = \\frac{1}{s+1} + \\frac{1}{s}\\frac{1}{s+1} \\]\nand U can be rewritten as\n\\[ U = \\exp\\left( -\\xi \\right) \\left[ \\frac{1}{s+1} \\exp\\left( \\frac{1}{s+1} \\xi \\right) +\\frac{1}{s} \\frac{1}{s+1} \\exp\\left( \\frac{1}{s+1} \\xi \\right) \\right] \\]\nthen by taking the inverse Laplace transform\n\\[ u = \\exp\\left( -\\xi \\right) \\left[ \\exp\\left( - \\tau \\right) I_0 \\left( 2 \\sqrt{\\tau \\xi} \\right) + \\int_0^\\tau \\exp\\left( -\\lambda \\right) I_0 \\left( 2 \\sqrt{\\lambda \\xi} \\right) d\\lambda \\right] \\]\n\\[ u = \\exp\\left( -(\\tau + \\xi) \\right) I_0 \\left( 2 \\sqrt{\\tau \\xi} \\right) + \\int_0^\\tau \\exp\\left( -(\\lambda + \\xi) \\right) I_0 \\left( 2 \\sqrt{\\lambda \\xi} \\right) d\\lambda \\]\nThis is the solution that Schumann13 arrives at via a different means, though it is not in the form one generally sees in the standard references. To get there, we must take advantage of some properties of the Anzelius J Function (named because it is the solution to this differential equation).14\n13 This is equation 27 in Schumann, “Heat Transfer” page 409.14 Goldstein, “On the Mathematics of Exchange Processes in Fixed Columns i. Mathematical Solutions and Asymptotic Expansions,” 160.\\[ J\\left( x, y \\right) = 1 - \\int_0^x \\exp\\left( -(\\lambda + y) \\right) I_0 \\left( 2 \\sqrt{\\lambda y} \\right) d\\lambda \\]\n\\[ J\\left( x, y \\right) + J\\left( y, x \\right) = 1 + \\exp\\left( -(x+y) \\right) I_0 \\left( 2 \\sqrt{ x y} \\right) \\]\nfrom which we can see that\n\\[ u = \\exp\\left( -(\\xi + \\tau) \\right) I_0 \\left( 2 \\sqrt{\\xi \\tau} \\right) + 1 - J\\left( \\tau, \\xi \\right) = J\\left( \\xi, \\tau \\right) \\]\nand finally\n\\[ u = 1 - \\int_0^\\xi \\exp\\left( - (\\tau + \\lambda) \\right) I_0 \\left( 2 \\sqrt{\\tau \\lambda} \\right) d\\lambda \\]\nWhich is the form typically given in references. I think it is important to pause here and comment that this answer is not the answer, it is an answer. Both the solution above from taking the inverse Laplace transform and this solution are valid and, in fact, both are used when evaluating the Anzelius J function. It just happens to be the case that the latter result is what one tends to see in the literature.\n\nDefining the Anzelius Solution\nAt this point we can calculate the dimensionless space and time for a point at the exit of the espresso bed, and at a similar point in (dimensionless) time.\n\nz = L_pb\n\nm  = ε/(1-ε)\naᵥ = 3/b\nξ  = (h*aᵥ*z)/(m*v)\n\n# τ = ξ\nt = (K/(h*aᵥ))*ξ + z/v\nτ  = (h*aᵥ/K)*(t - z/v)\n\n@show ξ; @show τ;\n\nξ = 7437.232219350375\nτ = 7437.232219350374\n\n\nIt is convenient to create an AnzeliusSolution struct that takes a PackedBed and a particular point in space and generates a datatype that allows us to go back and forth between the problem in dimensionless form and the problem in actual units.\n\nstruct AnzeliusSolution{F,Q1,Q2}\n    ξ::F\n    τ₁::Q1\n    τ₂::Q2\n    pb::PackedBed\nend\n\nfunction AnzeliusSolution(z, pb::PackedBed)\n    m  = ε/(1-ε) \n    aᵥ = 3/pb.b\n    ξ  = (pb.h*aᵥ*z)/(m*pb.v)\n    τ₁ = (pb.h*aᵥ/pb.K)\n    τ₂ = τ₁*(z/pb.v)\n\n    return AnzeliusSolution(ξ, τ₁, τ₂, pb)\nend\n\n\nanzelius = AnzeliusSolution(z, pb);\n\n\n\nEvaluating the Products of Exponentials and Bessel Functions\nGenerally the Anzelius solution is given in terms of an integral of a Bessel function that is wildly impractical to numerically integrate directly as written. For an example, Bird15 gives this as the the solution:\n15 Bird, Stewart, and Lightfoot, Transport Phenomena, 755.\\[ u = 1 - \\int_0^\\xi \\exp\\left( - (\\tau + \\lambda) \\right) J_0 \\left( i \\sqrt{4\\tau \\lambda} \\right) d\\lambda \\]\nwhere J0 in this case is the Bessel function of the first kind (not the Anzelius J function). This is correct, however, attempting to use it as written will run aground on numerical difficulties at even moderately large values of τ. The first issue with this form of the answer is that it requires one to cast everything into complex values only to cast back into floats (the answer is a real number), but the most important issue is that the integrand is the product of an exponential that decays rapidly to zero and a Bessel function that blows up rapidly to infinity. For even moderate values of τ this leads to NaN errors of the type 0*Inf.\nWhen I was first playing around with this I attempted to integrate it as is. After that clearly didn’t work, I looked into whether or not there are scaled versions of the Bessel function. I think this is a good practice that maybe isn’t taught well in school: often functions like Bessel functions or the Gamma function explode to large numbers that would overflow, thus leading to NaN errors, consequently libraries of special functions tend to have scaled or log versions. Bessels.jl has an exponentially scaled version of I0 that works perfectly for what I need, with the exponentially scaled version being\n\\[ I_{x,0}(z) = \\exp(-z) I_0(z) \\]\nBy completing the square we can rewrite\n\\[ u = 1 - \\int_0^\\xi \\exp\\left( - (\\tau + \\lambda) \\right) I_0 \\left( 2 \\sqrt{\\tau \\lambda} \\right) d\\lambda \\]\nas\n\\[ u = 1 - \\int_0^\\xi \\exp\\left( - \\left( \\sqrt{\\tau} - \\sqrt{\\lambda} \\right)^2 \\right) \\exp\\left(- 2 \\sqrt{\\tau \\lambda} \\right) I_0 \\left( 2 \\sqrt{\\tau \\lambda} \\right) d\\lambda \\]\n\\[ u =  1 - \\int_0^\\xi \\exp\\left( - \\left( \\sqrt{\\tau} - \\sqrt{\\lambda} \\right)^2 \\right) I_{x,0} \\left( 2 \\sqrt{\\tau \\lambda} \\right) d\\lambda \\]\nBelow is a figure showing a plot of the integrand for a value of τ much smaller than our particular example, just for illustration. The first curve is the original version of the solution (shifted up for visibility), which begins to fail due to NaN errors part-way up the curve (where the red X is). The second curve uses the exponentially scaled modified Bessel function and does not have this issue. The larger the τ the earlier problems arrive and by the time we get to the value of τ being used for this example the integrand doesn’t evaluate to any positive value prior to turning into NaNs.\n\nusing Bessels:besselj0, besseli0x\n\nf_orig(λ, τ) = exp(-(τ+λ))*real(besselj0(im*√(complex(4τ*λ))))\n\nfₐ(λ, τ) = exp(-(√(τ)-√(λ))^2)*besseli0x(√(4τ*λ))\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: The Anzelius integrand, the upper curve represents the original form which encounters numerical difficulties and fails to return valid answers after the red X, the lower curve is the transformed form of the integrand that does not have these difficulties.\n\n\n\n\n\n\nIntegration by Gauss-Kronold\nNow that we have a version of the integrand that we can actually calculate, the obvious approach is to integrate using a standard package for numerical integration, such as QuadGK.jl. To make things a little easier, I have made the change of variables \\(x = {\\lambda \\over \\xi}\\), thus changing the bounds of integration to \\(x \\in [0, 1]\\)\n\nusing QuadGK: quadgk_count\n\nintegrand(x) = exp(-(√(τ)-√(ξ*x))^2)*besseli0x(√(4τ*ξ*x))\n\n∫, e, N = quadgk_count(integrand,0,1)\n\nu = 1 - ξ*∫\n\n@show u; @show e; @show N;\n\nu = 0.5016355470840097\ne = 1.0518776639256381e-13\nN = 165\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Integrating the Anzelius integrand on the interval [0,1]\n\n\n\n\nBecause of my particular choice of τ the integral stops somewhere on the curve where it is appreciably positive. There is a potential trap here when using an integration routine with automatic step-sizes like Gauss-Kronold: if the bounds of integration extend well past the peak of this curve, it is possible for the algorithm to step over it entirely and return a value of 0, when the actual integral should be ~1.\nOne way of dealing with this issue is to take advantage of the symmetry of the J function and use the following rule:16\n16 Lassey, “On the Computation of Certain Integrals Containing the Modified Bessel Function \\(I_0(\\xi)\\),” 631.\nfor ξ≤τ, calculate \\(J \\left( \\xi, \\tau \\right)\\) by direct numerical integration\nfor ξ&gt;τ, calculate \\(J \\left( \\xi, \\tau \\right)\\) by using the relation \\[ J\\left( \\xi, \\tau \\right) = 1 + \\exp\\left( - \\left( \\sqrt{\\tau} - \\sqrt{\\xi} \\right)^2 \\right) I_{x,0} \\left( 2 \\sqrt{\\tau \\xi} \\right) - J\\left( \\tau, \\xi \\right) \\] where \\(J\\left( \\tau, \\xi \\right)\\) is then numerically integrated\n\nThis ensures that the numerical integration is always being taken to the left of the peak of the integrand (where ξ = τ) and thus avoids the stepping over problem.\n\nusing QuadGK: quadgk\n\nfunction J_quad(x,y)\n    if x ≈ 0\n        return 1.0, 0.0\n    elseif y ≈ 0\n        return exp(-x), 0.0\n    else\n        integrand(λ) = exp(-(√(y)-√(x*λ))^2)*besseli0x(√(4y*x*λ))\n        ∫, e = quadgk(integrand,0,1)\n        J = 1.0 - x*∫\n        return J, e\n    end\nend\n\nfunction c_quad(t, model::AnzeliusSolution)\n    # unpack some things, calculate model parameters\n    cₛ = model.pb.q₀/model.pb.K\n    c₀ = model.pb.c₀\n    τ = model.τ₁*t - model.τ₂\n    ξ = model.ξ\n\n    # use Gause-Kronold to integrate\n    # modified integral for τ &lt; ξ from Lassey p. 631\n    if τ &lt; 0 || ξ &lt; 0\n    # outside the domain of the problem\n        J = 0.0\n    elseif ξ ≤ τ\n        J, e = J_quad(ξ,τ)\n    else # τ &lt; ξ\n        J, e = J_quad(τ,ξ)\n        J = 1 + exp(-(√(τ)-√(ξ))^2)*besseli0x(√(4τ*ξ)) - J\n    end\n    \n    # return back the concentration\n    c = cₛ + (c₀ - cₛ)*J\n    return c\nend\n\n\n\nSeries Representations of the Anzelius J function\nA brief review of the literature around the Anzelius J function will reveal a multitude of series representations. For example, Goldstein17 gives the following\n17 Goldstein, “On the Mathematics of Exchange Processes in Fixed Columns i. Mathematical Solutions and Asymptotic Expansions,” 159.\\[ J(x,y) = 1 - \\exp\\left( - \\left( \\sqrt{x} - \\sqrt{y} \\right)^2 \\right) \\sum_{n=1}^{\\infty} \\left( x \\over y \\right)^{n \\over 2} \\exp \\left( -2 \\sqrt{xy} \\right) \\mathrm{I}_{n} \\left( 2 \\sqrt{xy} \\right) \\]\nNaïvely implementing this, without the use of any techniques like Richardson acceleration, ends up requiring a large number of iterations to approach the performance of direct integration by Gauss-Kronold. Since each iteration involves calculating exponentially scaled Bessel functions of higher and higher order, this doesn’t obviously lead to any improvement over direct numerical integration.\n\n# Naively, just adding them up\nusing Bessels:besselix\n\nfunction J_series(x,y,N)\n    if x ≈ 0\n        return 1.0\n    elseif y ≈ 0\n        return exp(-x)\n    else\n        α = 2√(x*y)\n        η = √(x/y)\n        partial_sum = 0.0\n        for k in 1:N\n            partial_sum += besselix(k,α)*η^k\n        end\n        J = 1 - exp(-(√(x)-√(y))^2)*partial_sum\n        return J\n    end\nend\n\nN = 900\nu = J_series(ξ,τ,N)\n\n@show u; @show N;\n\nu = 0.5016355470840961\nN = 900\n\n\nAn alternative approach from Bac̆lić et al.18 removes the need to calculate higher order Bessel functions and, though it also requires a large number of iterations, each iteration is a simpler calculation and thus the algorithm could be faster overall. If you were going to roll out the J function in production code it would be worthwhile bench marking this against numerical integration with QuadGK.jl.\n18 Bac̆lić, Gvozdenac, and Gragutinović, “Easy Way to Calculate the Anzelius-Schumann j Function,” 114.\nfunction bac̆lić_sub_A(α₋₁,β₋₁,α₀,β₀,d₁,z,ε,max_iter)\n    αₙ₋₂, βₙ₋₂ = α₋₁, β₋₁\n    αₙ₋₁, βₙ₋₁ = α₀, β₀\n    αₙ, βₙ = 0.0, 0.0\n    dₙ = d₁\n    for n in 1:max_iter\n        αₙ = dₙ + (n/z)*αₙ₋₁ + αₙ₋₂\n        βₙ = 1 + (n/z)*βₙ₋₁ + βₙ₋₂\n\n        if βₙ &gt; 1/ε\n            return αₙ, βₙ, n\n        else\n            dₙ = dₙ*d₁\n            αₙ₋₂, βₙ₋₂ = αₙ₋₁, βₙ₋₁\n            αₙ₋₁, βₙ₋₁ = αₙ, βₙ\n        end\n    end\n    return αₙ, βₙ, max_iter\nend\n\n\nfunction J_bac̆lić(x,y,ε=1e-9,max_iter=10^6)\n    if x ≈ 0\n        return 1.0\n    elseif y ≈ 0\n        return exp(-x)\n    else\n        z = √(x*y)\n        α₋₁ = 0.0\n        β₋₁ = 0.0\n        β₀ = 0.5\n        if y &lt; x\n            α₀ = 1.0\n            d₁ = √(y/x)\n            αₙ, βₙ, N = bac̆lić_sub_A(α₋₁,β₋₁,α₀,β₀,d₁,z,ε,max_iter)\n            J = (αₙ/(2*βₙ))*exp(-(√(y)-√(x))^2)\n        else\n            α₀ = 0.0\n            d₁ = √(x/y)\n            αₙ, βₙ, N = bac̆lić_sub_A(α₋₁,β₋₁,α₀,β₀,d₁,z,ε,max_iter)\n            J = 1.0 - (αₙ/(2*βₙ))*exp(-(√(y)-√(x))^2)\n        end\n        return J, ε, N\n    end\nend\n\n\nu, e, N = J_bac̆lić(τ,ξ)\n\n@show u; @show e; @show N;\n\nu = 0.5016355471003766\ne = 1.0e-9\nN = 698\n\n\n\nfunction c_bac̆lić(t, model::AnzeliusSolution)\n    # unpack some things, calculate model parameters\n    cₛ = model.pb.q₀/model.pb.K\n    c₀ = model.pb.c₀\n    τ = model.τ₁*t - model.τ₂\n    ξ = model.ξ\n    \n    if τ &lt; 0 || ξ &lt; 0\n        u = 0.0\n    else\n        u, err, N = J_bac̆lić(ξ,τ)\n    end\n    \n    c = cₛ + (c₀ - cₛ)*u\n    return c\nend\n\n\n\nApproximations to the Anzelius J Function\nThomas19 provides an asymptotic expansion of J which forms the basis for several approximations to the J function\n19 Thomas, “CHROMATOGRAPHY” page 171. Note: Thomas gives this in terms of φ where \\(J(x,y) = 1 - \\exp \\left( -(x+y) \\right)\\phi(x,y)\\)\\[ J(x,y) \\approx 1 - \\frac{1}{2}\\mathrm{erfc} \\left( \\sqrt{y} - \\sqrt{x} \\right) + \\exp \\left( -(x+y) \\right) { \\sqrt[4]{x} \\over { \\sqrt[4]{y} + \\sqrt[4]{x} } } I_0 \\left( 2\\sqrt{xy} \\right) + \\ldots\\]\nTaking the first terms of the asymptotic expansion and the limit \\(I_{x,0}(z) \\to \\frac{1}{\\sqrt{2\\pi z} }\\) as z → ∞20\n20 NIST DLMF 10.30.4\\[ J(x,y) \\approx \\frac{1}{2}\\mathrm{erfc} \\left( \\sqrt{x} - \\sqrt{y} \\right) + { \\exp \\left( - \\left( \\sqrt{x} - \\sqrt{y} \\right)^2 \\right) \\over { 2\\sqrt{\\pi} \\left( \\sqrt{y} + \\sqrt[4]{xy} \\right) } }\\]\n\nusing SpecialFunctions: erf, erfc\n\n\nfunction J_approx(x,y)\n    if x ≈ 0\n        return 1.0\n    elseif y ≈ 0\n        return exp(-x)\n    else\n        return 0.5*(erfc(√(x)-√(y)) + exp(-(√(x)-√(y))^2)/(√(π)*(√(y)+(x*y)^0.25)))\n    end\nend\n\nu = J_approx(ξ,τ)\n\n@show u;\n\nu = 0.5016355333390161\n\n\n\nfunction c_approx(t, model::AnzeliusSolution)\n    # unpack some things, calculate model parameters\n    cₛ = model.pb.q₀/model.pb.K\n    c₀ = model.pb.c₀\n    τ = model.τ₁*t - model.τ₂\n    ξ = model.ξ\n\n    # approximate integral\n    if τ &lt; 0 || ξ &lt; 0\n        u = 0.0\n    else\n        u = J_approx(ξ,τ)\n    end\n        \n    # return back the concentration\n    c = cₛ + (c₀ - cₛ)*u\n    return c\nend\n\nRice21 goes even further and suggests that, for \\(\\sqrt{xy} \\gt 60\\)\n21 Rice, “Letters to the Editor,” 334.\\[ J(x,y) \\approx \\frac{1}{2}\\mathrm{erfc} \\left( \\sqrt{x} - \\sqrt{y} \\right) \\]\n\nfunction J_rice(x,y)\n    if x ≈ 0\n        return 1.0\n    elseif y ≈ 0\n        return exp(-x)\n    else\n        return 0.5*erfc(√(x)-√(y))\n    end\nend\n\nu = J_rice(ξ,τ)\n\n@show u;\n\nu = 0.499999999999992\n\n\n\nfunction c_rice(t, model::AnzeliusSolution)\n    # unpack some things, calculate model parameters\n    cₛ = model.pb.q₀/model.pb.K\n    c₀ = model.pb.c₀\n    τ = model.τ₁*t - model.τ₂\n    ξ = model.ξ\n\n    # approximate integral\n    if τ &lt; 0 || ξ &lt; 0\n        u = 0.0\n    else\n        u = 0.5*erfc(√(ξ)-√(τ))\n    end\n        \n    # return back the concentration\n    c = cₛ + (c₀ - cₛ)*u\n    return c\nend\n\n\n\nReviewing Overall Performance\nI don’t see any great reason to use anything other than direct numerical integration, so that is the default method I am going to set going forward.\n\nc(t,m::AnzeliusSolution) = c_quad(t,m)\n\nThat said, for this particular case, the mass transfer across the thin film is so rapid that all of the approximations are close enough as to be indistinguishable. Indistinguishable to the naked eye when staring at a plot but, more importantly, experimentally indistinguishable.\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: The Anzelius solution and its approximations. For this problem the approximations are essentially exact.\n\n\n\n\nI wouldn’t take this to mean that the simple, single term, \\(\\mathrm{erfc}(\\ldots)\\) approximation will work for modelling an actual espresso shot, though it is certainly suggestive. This approach, neglecting the solid phase diffusion entirely and neglecting axial diffusion, has lead to a very sharp moving front that is physically unrealistic. What this model is telling us is that we should expect an espresso shot to start as fully and overly extracted coffee and, after some time, transition into essentially tap water in a fraction of a second. That is not my experience, qualitatively. What this shows is that we need to increase the complexity of the model."
  },
  {
    "objectID": "posts/engineering_a_cup_of_coffee_part-2/index.html#rosens-integral-solution",
    "href": "posts/engineering_a_cup_of_coffee_part-2/index.html#rosens-integral-solution",
    "title": "Engineering a Cup of Coffee Part Two: Espresso",
    "section": "Rosen’s Integral Solution",
    "text": "Rosen’s Integral Solution\nRosen22 solved the problem for the case where solid diffusion is included, but axial diffusion is still neglected, in a similar manner as above with Laplace transforms. In this case actually solving the pde is more tedious and what follows is just a loose sketch. Starting with the pde\n22 Rosen, “Kinetics of a Fixed Bed System for Solid Diffusion into Spherical Particles,” 387–94.\\[ { {\\partial c} \\over {\\partial t} } + v { {\\partial c} \\over {\\partial z} } = \\left({ 1 - \\varepsilon } \\over \\varepsilon \\right) a_s J_s \\]\n\\[ { {\\partial q} \\over {\\partial t} } = \\mathscr{D}_s \\left( { {\\partial^2 q} \\over {\\partial r^2} }  + {2 \\over r} { {\\partial q} \\over {\\partial r} }  \\right) \\]\nwere\n\\[ J_s = h \\left( c_{s} -  c \\right) \\]\nRosen introduced \\[ { {\\partial \\bar{q} } \\over {\\partial t} } = - a_s J_s \\]\nwhere\n\\[ \\bar{q} \\left( z,t \\right) = \\frac{3}{b^3} \\int_0^b q \\left( r, \\xi, \\tau \\right) r^2 dr \\]\nis the volumetric average concentration in a particle. This follows directly from a mass balance.\nWe can put the liquid phase equation in dimensionless form by introducing a dimensionless time, \\(\\tau = { { \\mathscr{D} a_s} \\over b} \\left( t - \\frac{z}{v} \\right)\\), and a dimensionless z-coordinate, \\(\\xi = { { K D a_s} \\over {b m v} } z\\), where \\(m = { \\varepsilon \\over \\left( 1 - \\varepsilon \\right) }\\) and, with the same dimensionless concentrations u and y as defined above for the Anzelius solution, we have\n\\[ { { \\partial u} \\over {\\partial \\xi} } = - { { \\partial \\bar{y} } \\over {\\partial \\tau} } = \\frac{1}{\\nu} \\left( y_s - u \\right)\\]\nwhere \\(\\nu = \\frac{\\mathscr{D} K}{b h}\\) and ys is the dimensionless solid phase concentration at the surface of a solid particle, i.e. at r = b.\nIf we further introduce a dimensionless particle radius \\(\\vartheta = \\frac{r}{b}\\) we can rewrite the solid phase diffusion equation in dimensionless form\n\\[ { {\\partial y} \\over {\\partial \\tau} } = \\frac{1}{3} \\left( { {\\partial^2 y} \\over {\\partial \\vartheta^2} }  + {2 \\over \\vartheta} { {\\partial y} \\over {\\partial \\vartheta} }  \\right) \\]\nwhere 1/3 is 1/(asb). The solution to the solid phase diffusion problem is available in Carslaw and Jaeger23 and, with initial condition y=0 when τ=0, gives\n23 Carslaw and Jaeger, Conduction of Heat in Solids, 233.\\[ y \\left( \\vartheta, \\xi, \\tau \\right) = \\frac{2}{3} \\sum_{n=1}^{\\infty} \\left(-1 \\right)^{n+1} n\\pi { {\\sin \\left( n \\pi \\vartheta \\right) } \\over \\vartheta} \\int_0^{\\tau} y_s \\left( \\xi, \\lambda \\right) \\exp \\left( -\\frac{n^2 \\pi^2}{3} \\left( \\tau - \\lambda \\right) \\right) d\\lambda\\]\nThis can be integrated over \\(\\vartheta\\) to get the volume average (dimensionless) concentration\n\\[ \\bar{y} = 3 \\int_0^1 y \\vartheta^2 d \\vartheta \\]\n\\[ \\bar{y} = 2 \\sum_{n=1}^{\\infty} \\int_0^{\\tau} y_s \\exp \\left( -\\frac{n^2 \\pi^2}{3}  \\left( \\tau - \\lambda \\right) \\right) d\\lambda \\]\nsince\n\\[ \\int_0^1 n\\pi \\sin \\left( n \\pi \\vartheta \\right) \\vartheta d\\vartheta = \\left(-1 \\right)^{n+1} \\]\nTaking the derivative with respect to τ gives (by integration by parts)\n\\[ { {\\partial \\bar{y} } \\over {\\partial \\tau} } = 2 \\sum_{n=1}^{\\infty} \\int_0^{\\tau} { { \\partial y_s } \\over {\\partial \\lambda} } \\exp \\left( -\\frac{n^2 \\pi^2}{3} \\left( \\tau - \\lambda \\right) \\right) d\\lambda \\]\nAt this point we can eliminate y and ys and have an expression entirely in terms of u. First we use the expression for the liquid phase concentration to obtain an expression for ys\n\\[ { { \\partial u} \\over {\\partial \\xi} } = \\frac{1}{\\nu} \\left( y_s - u \\right)\\]\n\\[ y_s = u + \\nu { { \\partial u} \\over {\\partial \\xi} } \\]\nThus\n\\[ { {\\partial \\bar{y} } \\over {\\partial \\tau} } = 2 \\sum_{n=1}^{\\infty} \\int_0^{\\tau} \\left[ { {\\partial u} \\over {\\partial \\lambda} } + \\nu { { \\partial^2 u} \\over {\\partial \\lambda \\partial \\xi} } \\right] \\exp \\left( -\\frac{n^2 \\pi^2}{3}  \\left( \\tau - \\lambda \\right) \\right) d\\lambda \\]\nand since \\({ { \\partial u} \\over {\\partial \\xi} } = - { { \\partial \\bar{y} } \\over {\\partial \\tau} }\\)\n\\[ { {\\partial u } \\over {\\partial \\xi} } = -2 \\sum_{n=1}^{\\infty} \\int_0^{\\tau} \\left[ { {\\partial u} \\over {\\partial \\lambda} } + \\nu { { \\partial^2 u} \\over {\\partial \\lambda \\partial \\xi} } \\right] \\exp \\left( -\\frac{n^2 \\pi^2}{3} \\left( \\tau - \\lambda \\right) \\right) d\\lambda \\]\nRosen solves this by taking the Laplace transform, with respect to τ, with the following relations:\n\\[ \\mathscr{L} \\left\\{ \\int_0^\\tau f(\\lambda) g(\\tau-\\lambda) d\\lambda \\right\\} = F(s) G(s) \\]\n\\[ \\mathscr{L} \\left\\{ { {\\partial u} \\over {\\partial \\tau} } + \\nu { { \\partial^2 u} \\over {\\partial \\tau \\partial \\xi} } \\right\\} =  s U + \\nu s { {d U} \\over {d\\xi} }\\]\n\\[ \\mathscr{L} \\left\\{ \\exp \\left( -\\frac{n^2 \\pi^2}{3} \\tau \\right) \\right\\} =  { 1 \\over {s + \\frac{n^2 \\pi^2}{3} } }\\]\narriving at\n\\[ { {d U} \\over {d\\xi} } = -2 \\left( s U + \\nu s { {d U} \\over {d\\xi} } \\right) \\sum_{n=1}^{\\infty} { s \\over {s + \\frac{n^2 \\pi^2}{3} } } \\]\nLetting\n\\[ Y_D(s) = 2 \\sum_{n=1}^{\\infty} { s \\over {s + \\frac{n^2 \\pi^2}{3} } } \\]\nthen\n\\[ { {d U} \\over {d\\xi} } = - { Y_D \\over  { 1 + \\nu Y_D } } U \\]\nand, solving this ode with initial condition u=1, U=1/s, gives\n\\[ U = \\frac{1}{s} \\exp \\left( - { Y_D \\over  { 1 + \\nu Y_D } } \\xi \\right) \\]\nThe final solution follows from taking the inverse Laplace transform, by way of the contour integral\n\\[ u \\left( \\xi, \\tau \\right) = \\frac{1}{2\\pi i} \\int_{\\alpha - i\\infty}^{\\alpha + i\\infty} \\frac{1}{s} \\exp \\left( s \\tau - { Y_D \\over  { 1 + \\nu Y_D } } \\xi \\right) ds \\]\nA major component of the integration involves first defining YD in terms of trigonometric functions. The details are tedious, but the main result is\n\\[ Y_T \\left( i \\beta \\right) = { Y_D \\over  { 1 + \\nu Y_D } } = H_1 \\left( \\lambda, \\nu \\right) + i H_2 \\left( \\lambda, \\nu \\right) \\]\nwith \\(\\lambda = \\sqrt{ \\frac{3}{2} \\beta}\\) and\n\\[ H_1 \\left( \\lambda, \\nu \\right) = { { H_{D1} + \\nu \\left( H_{D1}^2 + H_{D2}^2 \\right) } \\over { \\left( 1 + \\nu H_{D1} \\right)^2 + \\left( \\nu H_{D2} \\right)^2 } }\\]\n\\[ H_2 \\left( \\lambda, \\nu \\right) = { H_{D2} \\over { \\left( 1 + \\nu H_{D1} \\right)^2 + \\left( \\nu H_{D2} \\right)^2 } }\\]\nand\n\\[ H_{D1} = \\lambda { {\\sinh 2\\lambda + \\sin 2\\lambda} \\over { \\cosh 2\\lambda - \\cos 2\\lambda } } - 1\\]\n\\[ H_{D2} = \\lambda { {\\sinh 2\\lambda - \\sin 2\\lambda} \\over { \\cosh 2\\lambda - \\cos 2\\lambda } } \\]\nWhich allows Rosen to write the integral in terms of these harmonic functions24\n24 For details of the integration see Rosen, “Kinetics of a Fixed Bed System for Solid Diffusion into Spherical Particles” pages 390-391\\[ u \\left( \\xi, \\tau \\right) = \\frac{1}{2} + \\frac{2}{\\pi} \\int_0^\\infty { { \\exp \\left( -\\xi H_1 \\left( \\lambda, \\nu \\right) \\right) \\sin \\left( \\frac{2}{3} \\tau \\lambda^2 - \\xi H_2 \\left( \\lambda, \\nu \\right) \\right) } \\over \\lambda } d\\lambda \\]\n\nDefining the Rosen Solution\nAt this point we can calculate the dimensionless space and time for a point at the exit of the espresso bed, and say at a similar point in (dimensionless) time, and proceed with calculating the integral to find the concentration.\n\nm = ε/(1-ε)\naᵥ = 3/b\n\nξ = (K*𝒟ₛ*aᵥ)/(b*m*v)*z\n\nτ = (𝒟ₛ*aᵥ/b)*(t-z/v)\nν = (𝒟ₛ*K)/(b*h)\n\n@show ξ; @show τ; @show ν;\n\nξ = 144.6719346388569\nτ = 144.6719346388569\nν = 0.019452389057107278\n\n\nUnlike with the Anzelius case, I am not going to define a struct for the Rosen solution yet, first I am going to work through some details on how to perform the integral.\n\n\nThe Harmonic Functions\nThe integral extends to infinity and so the performance of the harmonic functions at very large λ is important. The hyperbolic trig functions will blow up to infinity and, in the naïve implementation, lead to NaN errors as the numerator and denominator overflow. Rosen provides limiting behaviour, and a pre-calculated table of values, which can be used with the integrand switching from the default definition of the harmonic functions to the limiting behaviour after some λ threshold. An alternative, which I employ below, is to rewrite the hyperbolic trig functions in terms of exponentials, cancelling a exp(+4λ) from the numerator and denominator, to generate a form that handles large values of λ more gracefully.\n\\[ { {\\sinh 2\\lambda + \\sin 2\\lambda } \\over {\\cosh 2\\lambda - \\cos 2\\lambda} } = { { 1 - \\exp(-2\\lambda) \\left( \\exp(-2\\lambda) + 2 \\sin 2\\lambda \\right) } \\over {1 + \\exp(-2\\lambda) \\left( \\exp(-2\\lambda) - 2 \\cos 2\\lambda \\right) } }\\]\n\nfunction HD1(λ)\n    if λ ≤ eps(0.0)\n        return 0.0\n    else\n        # λ*((sinh(2λ) + sin(2λ))/(cosh(2λ) - cos(2λ))) - 1\n        return λ*( (1 - exp(-4λ) + 2*exp(-2λ)*sin(2λ)) / (1 + exp(-4λ) - 2*exp(-2λ)*cos(2λ))) -1 \n    end\nend\n\nfunction HD2(λ)\n    if λ ≤ eps(0.0)\n        return 0.0\n    else\n        # λ*(sinh(2λ) - sin(2λ))/(cosh(2λ) - cos(2λ))\n        return λ*( (1 - exp(-4λ) - 2*exp(-2λ)*sin(2λ)) / (1 + exp(-4λ) - 2*exp(-2λ)*cos(2λ))) \n    end\nend\n\n\n\nIntegration by Gauss-Kronold\nThe integrand can be divided into a decay component, f, that is independent of τ, and an oscillatory component, K, that is a function of τ.\n\\[ f \\left( \\lambda; \\xi, \\nu \\right) = { { \\exp \\left( -\\xi H_1 \\left( \\lambda, \\nu \\right) \\right) } \\over \\lambda  }\\]\n\\[ \\mathscr{K} \\left( \\lambda, \\tau; \\xi, \\nu \\right) = \\sin \\left( \\frac{2}{3} \\tau \\lambda^2 - \\xi H_2 \\left( \\lambda, \\nu \\right) \\right) \\]\nThis clean division also presents an opportunity to pre-calculate the integral to an extent. With a predefined set of points { λi } then f can be entirely pre-calculated. Using prosthaphaeresis you could go further and pre-calculate parts of \\(\\mathscr{K}\\), for some incremental improvements, though I leave that as an exercise for a more motivated individual.\nIt is worth looking at the case where τ gets large as this becomes a highly oscillating integral and can be tricky to evaluate – requiring a very large number of steps for conventional numerical integration techniques like Gauss-Kronold. In this example that starts to happen near the end of the extraction, but if the bed were, say, twice as deep then much of the extraction curve would be in this regime.\n\nfunction fᵣ(λ; ξ, ν)\n    hd1, hd2 = HD1(λ), HD2(λ)\n    H1 = (hd1 + ν*(hd1^2 + hd2^2))/((1 + ν*hd1)^2 + (ν*hd2)^2)\n    return exp(-ξ*H1)/λ\nend\n\nfunction 𝒦ᵣ(λ, τ; ξ, ν)\n    hd1, hd2 = HD1(λ), HD2(λ)\n    H2 = hd2/((1 + ν*hd1)^2 + (ν*hd2)^2)\n    return sin((2/3)*τ*λ^2 - ξ*H2)\nend\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: The integrand of the Rosen solution, for moderate values of τ this is a highly oscillating integral.\n\n\n\n\nBy introducing the change of variables25 β = λ2, the integrand is “compressed” along β and we can take advantage of the exponential decay to truncate the integration.\n25 Rosen, “General Numerical Solution for Solid Diffusion in Fixed Beds,” 1590–94.\nfunction fᵣ₂(β; ξ, ν)\n    λ = √(β)\n    hd1, hd2 = HD1(λ), HD2(λ)\n    H1 = (hd1 + ν*(hd1^2 + hd2^2))/((1 + ν*hd1)^2 + (ν*hd2)^2)\n    return exp(-ξ*H1)/β\nend\n\nfunction 𝒦ᵣ₂(β, τ; ξ, ν)\n    λ = √(β)\n    hd1, hd2 = HD1(λ), HD2(λ)\n    H2 = hd2/((1 + ν*hd1)^2 + (ν*hd2)^2)\n    return sin((2/3)*τ*β - ξ*H2)\nend\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: The integrand of the Rosen solution, after the change of variables. The curve decays much more rapidly.\n\n\n\n\nQuadGK.jl can compute the improper integral directly, by making the substitution λ = t/(1-t).\n\nusing QuadGK: quadgk_count\n\nI₁, err₁, N₁ = quadgk_count(λ -&gt; fᵣ(λ; ξ=ξ, ν=ν)*𝒦ᵣ(λ, τ; ξ=ξ, ν=ν), 0, Inf)\n\n@show I₁; @show err₁; @show N₁;\n\nI₁ = 0.011703238397164204\nerr₁ = 1.0292584595556625e-10\nN₁ = 195\n\n\nBy making the substitution β = λ2 and truncating the integral to the range [0,2] we can achieve similar precision, with almost half as many steps.\n\nI_2, err_2, N_2 = quadgk_count( β -&gt; fᵣ₂(β; ξ=ξ, ν=ν)*𝒦ᵣ₂(β, τ; ξ=ξ, ν=ν), 0, 2)\n\n@show I_2/2; @show err_2; @show N_2;\n\nI_2 / 2 = 0.01170323839564812\nerr_2 = 2.183245425172356e-10\nN_2 = 105\n\n\n\n\nIntegration by Levin Colocation\nI have mentioned a few times that highly oscillating integrals can be tricky to evaluate. Gaussian quadrature will, in general, work but it will require a large number of steps. An alternative is to use Levin colocation, an example implementation is given below using ApproxFun.jl.\nGiven the integral\n\\[ \\int_a^b \\mathbf{f}(x) \\cdot \\mathbf{K}(x) dx \\]\nif we suppose there is a function \\(\\mathbf{F}\\) such that\n\\[ \\frac{d}{dx} \\mathbf{F}(x) \\cdot \\mathbf{K}(x) = \\mathbf{f}(x) \\cdot \\mathbf{K}(x) \\]\nThen we can eliminate the integral, using the fundamental theorem of calculus\n\\[ \\int_a^b \\mathbf{f}(x) \\cdot \\mathbf{K}(x) dx = \\int_a^b \\frac{d}{dx} \\mathbf{F}(x) \\cdot \\mathbf{K}(x) dx = \\mathbf{F}(b) \\cdot \\mathbf{K}(b) - \\mathbf{F}(a) \\cdot \\mathbf{K}(a)\\]\nThe problem is then one of finding the function \\(\\mathbf{F}\\).\nIf we choose \\(\\mathbf{K}(x)\\) such that \\(\\frac{d}{dx} \\mathbf{K}(x) = \\mathbf{A} \\mathbf{K}(x)\\), then\n\\[ \\frac{d}{dx} \\left( \\mathbf{F}(x) \\cdot \\mathbf{K}(x) \\right) =  \\mathbf{F}^{\\prime}(x) \\cdot \\mathbf{K}(x) + \\mathbf{F}(x) \\cdot \\mathbf{A} \\mathbf{K}(x) \\]\n\\[ \\mathbf{F}^{\\prime}(x) \\cdot \\mathbf{K}(x) + \\mathbf{F}(x) \\cdot \\mathbf{A} \\mathbf{K}(x) = \\mathbf{f}(x) \\cdot \\mathbf{K}(x) \\]\nEliminating K(x) gives\n\\[ \\mathbf{F}^{\\prime}(x) + \\mathbf{A}^{T} \\mathbf{F}(x) = \\mathbf{f}(x) \\]\nSolving this ode then gives the final solution.\nIn this case I set K(x) to\n\\[ \\mathbf{K}(x) = \\begin{pmatrix} \\sin(h(x) \\\\ \\cos(h(x)) \\end{pmatrix} \\]\nwhere h(x) = (2/3)τx2 - ξH2(λ), and f(x) is\n\\[ \\mathbf{f}(x) = \\begin{pmatrix} f(x) \\\\ 0 \\end{pmatrix} \\]\nand the ode is solved for F in terms of Chebyshev polynomials.\n\nusing ApproxFun:Interval, Fun, Derivative, Evaluation, \\, I\nusing LinearAlgebra: ⋅\n\nfunction levin(ξ, τ, ν, a, b)\n    d = Interval(a,b)\n    λ = Fun(d)\n    D = Derivative(d)\n    E = Evaluation(a)\n\n    HD1 = λ*(sinh(2λ) + sin(2λ))/(cosh(2λ) - cos(2λ)) - 1\n    HD2 = λ*(sinh(2λ) - sin(2λ))/(cosh(2λ) - cos(2λ))\n    \n    H1 = (HD1 + ν*(HD1^2 + HD2^2))/((1 + ν*HD1)^2 + (ν*HD2)^2)\n    H2 = HD2/((1 + ν*HD1)^2 + (ν*HD2)^2)\n\n    h = (2/3)*τ*λ^2 - ξ*H2\n    h′ = D*h\n    w⃗ = [ sin(h); cos(h) ]\n    \n    f = exp(-ξ*H1)/λ\n    f⃗ = [ 0; 0; f; 0 ]\n    \n    L = [ E    0;\n          0    E;\n          D    -h′*I;\n          h′*I  D ]\n    \n    F = L\\f⃗\n    \n    return F(b)⋅w⃗(b) - F(a)⋅w⃗(a)\nend\n\n\nlevin(ξ, τ, ν, 0.0001, 2)\n\n0.011703248012960965\n\n\nThis works well enough, though there is a complication in that there is a singularity at x=0, it is also rather slow.\nIf the system of interest was consistently at large values of τ, where it is a highly oscillating integral throughout the main part of the problem domain, it would be worth looking at techniques to eliminate the singularity and speed it up. I include it here mostly for completeness. It is by delightful coincidence alone that I can get by solving this particular problem using more conventional numerical integration techniques.\n\n\nPre-calculating the spatial component\nAs I mentioned above, by splitting the integral into a function f that depends only on space and a function K that is a function of time and space, we can pre-calculate all of the space dependent components and write the integral as a weighted sum.\n\\[ \\int_a^b f \\left(x; p\\right) \\mathcal{K} \\left(x, t; p\\right) dx \\approx \\sum_i^{N} w_i \\mathcal{K}\\left(x_i, t\\right)\\]\nThe obvious way to do this is to use QuadGK.jl to take the weight function f and generate points that way. For example:\npts, wts = gauss( x -&gt; fᵣ₂(x; ξ=ξ, ν=ν), 20, 0, 2);\nI could not get this to work reliably, it would routinely run aground on DomainErrors close to the singularity at x=0. When I did get it to work it took a very long time to generate points, like leave my desk and go make coffee and maybe it will be done when I get back long time. I think if you really wanted to invest the time, and evaluating this integral was going to be in production code, it would be worth investigating a better quadrature rule since, when it does work, it allows you to use significantly fewer points in each integration.\nThe alternative, which works well enough for my purposes, is to use the gauss function to generate a set of points and weights in the truncated range \\(\\beta \\in [0,2]\\) and then pre-calculate the values of f over those points. The final integral is then the weighted sum. This involves calculating far more points for any given integral, but it is much faster than either Levin colocation or trying to have QuadGK generate the weights.\n\nusing QuadGK: gauss\n\npts, wts = gauss(N₁, 0, 2);\n\nwts = wts .* fᵣ₂.(pts; ξ=ξ, ν=ν);\n\nI_gauss = sum( wts .* 𝒦ᵣ₂.(pts, τ; ξ=ξ, ν=ν) )/2\n\n0.011703238395545075\n\n\nThis can be packaged neatly into an IntegralTransform struct that, when constructed, generates the set of points and appropriate weights such that it only the kernel function actually needs to be evaluated for any given time.\n\nstruct IntegralTransform{T}\n    a::T\n    b::T\n    params::NamedTuple\n    numpts::Integer\n    pts::Vector{T}\n    wts::Vector{T}\n    kern::Function\nend \n\nfunction IntegralTransform(params, fun, kern; a=0.0, b=2.0, numpts=350)\n    pts, wts = gauss(numpts, a, b)\n    wts = wts .* fun.(pts; params...)\n    return IntegralTransform(a, b, params, numpts, pts, wts, kern)\nend\n\nfunction integrate(t, it::IntegralTransform)\n    return sum( it.wts .* it.kern.(it.pts, t; it.params...) )\nend\n\nOne could go further here: pre-calculating H1 and H2 as they only depend on ν and λ, splitting K into parts by prosthaphaeresis26 and pre-calculating the parts that only depend on ξ and λ. The current performance is more than good enough for me, but I think it worth highlighting that there are many opportunities for improvement.\n26 I love this word.\n\nPackaging a final approach\nAt this point I am finally ready to circle back and create my RosenSolution struct, one that includes the pre-calculated IntegralTransform for the particular location in the bed.\n\nstruct RosenSolution{Q1,Q2,T}\n    τ₁::Q1\n    τ₂::Q2\n    pb::PackedBed\n    it::IntegralTransform{T}\nend\n\nfunction RosenSolution(z, pb::PackedBed; fun=fᵣ₂, kern=𝒦ᵣ₂, a=0.0, b=2.0, numpts=200)\n    m = pb.ε/(1-pb.ε)\n    aᵥ = 3/pb.b\n    ξ = (pb.K*pb.𝒟ₛ*aᵥ*z)/(m*pb.v*pb.b)\n    ν = (pb.𝒟ₛ*pb.K)/(pb.b*pb.h)\n    τ₁ = pb.𝒟ₛ*aᵥ/pb.b\n    τ₂ = τ₁*(z/pb.v)\n\n    p = (ξ=ξ, ν=ν)\n    it = IntegralTransform(p, fun, kern; a=a, b=b, numpts=numpts)\n    \n    return RosenSolution(τ₁, τ₂, pb, it)\nend\n\nThe concentration can then be obtained by calling the integrate function with the integral transform.\n\nfunction c(t, model::RosenSolution)\n    # unpack some things\n    cₛ = model.pb.q₀/model.pb.K\n    c₀ = model.pb.c₀\n\n    # compute the integral\n    τ = model.τ₁*t - model.τ₂\n    I = integrate(τ, model.it)\n\n    # return back the concentration\n    u = 0.5 + I/π\n    c = cₛ + (c₀ - cₛ)*u\n    return c\nend\n\n\nrosen = RosenSolution(z, pb; a=0.0, b=2.0, numpts=200);\n\n\n\nApproximations to the Rosen Integral\nRosen27 provides an asymptotic approximation for cases where ξ is large\n27 Rosen, “General Numerical Solution for Solid Diffusion in Fixed Beds,” 1591.\\[ u = \\frac{1}{2} \\left[ 1 + \\mathrm{erf} \\left( { { \\frac{\\tau}{\\xi} - 1} \\over { 2 \\sqrt{ {1 + 5\\nu} \\over {5 \\xi} } } } \\right) \\right] \\]\nWhich is decidedly simpler to calculate.\n\nfunction c_approx(t, model::RosenSolution)\n    # unpack some things\n    cₛ = model.pb.q₀/model.pb.K\n    c₀ = model.pb.c₀\n\n    # compute the integral\n    τ = model.τ₁*t - model.τ₂\n    u = 0.5*(1 + erf( ( (τ/ξ) - 1 ) / ( 2*√((1+5ν)/(5ξ)) ) ) )\n\n    # return back the concentration\n    c = cₛ + (c₀ - cₛ)*u\n    return c\nend\n\n\n\nApproximating the Rosen integral with the Anzelius J Function\nI briefly mentioned, above, an effective mass transfer coefficient can be derived for the Anzelius solution, one that accounts for the solid phase diffusion. This can be calculated rather simply from the linear resistance model28\n28 LeVan and Carta, “Adsorption and Ion Exchange,” 16–24.\\[ \\frac{1}{h_{eff} } = \\frac{1}{(1-\\varepsilon) h} + \\frac{b}{5 K \\mathscr{D}_s} \\]\nAdapting the AnzeliusSolution to use a generic function to calculate the effective mass transfer coefficient allows us to reuse everything from the Anzelius case.\n\nfunction AnzeliusSolution(z, h_fun, pb::PackedBed)\n    m  = pb.ε/(1-pb.ε) \n    aᵥ = 3/pb.b\n    h  = h_fun(pb)\n    ξ  = (h*aᵥ*z)/(m*pb.v)\n    τ₁ = (h*aᵥ/pb.K)\n    τ₂ = τ₁*(z/pb.v)\n\n    return AnzeliusSolution(ξ, τ₁, τ₂, pb)\nend\n\nh_eff(pb) = 1/( 1/((1-pb.ε)*pb.h) + pb.b/(5*pb.K*pb.𝒟ₛ) )\n\n\n\nReviewing Overall Performance\nThe Rosen solution is a significant departure from the pure Anzelius solution, i.e. neglecting solid diffusion, showing that for this problem the rate of solid phase diffusion is quite important. In this case ξ is large enough that the asymptotic approximation to Rosen’s integral is also a very good model and, with an appropriate effective mass transfer coefficient, the Anzelius solution also works well.\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: The Rosen solution, and it’s approximations, compared with the Anzelius solution. Note the approximations are essentially exact for this problem."
  },
  {
    "objectID": "posts/engineering_a_cup_of_coffee_part-2/index.html#rasmusons-integral-solution",
    "href": "posts/engineering_a_cup_of_coffee_part-2/index.html#rasmusons-integral-solution",
    "title": "Engineering a Cup of Coffee Part Two: Espresso",
    "section": "Rasmuson’s Integral Solution",
    "text": "Rasmuson’s Integral Solution\nRasmuson and Neretnieks29 provide an exact solution for the case where axial diffusion is included. This is the original pde derived at the beginning. Their solution follows essentially the same steps as Rosen, with the main difference that the ode in the Laplace domain is second order, due to the inclusion of the \\(\\frac{\\partial^2 c}{\\partial z^2}\\) term. The original paper has an detailed derivation in the appendix, if you are interested. In practice, this amounts to a relatively minor modification on what we have already put together for the Rosen solution.\n29 Rasmuson and Neretnieks, “Exact Solution of a Model for Diffusion in Particles and Longitudinal Dispersion in Packed Beds,” 686–90.First we define the decay function, f, and kernel, K, using the same harmonic functions as Rosen.\n\nfunction f_rasmuson(λ; ν, δ, R, Pe)\n    hd1, hd2 = HD1(λ), HD2(λ)\n    H1 = (hd1 + ν*(hd1^2 + hd2^2))/((1 + ν*hd1)^2 + (ν*hd2)^2)\n    H2 = hd2/((1 + ν*hd1)^2 + (ν*hd2)^2)\n    a = Pe*(0.25*Pe + δ*H1)\n    b = δ*Pe*((2/3)*λ^2/R + H2)\n    return exp(0.5*Pe - √(0.5*(√(a^2 + b^2) + a)))/λ\nend\n\nfunction 𝒦_rasmuson(λ, y; ν, δ, R, Pe)\n    hd1, hd2 = HD1(λ), HD2(λ)\n    H1 = (hd1 + ν*(hd1^2 + hd2^2))/((1 + ν*hd1)^2 + (ν*hd2)^2)\n    H2 = hd2/((1 + ν*hd1)^2 + (ν*hd2)^2)\n    a = Pe*(0.25*Pe + δ*H1)\n    b = δ*Pe*((2/3)*λ^2/R + H2)\n    return sin(y*λ^2 - √(0.5*(√(a^2 + b^2) - a)))\nend\n\nRasmuson and Neretnieks parameterize things slightly differently, and add some extra dimensionless groups due to the \\(\\mathscr{D}_L\\), but the result a similar sort of integral problem as Rosen, namely integrating a highly oscillating integral that decays rapidly.\n\nγ = 3*𝒟ₛ*K/b^2\nδ = γ*z/(m*v)\nν = γ*b/(3h)\nσ = 2*𝒟ₛ/b^2\n\nR = K/m\nPe = (z*v)/𝒟ₗ\ny = σ*t\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: The integrand of the Rasmuson-Neretnieks solution, for moderate values of y this becomes a highly oscillating integral.\n\n\n\n\nThus it can be represented using the IntegralTransform type previously created.\n\np = (ν=ν, δ=δ, R=R, Pe=Pe)\n\nit = IntegralTransform(p, f_rasmuson, 𝒦_rasmuson; a=0.0, b=2.0, numpts=200);\n\n\nintegrate(y, it)\n\n0.011984085774006418\n\n\n\nstruct RasmusonSolution{Q,T}\n    σ::Q\n    pb::PackedBed\n    it::IntegralTransform{T}\nend\n\nfunction RasmusonSolution(z, pb::PackedBed; fun=f_rasmuson, kern=𝒦_rasmuson, a=0.0, b=2.0, numpts=350)\n    m = pb.ε/(1-pb.ε)\n    γ = 3*pb.𝒟ₛ*pb.K/pb.b^2\n    δ = γ*z/(m*pb.v)\n    ν = γ*pb.b/(3*pb.h)\n    σ = 2*pb.𝒟ₛ/pb.b^2\n    \n    R = pb.K/m\n    Pe = (z*pb.v)/pb.𝒟ₗ\n\n    p = (ν=ν, δ=δ, R=R, Pe=Pe)\n    it = IntegralTransform(p, fun, kern; a=a, b=b, numpts=numpts)\n    \n    return RasmusonSolution(σ, pb, it)\nend\n\n\nrasmuson = RasmusonSolution(z,pb);\n\n\nfunction c(t, model::RasmusonSolution)\n    # unpack some things\n    cₛ = model.pb.q₀/model.pb.K\n    c₀ = model.pb.c₀\n\n    # compute the integral\n    y = model.σ*t\n    I = integrate(y, model.it)\n\n    # return back the concentration\n    u = 0.5 + 2I/π\n    c = cₛ + (c₀ - cₛ)*u\n    return c\nend\n\nGoing from Rosen’s solution to Rasmuson’s solution is a less dramatic change than from Anzelius, but it is clear that axial dispersion is an important effect in this case. I haven’t shown it, since I think it should be obvious at this point, but one could generate asymptotic relations for Rasmuson, and also find effective mass transfer coefficients, h, that would bring both the Rosen and Anzelius solutions in line with the Rasmuson solution. I leave that as an exercise for the reader (hint: it is just linear mass transfer resistances).\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: The Rasmuson-Neretnieks solution for packed bed extraction, compared with the Rosen and Anzelius cases."
  },
  {
    "objectID": "posts/engineering_a_cup_of_coffee_part-2/index.html#integrating-the-pde-by-finite-difference",
    "href": "posts/engineering_a_cup_of_coffee_part-2/index.html#integrating-the-pde-by-finite-difference",
    "title": "Engineering a Cup of Coffee Part Two: Espresso",
    "section": "Integrating the PDE by finite difference",
    "text": "Integrating the PDE by finite difference\nThe more direct approach, when faced with a pde, is to integrate it by finite differences or Method of Lines. This allows one to use whatever kinetics and initial conditions one wants. The above cases are all limited by linear extraction and the initial conditions that the bed is at equilibrium. The major downside is that, for problems like this with a rather sharp moving front, the discretization needs to be very tight or another method, like moving finite element, needs to be used. Thus making the actual run time rather slow.\nThe first step is to put the pde in dimensionless form, by introducing the following variables\n\\[ \\xi = \\frac{z}{L} \\]\n\\[ \\vartheta = \\frac{r}{b} \\]\n\\[ \\tau = \\frac{t}{t_{shot} } \\]\n\\[ u = \\frac{c}{c_{sat} }\\]\n\\[ y = \\frac{q}{q_{0} } \\]\nwe can write the pde for the liquid phase concentration as\n\\[ { {\\partial u} \\over {\\partial \\tau} } = { { t_{shot} \\mathscr{D}_L } \\over L^2 } { {\\partial^2 u} \\over {\\partial \\xi^2} } - { {t_{shot} v} \\over L }{ {\\partial u} \\over {\\partial \\xi} } - { {1 - \\varepsilon} \\over \\varepsilon } h a_s t_{shot} \\left( u - y \\right)\\]\nand the pde for the solid phase concentration as\n\\[ { {\\partial y} \\over {\\partial \\tau} } = { {t_{shot} \\mathscr{D}_s}  \\over b^2 } \\left( { {\\partial^2 y} \\over {\\partial \\vartheta^2} } + \\frac{2}{\\vartheta} { {\\partial y} \\over {\\partial \\vartheta} } \\right) \\]\nwith\n\\[ \\left. { {\\partial y} \\over {\\partial \\tau} } \\right\\vert_{\\vartheta=1} = { {h a_s t_{shot} } \\over K} \\left( u - y \\right) \\]\nThese equations can be discretized in both spatial dimensions ξ and \\(\\vartheta\\), turning them into an ode in τ.\n\n\n\n\n\n\nFigure 11: A discretized mass transfer system, the column is divided into n thin slices and each slice is further subdivided into m+1 cells.\n\n\n\nIn general the bed can be divided into n cells with each cell transferring fluid to the cell below by advection and exchanging mass with the solid phase through the thin film approximation. The solid phase would then be divided into m cells per cell of the column making the overall ode an n×(m+1) vector of cells.\n\nThe Anzelius Example Case\nAs an example of the how tight the discretization needs to be, I have implemented the simple Anzelius case using an effective mass transfer coefficient. This is equivalent to the pde for the Rosen model, but with the solid phase mass transfer incorporated into the mass transfer coefficient, making the problem simpler to simulate: in this case m=1.\nI divide the bed into n cells with the first n elements in the vector u the liquid phase concentrations and the next n elements the average solid phase concentrations. The spatial derivatives are replaced with their discrete equivalents.\n\nusing SparseArrays\n\nh_e = h_eff(pb)\n\nfunction parameters(n)\n    v_dm = v*t_shot/L_pb\n    h_dm = h_e*(3/b)*t_shot\n    dξ=1/(n-1)\n\n    # initial conditions\n    u0 = ones(Float64,2n)\n\n    M = spzeros(Float64,2n,2n)\n    # Liquid phase\n    # start of column, with the boundary condition that u[0]=0\n    # du[1] = -v/2dξ*(u[2] - u[0]) - h/m*(u[1] - u[n+1])\n    M[1,1] = -h_dm/m\n    M[1,2] = -v_dm/2dξ\n    M[1,1+n] = h_dm/m\n    \n    # middle column\n    for i in 2:n-1\n        # du[i] = -v/2dξ*(u[i+1] - u[i-1]) - h/m*(u[i] - u[n+i])\n        M[i,i-1] = v_dm/2dξ\n        M[i,i] = -h_dm/m\n        M[i,i+1] = -v_dm/2dξ\n        M[i,i+n] = h_dm/m\n    end\n    \n    # end of column\n    # du[n] = -v*(u[n]-u[n-1])/dξ - h/m*(u[n] - u[2n])\n    M[n,n-1] = v_dm/dξ\n    M[n,n] = -v_dm/dξ - h_dm/m\n    M[n,2n] = h_dm/m\n    \n    # Solid phase\n    for i in n+1:2n\n        # du[i] = h/K*(u[i-n] - u[i]\n        M[i,i-n] = h_dm/K\n        M[i,i] = -h_dm/K\n    end\n    \n    return u0, (0.0, 1.0), M\nend\n\nThe ode for this system is linear and is simply\n\\[ { {d \\mathbf{u} } \\over {d \\tau} } = \\mathbf{M} \\mathbf{u} \\]\n\nfunction rhs!(du,u,M,t)\n    du .= M*u\nend\n\nWhich could presumably be solved by eigendecomposition, but more generally this would be solved using a standard ode solver.\n\nusing OrdinaryDiffEq\n\nsol = solve(ODEProblem(rhs!,parameters(10)...), Tsit5(thread=OrdinaryDiffEq.True()))\n\nsol.retcode\n\nReturnCode.Success = 1\n\n\nThe liquid concentration at the exit is then extracted from the vector solution.\n\n# Pull out the concentration at the exit\nfunction c(t,sol::ODESolution)\n    n = length(sol.u[1])÷2\n    τ = t/t_shot\n    u = sol(τ)\n    return u[n]*c_sat\nend\n\nBelow is a figure showing a series of runs for increasing n. At low values of n the solution looks reasonable, but with much more diffusion than is actually warranted given the mass transfer coefficient. This is a common feature of Method of Lines when applied to pdes of this type and can, if one is not careful, lead to under-estimates of the actual effective diffusion (since much of the diffusion seen in the results is coming from the numerical method). As n increases, a spurious oscillatory behaviour appears at the end of the extraction and damping this requires increasing n &gt; 250.\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 12: Method of Lines with increasing n, converging slowly to the exact (Anzelius) solution.\n\n\n\n\nSince I am ultimately modeling the same pde as was solved, exactly, by Anzelius I can plot the exact solution, showing that n must be quite large, &gt;500, to start to align with the correct answer. Were we to have used the full pde, with n=m=500, this would have required a 250,500 element state vector. This approach becomes severely computationally intensive rather quickly. That said, this is mostly an issue when the moving front is very sharp.\nThis can be alleviated by using a different discretization technique, such as moving finite element but, personally, there is a point where solving the pde gets complicated enough that it’s easier to just use a multiphysics program like comsol than to try implementing it yourself."
  },
  {
    "objectID": "posts/engineering_a_cup_of_coffee_part-2/index.html#conclusion",
    "href": "posts/engineering_a_cup_of_coffee_part-2/index.html#conclusion",
    "title": "Engineering a Cup of Coffee Part Two: Espresso",
    "section": "Conclusion",
    "text": "Conclusion\nSo far I have reviewed the “conventional” approach to packed bed mass transfer, examining the solutions that are recommended in the standard texts on unit operations and leaching.30 All of these approaches over-estimate the initial concentration in the espresso because they assume the bed starts in equilibrium, though if the extraction runs for long enough these models fit the observed results better and better. An alternative approach is to assume the bed is filled with water that is not at equilibrium and the extraction only begins at t=0. This is the approach taken by much of the recent literature on modeling espresso31. The downside to this approach is that it generally underestimates the initial concentration of the espresso.\n30 Schwartzberg, “Leaching – Organic Materials,” 558–63.31 Cameron et al., “Systematically Improving Espresso” page 635; Moroney et al., “Modelling of Coffee Extraction During Brewing Using Multiscale Methods” page 225; Vaca Guerra et al., “Modeling the Extraction of Espresso Components as Dispersed Flow Through a Packed Bed” page 5Since the initial conditions are different, none of the models above are directly comparable to the approaches taken in the literature. Though it should not be a huge undertaking to change the initial conditions after taking the Laplace transform and completing the result from there. Since the Laplace transform and its inverse are linear this should equate to adding an \\(\\exp\\left(...\\right)\\) term to the solution somewhere.\nI think it is also reasonable to be skeptical of the mass transfer coefficients that I estimated. These are based on correlations for packed beds with spherical packing and, while I am modeling the particles as spheres, something may have been lost in the accounting. Most of the mass transfer coefficients ultimately depend upon a good estimate for the solid phase diffusion, which in this case I obtained from literature and is comparable to what one would expect for plant matter like coffee beans.32 But an obvious next step is to compare with actual measured data.\n32 Schwartzberg, “Leaching – Organic Materials,” 557.The model is also highly sensitive to particle size, which the figure below illustrates by successively doubling the effective diameter of the particles, while using the Rasmuson solution (all other parameters remaining equal). This diameter is also an effective parameter and not a directly measured one. It is the diameter of the sphere with an equivalent surface area to a coffee ground or, more accurately, the average of such diameters over the actual particle size distribution of the coffee grounds. This makes it somewhat difficult to determine exactly, especially if one is trying to incorporate the effects of microscopic pores on the effective surface area of coffee grounds. Rough estimates can be made using images taken of the grounds, using an app, but that will always be limited by the resolution of a camera.\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 13: The impact of particle size on espresso extraction.\n\n\n\n\nAnother note of caution is in using the direct numerical integration of the Rosen and Rasmussen solutions. In this notebook I specified the (finite) bounds of integration and the number of points to sample within the interval, which were tuned more or less by eye. That does mean the code is brittle to major changes in some of the packed bed parameters, without going back and re-tuning the parameters of the numerical integration. A more robust approach would determine some of these algorithmically, especially the bounds of integration. I could just leave the upper bound of the integral as Inf, but in my experience there can be domain issues if one isn’t careful and the integrand isn’t capturing all edge cases properly.\nWhile I focused mostly on calculating the various integrals numerically, the asymptotic and approximate forms are probably more useful if you just want to play around and explore how changing different coffee parameters changes overall extraction. They are certainly easier to calculate."
  },
  {
    "objectID": "posts/engineering_a_cup_of_coffee_part-2/index.html#references",
    "href": "posts/engineering_a_cup_of_coffee_part-2/index.html#references",
    "title": "Engineering a Cup of Coffee Part Two: Espresso",
    "section": "References",
    "text": "References\n\n\nAnzelius, A. “Über Erwärmung Vermittels Durchströmender Medien.” Zeitschrift für Angewandte Mathematik Und Mechanik. 6, no. 4 (1926): 291–94. https://doi.org/10.1002/zamm.19260060404.\n\n\nBac̆lić, Branislav, Dus̆an Gvozdenac, and Gordan Gragutinović. “Easy Way to Calculate the Anzelius-Schumann j Function.” Thermal Science 1, no. 1 (1997): 109–16.\n\n\nBird, R. Byron, Warren E. Stewart, and Edwin N. Lightfoot. Transport Phenomena. 2nd ed. Hoboken, NJ: John Wiley & Sons, 2007.\n\n\nCameron, Michael I., Dechen Morisco, Daniel Hofstetter, Erol Uman, Justin Wilkinson, Zachary C. Kennedy, Sean A. Fontenot, William T. Lee, Christopher H. Hendon, and Jamie M. Foster. “Systematically Improving Espresso: Insights from Mathematical Modeling and Experiment.” Matter 2, no. 3 (2020): 631–48. https://doi.org/10.1016/j.matt.2019.12.019.\n\n\nCarslaw, Horatio S., and John C. Jaeger. Conduction of Heat in Solids. 2nd ed. London: Oxford University Press, 1959.\n\n\nGagné, Jonathan. The Physics of Filter Coffee. Scott Rao, 2020.\n\n\nGoldstein, Sydney. “On the Mathematics of Exchange Processes in Fixed Columns i. Mathematical Solutions and Asymptotic Expansions.” Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences 219, no. 1137 (1953): 151–71. https://doi.org/10.1098/rspa.1953.0137.\n\n\nGreen, Don W., ed. Perry’s Chemical Engineers’ Handbook. 8th ed. New York: McGraw Hill, 2008.\n\n\nHottel, Hoyt C., James J. Noble, Adel F. Sarofim, Geoffrey D. Silcox, Phillip C. Wankat, and Kent S. Knaebel. “Heat and Mass Transfer.” In Perry’s Chemical Engineers’ Handbook, edited by Don W. Green, 8th ed. New York: McGraw Hill, 2008.\n\n\nLassey, Keith R. “On the Computation of Certain Integrals Containing the Modified Bessel Function \\(I_0(\\xi)\\).” Mathematics of Computation 39, no. 160 (1982): 625–37. https://doi.org/10.1090/s0025-5718-1982-0669654-6.\n\n\nLeVan, M. Douglas, and Giorgio Carta. “Adsorption and Ion Exchange.” In Perry’s Chemical Engineers’ Handbook, edited by Don W. Green, 8th ed. New York: McGraw Hill, 2008.\n\n\nMoroney, K. M., W. T. Lee, S. B. G. O׳Brien, F. Suijver, and J. Marra. “Modelling of Coffee Extraction During Brewing Using Multiscale Methods: An Experimentally Validated Model.” Chemical Engineering Science 137 (2015): 216–34. https://doi.org/10.1016/j.ces.2015.06.003.\n\n\nMoroney, Ken AND Meikle-Janney, Kevin M. AND O’Connell. “Analysing Extraction Uniformity from Porous Coffee Beds Using Mathematical Modelling and Computational Fluid Dynamics Approaches.” PLOS ONE 14, no. 7 (July 2019): 1–24. https://doi.org/10.1371/journal.pone.0219906.\n\n\nRasmuson, Anders, and Ivars Neretnieks. “Exact Solution of a Model for Diffusion in Particles and Longitudinal Dispersion in Packed Beds.” AIChE Journal 26, no. 4 (1980): 686–90. https://doi.org/10.1002/aic.690260425.\n\n\nRice, R. G. “Letters to the Editor.” AIChE Journal 26, no. 2 (1980): 334. https://doi.org/10.1002/aic.690260241.\n\n\nRosen, J. B. “General Numerical Solution for Solid Diffusion in Fixed Beds.” Industrial & Engineering Chemistry 46, no. 8 (1954): 1590–94. https://doi.org/10.1021/ie50536a026.\n\n\n———. “Kinetics of a Fixed Bed System for Solid Diffusion into Spherical Particles.” The Journal of Chemical Physics 20, no. 3 (1952): 387–94. https://doi.org/10.1063/1.1700431.\n\n\nRousseau, Ronald W., ed. Handbook of Seperation Process Technology. Hoboken, NJ: John Wiley & Sons, 1987.\n\n\nSchumann, T. E. W. “Heat Transfer: A Liquid Flowing Through a Porous Prism.” Journal of the Franklin Institute 208, no. 3 (1929): 405–16. https://doi.org/10.1016/S0016-0032(29)91186-8.\n\n\nSchwartzberg, Henry G. “Leaching – Organic Materials.” In Handbook of Seperation Process Technology, edited by Ronald W. Rousseau. Hoboken, NJ: John Wiley & Sons, 1987.\n\n\nSeader, J. D., Ernest J. Henley, and D. Keith Roper. Separation Process Principles. 3rd ed. Hoboken, NJ: John Wiley & Sons, 2011.\n\n\nThomas, Henry C. “CHROMATOGRAPHY: A PROBLEM IN KINETICS.” Annals of the New York Academy of Sciences 49, no. 2 (1948): 161–82. https://doi.org/10.1111/j.1749-6632.1948.tb35248.x.\n\n\nVaca Guerra, Mauricio, Yogesh M. Harshe, Lennart Fries, James Payan Lozada, Aitor Atxutegi, Stefan Palzer, and Stefan Heinrich. “Modeling the Extraction of Espresso Components as Dispersed Flow Through a Packed Bed.” Journal of Food Engineering 368 (2024): 111913. https://doi.org/10.1016/j.jfoodeng.2023.111913."
  },
  {
    "objectID": "posts/sizing_a_gooseneck_example/index.html",
    "href": "posts/sizing_a_gooseneck_example/index.html",
    "title": "Compressible Flow Example - Sizing a Goose Neck Vent",
    "section": "",
    "text": "When determining the required venting for above ground storage tanks it is typical to calculate normal and emergency vent rates using standards such as API 2000, which gives the venting as an equivalent flow rate of air at standard state. Most off-the-shelf vents are sized in terms of pressure drop and SCFH through it, so after calculating the required venting one can simply buy a vent with the needed characteristics.\nIt’s not uncommon, though, for tanks to have goose-neck vents constructed from piping. This is fairly normal for tanks holding water, or other nonvolatile substances, where the tank is open to atmosphere and there are no dangerous vapours that need to be managed. The goose-neck itself is merely to keep rain, and wildlife, out of the tank.\nSizing the goose-neck to match the required venting involves performing some simple compressible flow calculations, which is fairly straightforward to set up in a generalized way such that, beyond this motivating example, it can be extended to lots of other problems. Though it isn’t at all uncommon, in this particular case, for the flow calculations to be done assuming an incompressible fluid as, over the length of the goose-neck, the pressure drop is typically slight and the compressibility of the gas (air usually) is not important.\nWith that in mind, I am going to work through the problem in stages of escalating complexity, very likely the most complicated (fanno flow) is overkill for this specific example but it’s worth putting it all down as the same tools can be used for compressible flow calculations through many piping situations."
  },
  {
    "objectID": "posts/sizing_a_gooseneck_example/index.html#the-scenario",
    "href": "posts/sizing_a_gooseneck_example/index.html#the-scenario",
    "title": "Compressible Flow Example - Sizing a Goose Neck Vent",
    "section": "The Scenario",
    "text": "The Scenario\nSuppose an atmospheric storage tank with a normal venting requirement, as calculated from API 2000 or the like, of 200×10³ SCFH and a max pressure of 1 psig. We wish to design a goose-neck vent that can handle that level of venting. Suppose that the goose-neck design we have in mind is a vertical length of pipe extending up from the tank roof, two 90° bends, and an exit covered with a mesh screen (to keep birds from nesting in it, yes this is a thing). The goose-neck is a constant diameter of pipe throughout.\nFor notation, the flow begins at the pipe entrance (1) and ends at the exit (2).\n\n\n\n\n\n\nFigure 2: A sketch of the goose neck vent.\n\n\n\nFor the pipe bends the bend radius to diameter ratio needs to be specified, I’m going to suppose \\(r/D = 1.5\\). Another important parameter is the pipe roughness, \\(\\epsilon\\), which for commercial steel is \\(\\epsilon = 0.0457 \\mathrm{mm}\\).1 At this point I could specify a length for the straight section of pipe, a fixed height above the tank roof that is independent of the final chosen diameter of the piping, or I could fix a design and scale the whole vent up and down as required. For simplicity I am going to assume 3ft of piping.\n1 Tilton, “Fluid and Particle Dynamics,” 6–10.At this point I am going to set up the equations with no knowledge of what the final pipe diameter \\(D\\) will be, then numerically solve for the minimum diameter that meets the requirements. The actual diameter will be the next largest NPS size pipe.\n\nusing Unitful: @u_str, uconvert, ustrip, upreferred\n\n# Setting up some convenient unit conversions\nSCFH = uconvert(u\"m^3/s\", 1u\"ft^3/hr\")\npsi = uconvert(u\"Pa\", 1u\"psi\")\nft = uconvert(u\"m\", 1u\"ft\")\ninch = uconvert(u\"m\", 1u\"inch\")\nmm = uconvert(u\"m\", 1u\"mm\")\n\n# Given in the scenario, now converted to SI\nQ = 200e3SCFH\npₐ = 14.696psi\npₘₐₓ = 1psi + pₐ\nL = 3ft\nϵ = 0.0457mm;\n\nI am assuming, for simplicity, that ambient conditions are standard conditions.\n\n# Universal gas constant to more digits than are at all necessary\nR = 8.31446261815324u\"Pa*m^3/mol/K\"\n\n# Standard conditions, 15°C \nTₐ = 288.15u\"K\"\n\n# Some useful physical properties of air\nMw = 0.02896u\"kg/mol\"  # Molar weight of air, from Perry's\nk = 1.4                # Ratio of heat capacities, Cp/Cv, ideal gas\n\n# density of air, ideal gas law, kg/m^3\nρ(p, T) = (p * Mw)/(R * T)\n\n# viscosity of air, from Perry's\nμ(T) = 1u\"Pa*s\"*(1.425e-6*ustrip(u\"K\",T)^0.5039)/(1 + 108.3/ustrip(u\"K\",T));\n\n\nFrictional head loss\nRegardless of the method of performing the compressible flow calculations, the frictional head loss in the piping needs to be accounted for. I am using the K factor method as it is convenient and K factors are tabulated for most everything in references such as Crane’s TP-410. One thing to be very careful with is the difference between the Darcy and Fanning friction factors. I am using Crane’s where everything is in terms of the Darcy friction factor, which is 4× the Fanning friction factor, but Perry’s defaults to the Fanning friction factor.\nFrom Crane’s I have the following K factors for each piece of the goose-neck2\n2 Crane, TP410M Flow of Fluids, A–30.\nEntrance - \\(K_1 = 0.5\\)\nVertical Pipe - \\(K_2 = f \\frac{L}{D}\\)\nFirst 90° bend - \\(K_3 = 14 f_T\\)\nSecond 90° bend - \\(K_4 = 14 f_T\\)\nMesh screen - \\(K_5 = f_T\\)\nExit to atmosphere - \\(K_6 = 1.0\\)\n\nWhere \\(f\\) is the Darcy friction factor, \\(f_T\\) is the turbulent friction factor. I am assuming the entrance to the vent is sharp edged, and the K factors for the bends are for bends with \\(r/D = 1.5\\).\nFor some notational convenience I am going to define the relative roughness \\(\\kappa = { \\epsilon \\over D }\\) and the reduced length \\(l = {L \\over D}\\) so that, along with the Reynolds number \\(Re\\), the K factors are in terms of dimensionless numbers only.\nThe Darcy friction factor generally depends on which regime the flow is in, laminar, transitional, or turbulent. Since I don’t want to be referring to a Moody diagram I want to use a single equation that operates over a wide range of Reynolds numbers and potentially in laminar and transitional flow regimes since I don’t know a priori what the flow in the vent will be. There are equations like the Serghide correlation or Churchill correlation that attempt to fit a Moody diagram but in a more convenient to use manner.\n\n\n\n\n\n\n\n\nFigure 3: A Moody diagram with the Serghide correlation and Churchill correlation overlaid.\n\n\n\n\n\nThe black line conservatively takes the max of either the laminar or turbulent friction factor in the transitional region \\(2100 \\le Re \\le 4000\\). The Churchill correlation fits the general curve well for both the turbulent and laminar region, and provides reasonable values in the transitional region.\nThe Churchill correlation is3\n3 Tilton, “Fluid and Particle Dynamics,” 6–11. The equation here is given in terms of the darcy friction factor.\\[ f = 8 \\left( \\left( \\frac{8}{Re} \\right)^{12} + { 1 \\over {\\left( A+B \\right)^{3/2} } } \\right)^{1/12} \\]\n\\[ A = \\left( 2.457 \\ln\\left( {1 \\over {\\left( \\frac{7}{Re} \\right)^{0.9} + 0.27\\kappa} } \\right) \\right)^{16} \\]\n\\[ B = \\left( \\frac{37530}{Re} \\right)^{16} \\]\nThe turbulent friction factor is the friction factor at fully turbulent flow, when f is no longer dependent upon the Reynolds number.\n\\[ f_T = { 0.25 \\over { \\left( \\log \\left( \\kappa \\over 3.7 \\right) \\right)^2 } } \\]\nWith these defined I can write a function that gives \\(\\sum_j K_j\\) for any \\(\\kappa\\), l, and Re\n\nfunction f(κ, Re)\n    A = (2.457 * log(1/((7/Re)^0.9 + 0.27*κ)))^16\n    B = (37530/Re)^16\n    return 8*((8/Re)^12 + 1/(A+B)^(3/2))^(1/12)\nend\n\nfT(κ) = 0.25/log10(κ/3.7)^2\n\nΣK(κ,l,Re) = 0.5 + f(κ,Re)*l + 14*fT(κ) + 14*fT(κ) + fT(κ) + 1.0;\n\n\n\nThe Reynolds number\nSince the vent has a constant cross sectional area, the mass velocity, \\(G\\), is constant throughout\n\\[ G = \\frac{4 \\dot{m} }{\\pi D^2} \\]\nWhere \\(\\dot{m}\\) is the mass flow rate in kg/s flowing through the vent, which is \\[\\dot{m} = Q_{std} \\cdot \\rho \\left(p_{std}, T_{std} \\right) \\] with \\(Q\\) the flow at standard conditions and \\(\\rho\\) the density at standard conditions.\nNote The required vent flow is given in SCFH, this is not temperature or pressure dependent and \\(\\dot{m}\\) is a constant. If the flow given was a true volumetric flow rate, then \\(\\dot{m}\\) would be a function of temperature and pressure in general. This is one of those things that routinely snags inexperienced engineers as a flow in terms of a standard volume looks like a volumetric flow rate, it’s in units of volume, but really isn’t one since the temperature and pressure are set to standard state by definition.\nThe Reynolds number in terms of the mass velocity is\n\\[ Re = \\frac{G D}{\\mu} \\]\nThe only parameter in the Reynolds number which is not a constant is the viscosity, \\(\\mu\\), which is mostly dependent upon temperature and not pressure. So, to a very good first approximation, the Reynolds number is only a function of temperature. Which is very convenient.\n\n# mass flow, recall Q is at standard state so is not a function of \n# temperature or pressure\nm = Q*ρ(pₐ, Tₐ)\n\n# mass velocity\nG(D) = (4*m)/(π*D^2)\n\n# Reynold's number\n# upreferred promotes derived units to combos of base units\n# e.g. converts Pa -&gt; kg/m/s^2\nRe(D, T) = upreferred(G(D)*D/μ(T));"
  },
  {
    "objectID": "posts/sizing_a_gooseneck_example/index.html#incompressible-flow",
    "href": "posts/sizing_a_gooseneck_example/index.html#incompressible-flow",
    "title": "Compressible Flow Example - Sizing a Goose Neck Vent",
    "section": "Incompressible Flow",
    "text": "Incompressible Flow\nFor problems, like this vent, where the pressure drop is expected to be small it is not unreasonable to assume the flow is approximately incompressible. This is very often what is done since, typically, it does not require any iterative methods and one can solve for incompressible flow directly. It is also useful to do even if one plans on performing a more complete compressible flow calculation, since this provides something of a sanity check and can be a good place to start compressible flow iterative calculations, i.e. the initial guess is from the incompressible case\nTo check whether or not the incompressible assumption is reasonable, consider the ratio of density inside the tank (at the max allowable pressure) to the density outside the tank, assuming ambient temperature. For an ideal gas this is\n\\[ { \\rho_1 \\over \\rho_2 } = { {p_1  Mw} \\over {R  T_a} } { {R T_a} \\over {p_2 Mw} } = \\frac{p_1}{p_2} = \\frac{p_{max} }{p_a}\\]\n\npₘₐₓ/pₐ\n\n1.0680457267283614\n\n\nTypically flows are considered incompressible if the density varies by less than ~5-10%, so this example (where the density varies by ~7%) is right in that range. You could justify it either way and it’s more a function of how accurate the calculations need to be. Since, ultimately, we are solving for the pipe diameter and choosing the next largest pipe size it’s probably fine to use an incompressible flow assumption. If anything the incompressible flow assumption will overestimate the pressure drop and thus lead to an oversized pipe (erring on the side of caution)\nThe mechanical energy balance for an incompressible fluid is4\n4 Tilton, 6–16.\\[ p_1 - p_2 = \\alpha_2 \\frac{\\rho v_2^2}{2} - \\alpha_1 \\frac{\\rho v_1^2}{2} + \\rho g \\left( h_2 - h_1 \\right) + \\sum_j K_j \\frac{\\rho v^2}{2} \\]\nWith the following simplifications + given the assumption of incompressible flow and a vent with a constant cross-sectional area \\(v_1 = v_2 = v\\), + the flow is uniform throughout \\(\\alpha_1 = \\alpha_2 = 1.0\\) + the contribution due to hydro-static pressure is negligible as the gas density is very small, \\(\\rho g \\left( h_2 - h_1 \\right) \\approx 0\\)\nThis becomes\n\\[ p_1 - p_2 = \\sum_j K_j \\frac{\\rho v^2}{2} \\]\nWhere the velocity, \\(v\\) is\n\\[ v = \\frac{Q}{A} = { Q \\over { \\frac{\\pi}{4} D^2 } } \\]\nWhich can be solved algebraically for \\(D\\), where \\(\\rho\\) is taken at the average pressure. That said it is easier to solve it numerically.\n\nusing Roots: find_zero, Brent\n\nv(D) = Q / ((π/4)*D^2)\n\nDᵢₙ(Dₗ, Dᵤ) = find_zero( \n    (D) -&gt; pₘₐₓ - pₐ - 0.5*ΣK(ϵ/D,L/D,Re(D,Tₐ))*ρ((pₘₐₓ + pₐ)/2, Tₐ)*v(D)^2,\n    (Dₗ, Dᵤ), # Lower and upper bracket of the root\n    Brent() ) # Solve using Brent's method\n\nD0 = Dᵢₙ(4inch, 12inch)\n\nuconvert(u\"inch\", D0)\n\n6.497118827423374 inch\n\n\nAt this point one would typically stop for this example, compressible flow calculations are probably unnecessary."
  },
  {
    "objectID": "posts/sizing_a_gooseneck_example/index.html#compressible-flow",
    "href": "posts/sizing_a_gooseneck_example/index.html#compressible-flow",
    "title": "Compressible Flow Example - Sizing a Goose Neck Vent",
    "section": "Compressible Flow",
    "text": "Compressible Flow\nIn general compressible flow situations can be very difficult to solve, since the density of the working fluid is a function of the pressure and temperature and the pressure and temperature are varying throughout, which means heat transfer must also be accounted for in some way. There are several key simplifying assumptions that take this from the sort of problem solved with CFD to something actually quite simple. The first is to assume an ideal gas, the second is to examine two extreme cases of heat transfer: isothermal flow and adiabatic flow.\nAt these two extreme cases, the first where heat transfer is instantaneous and the second where it doesn’t occur at all, provide bounds on the problem.\n\nIsothermal Compressible Flow\nIsothermal compressible flow of an ideal gas is fairly straight forward. As already mentioned the Reynolds number depends only on temperature, which is constant by definition, so the Reynolds number is a constant. This means the frictional head loss is also constant throughout, and it is a simple matter to calculate the pressure drop.\nThe assumption that the flow is isothermal is very reasonable in this case. We are assuming normal venting from a tank at thermal equilibrium with it’s surroundings, that is that the air flowing through the vent starts and ends in reservoirs of equal temperature. As gases expand the temperature decreases but the pressure drop across the vent is small so this effect should be negligible.\nA quick check is to estimate the ratio of temperatures at the start and end of the vent assuming a friction-less adiabatic expansion\n\\[ p_1^{1-k} T_1^k = p_2^{1-k} T_2^k = \\mathrm{const}\\]\n\\[ \\frac{T_2}{T_1} = \\left( p_1 \\over p_2 \\right)^{ {1-k} \\over k}\\]\n\n(pₘₐₓ/pₐ)^((1-k)/k)\n\n0.9813670503935878\n\n\nSo we expect even in the most extreme case the temperature change is ~2%, justifying the assumption that the venting is isothermal.\nThe isothermal flow of an ideal gas going through a length of piping is5\n5 Tilton, 6–23. This equation also neglects changes in elevation.\\[ p_{1}^{2} = G^{2} \\frac{RT}{Mw} \\left[ \\sum \\limits_{j} K_{j} + 2\\ln \\frac{p_{1} }{p_{2} } \\right] + p_{2}^{2} \\]\nIf we assume the system is at thermal equilibrium with the outside air, then \\(T = T_a\\) and \\(p_2 = p_a\\)\nThe only unknown is p1, which can be solved for numerically.\n\npᵢₜ(G,κ,l,Re) = find_zero(\n    p -&gt; p^2 - G^2 * (R*Tₐ/Mw) * (ΣK(κ,l,Re) + 2*log(p/pₐ)) - pₐ^2, \n    pₘₐₓ); # initial guess\n\nAt this point we can write a simple function to solve for the minimum diameter that meets our requirement that \\(p_1 \\le p_{max}\\).\n\nDᵢₜ(Dₗ, Dᵤ) = find_zero( \n    D -&gt; pₘₐₓ - pᵢₜ( G(D), ϵ/D, L/D, Re(D, Tₐ)), \n    (Dₗ, Dᵤ), # Lower and upper bracket of the root\n    Brent() ) # Solve using Brent's method\n\nD1 = Dᵢₜ(4inch, 12inch)\n\nuconvert(u\"inch\", D1)\n\n6.491472166277518 inch\n\n\n\n\nAdiabatic (Fanno) Flow\nAdiabatic flow of an ideal gas through a pipe, also called Fanno flow, is somewhat more difficult than isothermal flow – there are more steps in the iterative solution as the temperature along the length of the vent changes and thus the Reynolds number changes. The general process starts by assuming a constant friction factor, calculating the pressure and temperature changes due to the adiabatic expansion of an ideal gas, adjusting the friction factor for the temperature change, and iterating until everything converges.\nThere are a few ways of setting up the calculations. We could assume the gas exits at ambient conditions – both ambient temperature and pressure – or assume the tank starts at thermal equilibrium with the environment but at a higher pressure and the gas exits at ambient pressure and some other temperature – less than ambient due to adiabatic expansion. The first set of assumptions is in some ways easier to calculate, but the second set of assumptions is more physically realistic, and consistent with the assumptions made when solving the isothermal case.\nOne thing we should check before proceeding is whether or not the flow will be choked, essentially will the flow velocity reach \\(Ma = 1\\), the following discussion assumes flow remains subsonic, and this is easy to check. The critical pressure, at which flow becomes sonic, is given by6\n6 Tilton, 6–23.\\[ { p^o \\over p_1 } = \\left(2 \\over k+1 \\right)^{k \\over {k-1} } \\]\nwith the criteria that flow is subsonic if\n\\[ { p_2 \\over p_1 } &gt; { p^o \\over p_1 } \\]\n\n(pₐ / pₘₐₓ) &gt; (2 / (k+1))^(k/(k-1))\n\ntrue\n\n\nThe basic relation of Fanno flow that drives the equations is the relationship between the Fanno parameter and the Mach number7\n7 Tilton, 6–24.\\[ Fa = \\left( \\frac{fL^{*} }{D} \\right) = \\frac{1 - Ma^{2} }{kMa^{2} } + \\frac{k+1}{2k} \\ln \\left( \\frac{ \\left( k+1 \\right) Ma^{2} }{ 2+\\left( k+1 \\right) Ma^{2} } \\right) \\]\nWhere I am defining \\(Fa\\) to be the Fanno parameter. The Fanno parameter is calculated from some point in the flow path through to the critical point, where flow goes sonic. The critical point can be a hypothetical point, assuming the pipe is infinite, or it can be real. In this case I am assuming the flow within the vent will remain subsonic.\nIt is worth noting that elbows near the exit of a pipe may choke the flow even though the \\(Ma &lt; 1\\) due to the nonuniform velocity profile in the elbow. By the design of this goose-neck we know this is the case and should keep that in mind when evaluating the results.\nFor two points along a pipe, 1 and 2, the difference between their Fanno parameters is the frictional loss between those two points8\n8 Tilton, 6–24.\\[ Fa_1 - Fa_2 = \\sum_{j} K_{j} \\]\nWhere the \\(K_j\\) are usually evaluated at the average temperature \\({ {T_1 + T_2} \\over 2}\\).\nThe Mach number at some point i along the pipe, for an ideal gas, is given by9\n9 Derived for an ideal gas:\n\\[ G = \\rho v = { {p  Mw} \\over {R T} } v\\]\n\\[ c = \\sqrt{ {k R T} \\over Mw } \\]\n\\[ Ma = { v \\over c } = G { {R T} \\over {p  Mw} } \\sqrt{ Mw \\over {k R T} } = \\frac{G}{p} \\sqrt{ \\frac{RT}{kMw} }\\]\\[ Ma_{i} = \\frac{v}{c} = \\frac{G}{p_{i} } \\sqrt{ \\frac{RT_{i} }{kMw} } \\]\nand the pressure can be calculated given a Mach number by rearranging\n\\[ p_{i} = \\frac{G}{Ma_{i} } \\sqrt{ \\frac{RT_{i} }{kMw} } \\]\nand for any two points along the pipe the temperatures are related by10\n10 Tilton, “Fluid and Particle Dynamics” equation 6-116. Taking two points and cancelling out the stagnation temperature.\\[ T_{1} = T_{2} \\frac{2 + \\left( k-1 \\right) Ma_{2}^{2} }{2 + \\left( k-1 \\right) Ma_{1}^{2} } \\]\nPutting all of this together, the procedure for adiabatic ideal gas flow through piping with a given diameter \\(D\\) is:\n\nGiven \\(G\\) calculate \\(Ma_2\\) at ambient conditions, this is the initial guess for the exit conditions\nCalculate \\(\\sum_j K_j\\) at ambient conditions, this is the initial guess for the frictional loss\nCalculate \\(Fa_2\\) with \\(Ma_2\\)\nCalculate \\(Fa_1\\) from \\(Fa_2\\) and \\(\\sum_j K_j\\)\nSolve for \\(Ma_1\\) given \\(Fa_1\\), this is done numerically\nSolve for \\(T_2\\) given \\(Ma_1\\) and letting \\(Ma_2\\) vary with temperature, this is done numerically\nRecalculate \\(Ma_2\\) given \\(T_2\\) and \\(\\sum_j K_j\\) at the average temperature \\({ {T_1 + T_2} \\over 2}\\) and repeat from step 3\nContinue to iterate until \\(p_1\\) stops changing\n\nWhile that looks complicated, each step is fairly easy. In my experience, with subsonic flow, this converges very quickly.\n\n# Fanno parameter\nFa(Ma) = ((1-Ma^2)/(k*Ma^2)) + ((k+1)/(2k))*log( ((k+1)*Ma^2) / (2 + (k+1)*Ma^2))\n\n# Mach number\n# upreferred(...) ensures units cancel appropriately and the Ma is unitless\nMa(G, p, T) = upreferred((G/p)*√((R*T)/(k*Mw)))  \n\n\nT2(T₁, Ma₁, G, p) = find_zero(\n    T -&gt; (2 + (k-1)*Ma₁^2)*T₁ - (2 + (k-1)*Ma(G, p, T)^2)*T,\n    T₁) # Use the isothermal case as an initial guess\n\n\nfunction pfa(D)\n    # pre-calculating diameter dependent variables\n    Gᵢ = G(D)\n    κᵢ = ϵ/D\n    lᵢ = L/D\n\n    # initial values\n    T₂ = Tₐ\n    Ma₁ = Ma(Gᵢ, pₐ, Tₐ)\n    p₁ⁿᵉʷ = uconvert(u\"Pa\",(Gᵢ/Ma₁)*√((R*Tₐ)/(k*Mw)))\n    \n    # loop until the error is below the given tolerance, but don't loop forever!\n    err, i = 1.0, 0\n    rtol, max_count = 1e-9, 1e5\n    while (err &gt; rtol) && (i &lt; max_count)\n        # Starting up\n        Tₐᵥ = 0.5*(Tₐ + T₂)\n        Reᵢ = Re(D,Tₐᵥ)\n        Ma₂ = Ma(Gᵢ, pₐ, T₂)\n             \n        # Steps 3 - 6\n        Fa₂ = Fa(Ma₂)\n        Fa₁ = Fa₂ + ΣK(κᵢ,lᵢ,Reᵢ)\n        Ma₁ = find_zero(x -&gt; Fa₁ - Fa(x), (Ma₂ + Ma₁)/2)\n        T₂ = T2(Tₐ, Ma₁, Gᵢ, pₐ)\n        \n        # Check if pressure has converged\n        p₁ᵒˡᵈ = p₁ⁿᵉʷ\n        p₁ⁿᵉʷ = uconvert(u\"Pa\",(Gᵢ/Ma₁)*√((R*Tₐ)/(k*Mw)))\n        err = abs(p₁ⁿᵉʷ - p₁ᵒˡᵈ)/p₁ᵒˡᵈ\n        i += 1\n    end\n       \n    # if the loop failed to converge, let me know\n    if i &gt;= max_count\n        error_msg = \"iterations exceeded max count, remaining error is $err\"\n        error(error_msg)\n    end\n    \n    return p₁ⁿᵉʷ\nend;\n\n\nDfa(Dₗ, Dᵤ) = find_zero( \n    D -&gt; pₘₐₓ - pfa(D), \n    (Dₗ, Dᵤ), # Lower and upper bracket of the root\n    Brent() ) # Solve using Brent's method\n\nD2 = Dfa(4inch, 12inch)\n\nuconvert(u\"inch\", D2)\n\n6.485474802835819 inch\n\n\n\n\n\n\n\n\nUpdate\n\n\n\nThe method given here is from Perry’s and, while it works, is an awkward way of calculating the flow from a given pressure drop. A better method, adapted from Coulson and Richardson’s is presented here."
  },
  {
    "objectID": "posts/sizing_a_gooseneck_example/index.html#minimum-diameter",
    "href": "posts/sizing_a_gooseneck_example/index.html#minimum-diameter",
    "title": "Compressible Flow Example - Sizing a Goose Neck Vent",
    "section": "Minimum Diameter",
    "text": "Minimum Diameter\nAt this point we have solved for the minimum vent diameter in three different ways and, more or less, got the same answer three times. The minimum diameter is ~6.4in ID for all cases and the next largest standard pipe size is 8in so regardless of the method, in this particular example, one arrives at the same final answer.\nIn general the incompressible model will always overestimate the pressure drop across the vent, leading to a larger vent size, and the adiabatic flow will provide an underestimate, the true minimum would be somewhere between the two. This is seen much more clearly at vent diameters less than ~5in where the pressure drop is more significant, more analogous to relief piping for a pressure vessel than venting for an atmospheric storage tank. Of course all of this is assuming flow remains subsonic, if the pressure drop leads to sonic flow then things are quite different.\n\n\n\n\n\n\n\n\nFigure 4: Pressure drop versus vent diameter for the three models explored."
  },
  {
    "objectID": "posts/sizing_a_gooseneck_example/index.html#concluding-remarks",
    "href": "posts/sizing_a_gooseneck_example/index.html#concluding-remarks",
    "title": "Compressible Flow Example - Sizing a Goose Neck Vent",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\nOften things like compressible flow can be intimidating since these problems, even in the simplified ideal gas case, require iterative solutions and often iterative solutions within iterative solutions. However, once the basic pieces are set up, compressible flow can be fairly simple to deal with. There are some pitfalls here that, if one wanted to create a nice generalized set of code, would have to be dealt with.\nThe big one being all the find_zero() calls that rely on the initial guess being a good one, or the bracketed values actually bracketing the answer. It’s more than possible to supply a terrible initial guess, especially for pipe diameter, and have the root solver fail outright. Adding some code to check that guesses are within the domains of functions would be a start, e.g. catching attempts to take log(0) and returning -Inf or something to ensure that the root-finding algorithms respect function domains. This also presents an opportunity to generate better default values, programmatically, prior to solving. As opposed to me just picking reasonable numbers off the top of my head and having everything work out because I’m lucky.\nRelatedly there is a lot of room to fiddle around with which root finding algorithm is employed."
  },
  {
    "objectID": "posts/sizing_a_gooseneck_example/index.html#references",
    "href": "posts/sizing_a_gooseneck_example/index.html#references",
    "title": "Compressible Flow Example - Sizing a Goose Neck Vent",
    "section": "References",
    "text": "References\n\n\nCrane. TP410M Flow of Fluids. Stamford, CT: Crane, 2013.\n\n\nGreen, Don W., ed. Perry’s Chemical Engineers’ Handbook. New York: McGraw Hill, 2007.\n\n\nTilton, James N. “Fluid and Particle Dynamics.” In Perry’s Chemical Engineers’ Handbook, edited by Don W. Green. New York: McGraw Hill, 2007."
  },
  {
    "objectID": "posts/engineering_a_cup_of_coffee/index.html",
    "href": "posts/engineering_a_cup_of_coffee/index.html",
    "title": "Engineering a Cup of Coffee",
    "section": "",
    "text": "While making coffee one day, I started thinking about how the coffee making process is both a perfect representation of the sorts of systems chemical engineers work on every day and also a weird edge case unlike most of the unit operations in the standard repertoire of process engineering.\nMaking coffee involves heat, mass, and momentum transfer across multiple phases – pretty standard stuff for undergraduate chemical engineering curricula. On the other hand, while industrial scale leaching operations are generally designed for maximum efficiency – removing the most amount of a substance with the least amount of solvent, energy, etc. – coffee makers are specifically designed to avoid that outcome. A saturated cup of coffee would be strong, harsh, and undrinkable. Coffee making, as a unit op, aims at a managed inefficiency, which makes for an interesting design case1"
  },
  {
    "objectID": "posts/engineering_a_cup_of_coffee/index.html#extraction-and-the-coffee-control-chart",
    "href": "posts/engineering_a_cup_of_coffee/index.html#extraction-and-the-coffee-control-chart",
    "title": "Engineering a Cup of Coffee",
    "section": "Extraction and the Coffee Control Chart",
    "text": "Extraction and the Coffee Control Chart\nI claimed that making coffee is, in a sense, a deliberately inefficient process. By this I mean the goal is not to maximize extraction – defined as the mass of solids dissolved in the final cup of coffee relative to the starting mass of coffee grounds – but instead to target some middle ground. This is largely because extraction is an imperfect measure of what we actually want. Coffee releases a whole slew of flavour compounds and a good cup of coffee is a balance of all of these. However we have both limited variables to control and limited knowledge of that final composition. Even simply measuring the total dissolved solids (TDS) with a refractometer puts one well into the stratospheric heights of coffee nerd-dom. Trying to monitor all of the relevant flavour compounds would require something like a quarter-million dollar GC-MS, well out of the reach of most coffee obsessives.\nSo extraction is really the best we have, as far as quantitative measures go, with the giant caveat that coffee with the same extraction, from the same beans, can taste quite different depending on the brew method. Using an indirect measurement for the actual process variable of interest is not too different from how a lot of unit operations are controlled, distillation, for example, often uses temperature as a proxy for the composition.\n\n\n\n\n\n\nFigure 1: The standard coffee control chart.2\n\n2 Batali, Ristenpart, and Guinard, “Brew Temperature, at Fixed Brew Strength and Extraction, Has Little Impact on the Sensory Profile of Drip Brew Coffee,” fig. 1.\n\nThe standard way of thinking about coffee extraction starts with Lockhart’s coffee control chart, this plots the concentration of solids (TDS) against total extraction. The diagonal lines represent a given dose of coffee (I typically brew 55g/L with my V60, which puts me pretty near the sweet spot). A given brew moves along the diagonal line for the given dose, moving from the bottom left to the upper right as the brew proceeds. The goal is to stop the brew once the extraction and strength (concentration) have reached the optimal level3:\n3 For a given dose of coffee, the concentration and extraction are directly proportional to one another.\n\\[ \\mathrm{Dose} = D = { m_{beans} \\over V_{water} } \\approx { m_{beans} \\over V_{cup} } \\]\n\\[ \\mathrm{Extraction} = E = { m_{cup} \\over m_{beans} } = { { c_{cup} V_{cup} } \\over m_{beans} } \\]\n\\[ c_{cup} = D \\cdot E \\]\nwhere the subscript cup means the mass/volume that ends up in the final cup of coffee. This is only approximately the case as some water is absorbed into the coffee grounds. The amount of water retained in the coffee grounds can be accounted for, giving a more accurate measure of final extraction.For industrial scale distillation, absorption, extraction, leaching, etc. the process is usually modeled as a series of equilibrium stages, and the whole point is to maximize extraction and concentration. This leads to designs for counter-current solids extractors such as a Rotocel extractor or a Bollman extractor\n\n\n\n\n\n\nFigure 2: A Rotocel extractor, you are unlikely to see one of these at your local coffee shop.\n\n\n\nExtractors like this are, in fact, how one might decaffeinate coffee. In that case one does want to maximize the extraction of caffeine, and is free to adjust several parameters such as the solvent (with options such as supercritical CO2, dichloromethane, or ethyl acetate) that are otherwise pretty fixed for normal coffee making. At the end of the day a cup of coffee has to be made with water, a steaming cup of dichloromethane just won’t cut it.\nCoffee makers inhabit a space where the design parameters are highly restricted. Outside of espresso, the machine has to operate at atmospheric pressure and temperatures achievable with a normal kettle. The solvent must be water. The process is likely batch or semi-batch.4 The extraction happens fully within the mass-transfer dominated regime, specifically avoiding reaching equilibrium (the fundamental design assumption in most industrial extractors) as that leads to over-extracted coffee.\n4 I would love to see a fully continuous coffee maker, like the fully continuous industrial operations, and there is no reason why you couldn’t make one. Imagine going into your local coffee shop and seeing a glass fluidized bed continuously circulating grounds and hot water, that would be pretty groovy."
  },
  {
    "objectID": "posts/engineering_a_cup_of_coffee/index.html#the-simplest-coffee-maker",
    "href": "posts/engineering_a_cup_of_coffee/index.html#the-simplest-coffee-maker",
    "title": "Engineering a Cup of Coffee",
    "section": "The simplest coffee maker",
    "text": "The simplest coffee maker\nPerhaps the simplest method for making coffee is to put coffee grounds and water in a vessel, add heat, and let it steep for a while. This is, for example, how Turkish coffee is made as well as qahwa, bunna, and many others. A French press and other infusion brewers are a very similar idea except that the water is also the source of heat, and the pot is left to steep without any additional heat input. That’s not the only difference, of course, they differ quite substantially in grind size, whether or not the grounds are strained out at the end, and in the addition of spices or sugar during the brew. But for the purposes of building a simple model all of these methods are vessels in which coffee steeps in hot water. There are three main process variables that impact coffee extraction, and taste, for a given set of beans: brew temperature, grind size, and brew time.\nIn some ways this makes these methods some of the easier ways to make good coffee. Dialing in grind size and temperature is reasonably straight forward and once set remain constant. The remaining variable, time, is relatively easy to adjust: simply wait longer.\nModeling extraction is fairly straight forward, after some basic assumptions are made: that the brew is isothermal, that the ground coffee is uniform and with constant dimensions, and that the liquid phase is well mixed. All of these assumptions are wrong to some degree, and how wrong they are will ultimately govern how useful this model is.\n\nBrew temperature\nBrew temperature is an obvious variable to change, though it has wide ranging impacts and parsing out what exactly changing the temperature does is not obvious. Firstly, the solubility of the various compounds extracted from the beans is a function of temperature and in general solubility is difficult to predict, but broadly speaking solutes are more soluble at higher temperatures. Coffee is more extractable at higher temperatures. However the coffee matrix is complex and there are more than just two phases involved: flavour compounds in the coffee will partition between the solid matrix, coffee oils, and the water at different proportions depending upon the temperature. This is perhaps what is behind the notable difference in taste between cold brew versus a hot immersion brew. Even when made with the same beans, and to the same concentration, the flavour profile of cold brew is quite different.5 That said, over the range of temperatures used to brew a French press, this may not be very important.6\n5 Batali et al., “Sensory Analysis of Full Immersion Coffee”.6 Batali, Ristenpart, and Guinard, “Brew Temperature, at Fixed Brew Strength and Extraction, Has Little Impact on the Sensory Profile of Drip Brew Coffee”.7 Schwartzberg, “Leaching – Organic Materials,” 558; Poling, Prausnitz, and O’Connell, The Properties of Gases and Liquids, 11.21–33.Secondly, brew temperature impacts the rate of extraction. Generally speaking, diffusion coefficients are proportional to (absolute) temperature, \\(\\mathscr{D} \\propto T\\).7 At higher temperatures the various flavour compounds will diffuse more quickly through the grounds and also through the coffee, thus making the brew faster.\nTo make modeling extraction simpler, we assume the brew temperature is constant. This means that, whatever the relative solubilities or rate constants turn out to be, they are constant with respect to time. The only thing varying over time is the concentration of coffee solubles in water and remaining in the grounds. For something like Turkish coffee, the system is probably close to isothermal as it is continuously heated and will remain at or near the boiling point of water the entire time. For a French press this is less true, as the press will lose heat to the environment. How much heat is lost over the course of the brew is going to depend strongly upon the press and the environment it is in. My French press is a double walled stainless steel carafe like this one and likely loses much less heat than a more typical glass carafe. It is also important to consider whether or not the French press is pre-heated. If not, the brew temperature is not going to be the temperature of the kettle. The carafe has significant thermal mass, especially if it is glass, and it will absorb a lot of heat out of the water over the course of the brew (in addition to losing heat to the environment).\nSuppose my French press starts off at 95°C and cools to 75°C – a sizable loss of heat – how much impact would that have on extraction rate? Since \\(\\mathscr{D} \\propto T\\), the percent change in the rate constant is equal to the percent change in (absolute) temperature\n\\[ { { \\Delta \\mathscr{D} } \\over \\mathscr{D} } = { { \\Delta T } \\over T } \\]\n\n\nΔT = 20\nT = 368.15\nΔT / T = 0.054325682466385986\n\n\nEven over this significant loss of heat, that translates to only a 5.4% change in the rate constants. To the exacting standards of a coffee nerd that may seem like a lot, but to chemical engineer that is really not much, it justifies the isothermal assumption (at least as a first approximation).\n\n\nGrind size and uniformity\nGrind size is important if only for being where most of your money can get sunk when building out your home coffee set-up. A good grinder is not cheap, and a bad grinder leads to truly bad coffee. In this case what you are chasing is the ability to tune the average grind size as well as the uniformity of the size of particles produced by the grinder. A good grinder can reliably produce a consistent and suitably narrow particle size distribution.\nWhy does grind size matter at all? The grind size determines the available surface area of the coffee. Mass transfer from the coffee beans (grounds) to the water is proportional to the surface area of coffee exposed to water, and so changing the grind size directly impacts the rate of extraction. The direct impact of grind-size is typically quantified through the specific area, av, which is the surface area of the particle per unit volume. For a sphere this is\n\\[ a_v = {S \\over V} = { {4 \\pi b^2} \\over { \\frac{4}{3} \\pi b^3} } = {3 \\over b} \\]\nwhere b is the radius of the particle. This leads immediately to the observation that, for the same dose of coffee, a finer grind leads to larger overall surface area and thus a faster rate of extraction. It also hints at why a uniform particle size distribution is important: a smaller particle has proportionately more surface area and will experience faster extraction than a larger particle, leading to the smallest particles (the fines) being over extracted while the largest particles (the boulders) are under extracted.\nOf course coffee grounds are not perfect spheres, they have a complex shape arising from the combination of cutting and brittle fracture that characterize the grinding process. The standard engineering approach is to assume that they are spheres anyways, since that is a simpler geometry to work with, and adjust for the non-sphericity with some sort of shape factor or other parameter. In the case of mass and heat transfer, typically that is the Sauter mean diameter (or Sauter mean radius), which is essentially the average diameter of the distribution of spheres that would have the same specific area as the actual particles. For an individual particle the Sauter radius is\n\\[ b_{s} = {3 \\over a_v } \\]\nIt is important to note, though, that the following model is developed for spheres and only works as well as the grounds can be approximated as spheres.\n\n\nMixing and rate constants\nMass transfer problems, like this one, ultimately come down to finding good rate constants. They can be measured, estimated from a correlation, or simply tabulated in a reference, but regardless the model is only as good as the rate constants. The rate constants define, to some extent, the model itself and govern one of the key brew variables: brew time.\nIn the case of coffee, and organic materials in general, there is a complex micro-scale geometry involving multiple phases: the solid ground itself, coffee oils, and water. The coffee will diffuse from the solid into the oils, into water in the interstitial spaces, and also out into the bulk liquid. All of these processes have potentially different rate constants. Additionally the solid phase is not structurally homogeneous, it is a complex arrangement of coffee bean cells, voids, pores and such. Building a model to incorporate all of this complexity is certainly possible8 but the standard approach is to treat this as a two-phase problem where all of the complexity of the solid phase, the marc, and any secondary phases (e.g. coffee oils) are all averaged together into one pseudo-homogeneous solid phase and the solvent (water) forms the liquid phase. This approximation leaves us with two mass transfer rates: the diffusion through the (pseudo-homogeneous) solid phase, within the coffee particles, and the diffusion through the solvent phase, the water outside of the coffee particles. At the interface, the solute leaves the solid phase and enters the liquid phase.\n8 Moroney et al., “Modelling of Coffee Extraction During Brewing Using Multiscale Methods”.9 Schwartzberg, “Leaching – Organic Materials,” 557.For organic material with hard cell walls the relative diffusivity of the solid phase to the liquid phase generally falls along the range \\(\\frac{\\mathscr{D}_s}{\\mathscr{D}_l} = 0.1-0.2\\),9 this allows us to estimate the (effective) diffusivity within the solid based on measured diffusivities in liquid water. It also tells us that diffusion through the solid is 5-10× slower than in the liquid phase and so, depending upon the geometry of the problem, diffusion through the solid phase may be the governing rate.\nDiffusion through the liquid phase is complicated by mixing. The diffusivity used above is the diffusivity in quiescent liquid water. In practice, in the brew vessel, the liquid will be moving and convective mass transfer will be very significant. Usually for mass transfer problems this is all rolled up into a mass transfer coefficient h which combines all of the flow complexity and geometry of the problem into a single coefficient. This is then typically estimated using correlations for the Sherwood number.\nThe interface between the solid and liquid phase introduces a complication as there is some partitioning between the phases happening at the interface. If there wasn’t coffee couldn’t be made. A critical piece of the model is assuming a relationship between the concentration immediately on the solid side of the interface and the concentration immediately on the liquid side of the interface. For organic leaching it is typical to assume linear equilibrium with an equilibrium distribution coefficient\n\\[ K = \\frac{ q^{*} }{ c^{*} } \\]\nWhere q is the concentration of solute in the solid phase and c is the concentration of solute in the liquid phase. This is equivalent to assuming that there are two first order processes happening\n\\[ \\mathrm{coffee}_{s} \\xrightarrow{k_1} \\mathrm{coffee}_{l} \\]\n\\[ \\mathrm{coffee}_{l} \\xrightarrow{k_2} \\mathrm{coffee}_{s} \\]\nAt equilibrium the rates of these two processes are equal\n\\[ k_1 q^{*} = k_2 c^{*} \\Leftrightarrow \\frac{k_2}{k_1} = \\frac{ q^{*} }{ c^{*} } = {K} \\]\nTypically one assumes that at the interface, in the infintesimally thin slice of liquid on one side and the infintesimally thin slice of solid on the other, the solute is always at equilibrium (this is not the same as assuming the system is at equilibrium)\n\n\nAn example brew\nAt this point we can start defining what our specific brew is going to be: roast, grind size, dose, and water temperature. From this we can work to estimate the necessary parameters, such as the equilibrium constant, solid and liquid phase diffusivities. To an extent, these parameters then govern what specific model is used to model the brew.\n\n# properties of the coffee grounds\n# equilibrium parameters\n# Moroney et al. 2015\nq_sat = 118.95 # kg/m³\nc_sat = 212.4 # kg/m³\nK = q_sat/c_sat\n\n# effective diffusivity\n# Moroney et al. 2015; Schwartzberg 1987, 557\n𝒟ₗ = 2.2e-9 # m²/s\n𝒟ₛ = 0.1*𝒟ₗ\n\n# particle size\n# Moroney et al. 2015\nb = 569.45e-6 # m\n\n# density, medium roast \n# Rodrigues et al. 2002, 8\nρₛ = 314.0 # kg/m³\n\n# dose\n# assumed, 22.5g in 500mL\nmₛ = 0.0225  # kg\nVₛ = mₛ/ρₛ  # m³\nVₗ = 500e-6 # m³\n\n\n# properties of the water\n# density\n# Poling et al. 2007, 2-103\nMW = 18.015 #kg/kmol\nfunction ρₗ(T)\n    τ = 1 - T/647.096\n    mol_dens = 17.863 + 58.606*τ^0.35 - 95.396*τ^(2/3) + 213.89*τ - 141.26*τ^(4/3)\n    return mol_dens*MW\nend\n\n# viscosity\n# Poling et al. 2007, 2-432\nμₗ(T) = exp(-52.843 + 3703.6/T + 5.866*log(T) - 5.879e-29*T^10)\nνₗ(T) = μₗ(T)/ρₗ(T)\n\n# brew temperature\n# assumed, 95°C\nTₗ = 95+273.15 #K\n\n# initial concentration\nc₀ = 0.0 # kg/m³\n\n# final (max) concentration\nc_max = min(q_sat*Vₛ/Vₗ + c₀, c_sat) # kg/m³\n\nThese are a lot of parameters and I think it is good practice to think about how to organize them into a struct. In this case I define an InfusionBrew struct to store all of the parameters necessary for defining the brew recipe for an infusion brewer.\n\nstruct InfusionBrew{T}\n    K::T\n    q_max::T\n    c_max::T\n    Vₗ::T\n    Vₛ::T\n    mₛ::T\n    𝒟ₛ::T\n    𝒟ₗ::T\n    b::T\nend \n\n\nbrew = InfusionBrew(K,q_sat,c_max,Vₗ,Vₛ,mₛ,𝒟ₛ,𝒟ₗ,b);"
  },
  {
    "objectID": "posts/engineering_a_cup_of_coffee/index.html#a-mass-transfer-model-of-coffee-brewing",
    "href": "posts/engineering_a_cup_of_coffee/index.html#a-mass-transfer-model-of-coffee-brewing",
    "title": "Engineering a Cup of Coffee",
    "section": "A mass transfer model of coffee brewing",
    "text": "A mass transfer model of coffee brewing\nPulling together all of the information we have collected about coffee we can build a partial differential equation to describe the brewing process, making the following assumptions:\n\nThe system is isothermal with brew temperature Tl\nCoffee grounds are spherical and have constant radius b\nThe coffee matrix is a pseudo-homogeneous solid, diffusion through the solid follows Fick’s second law with diffusivity \\(\\mathscr{D}_s\\) and diffusion is only relevant in the radial direction r\nThe liquid phase is well mixed, i.e. the bulk concentration c is spatially homogeneous and is only a function of time\nMass transfer into the liquid phase occurs through a thin film with a mass transfer coefficient h\nAt the interface between the thin film and the solid coffee, the concentration of solubles is in equilibrium with equilibrium constant K\n\nWe can visualize this set-up with three “phases”, the bulk liquid, a thin film around the coffee particle, and the pseudo-homogeneous solid coffee particle itself. Coffee is extracted from the particles into the thin film and from the thin film into the bulk liquid.\n\n\n\n\n\n\nFigure 3: The mass transfer system for making coffee with a French press.\n\n\n\nThere are two rates important processes governing the extraction of coffee:\n\nDiffusion across the interface into the thin film, governed by Fick’s first law\nTransfer from the thin film into the bulk liquid\n\nStarting with (1) the mass flux into the thin film is given by Fick’s first law (in spherical coordinates)\n\\[ J_1 = - \\mathscr{D}_s \\left( { \\partial q } \\over { \\partial r} \\right)_{r=b} \\]\nOf course the concentration in the solid, q, is a function of time (as more is extracted, there less left behind), which is given by Fick’s second law (in spherical coordinates)\n\\[ { {\\partial q} \\over {\\partial t} } = \\frac{1}{r^2} { \\partial \\over {\\partial r} } \\left( r^2 \\mathscr{D}_s { {\\partial q} \\over {\\partial r} } \\right) \\]\nTurning to (2) the mass flux from the thin film into the bulk liquid is given by\n\\[ J_2 = - h \\left( c - c_s \\right) \\]\nWhere c is the concentration in the bulk liquid and cs is the concentration at the surface.\nThe change in concentration in the bulk liquid with respect to time can also be written in terms of a mass balance on the liquid phase:\n\\[ V_l { { \\partial c} \\over {\\partial t} } = a_v V_s J_2  = \\frac{3}{b} V_s J_2 \\]\n\\[ { { \\partial c} \\over {\\partial t} } = \\frac{3}{b} \\frac{V_s}{V_l} J_2 \\]\nThe solution to this partial differential equation depends upon which of these mass transfer processes, (1) or (2), is dominant.\n\nThe dominant rate\nThe standard approach to solving this problem is to look at the limiting cases, where the Biot number is either very large or very small10\n10 Seader, Henley, and Roper, Separation Process Principles, 663.\nBi &lt; 0.001 : the mass transfer through the film dominates, a simple exponential model is appropriate\n0.001 &lt; Bi &lt; 200 : use an intermediate method11\nBi &gt; 200 : the mass transfer through the coffee particles dominates, the more complicated solution from Carslaw and Jaeger is best\n\n11 The intermediate solution is not given in Seader, Henley, and Roper, Separation Process Principles, only a reference: Schwartzberg, Henry G. and R. Y. Chao. 1982. “Solute Diffusivities in Leaching Processes.” Food Technology. 36, no. 2: 73-86, which has not been digitized and is not available from my local library, so I have no idea what it says ¯\\_(ツ)_/¯12 Conduction of Heat in Solids, 240–41.13 Seader, Henley, and Roper, Separation Process Principles, 663.I will argue in a very hand-wavy way that the Biot number for mass transfer is likely to be large, and so the model from Carslaw and Jaeger12 is the probably the best model. First let’s start with Biot number for mass transfer, which for this situation is13\n\\[ \\mathrm{Bi} = { {h b} \\over {K \\mathscr{D}_s} } = {\\mathscr{D}_l \\over \\mathscr{D}_s} { \\mathrm{Sh} \\over K }\\]\nWhere Sh is the Sherwood number, defined as\n\\[ \\mathrm{Sh} = { {h b} \\over \\mathscr{D}_l } \\]\nDefining the Biot number in terms of the Sherwood number might, at first glance, not seem tremendously useful. However, if we suppose the Froessling equation14 for flow past a single sphere applies\n14 Hottel et al., “Heat and Mass Transfer,” 5–69.\\[ \\mathrm{Sh} = 2 + 0.552 \\mathrm{Re}^{1/2} \\mathrm{Sc}^{1/3} \\]\nwith Re the Reynold’s number and Sc the Schmidt number, then we have a correlation for the Biot number as a function of the Reynold’s number.\n\\[ \\mathrm{Bi} = {\\mathscr{D}_l \\over \\mathscr{D}_s} { 1 \\over K } \\left( 2 + 0.552 \\mathrm{Re}^{1/2} \\mathrm{Sc}^{1/3} \\right) \\]\n\\[ \\mathrm{Bi} = { 20 \\over K } + { 5.52 \\over K } \\mathrm{Re}^{1/2} \\mathrm{Sc}^{1/3} \\]\nWhere \\({\\mathscr{D}_s \\over \\mathscr{D}_l}\\) = 0.1 is assumed from Schwartzberg15 The Schmidt number, Sc, and equilibrium constant, K, can be calculated\n15 “Leaching – Organic Materials,” 557.\n\nSc = νₗ(Tₗ) / 𝒟ₗ = 139.98370415887905\nK = q_sat / c_sat = 0.5600282485875706\n\nBi = 35.7124842370744 + 51.178588534736114 √Re\n\n\nUnder this model, any flow with Re &gt; 10.3 corresponds to Bi &gt; 200, which occurs when the velocity is\n\nRe = 10.3\n\nv = Re*νₗ(Tₗ)/b\n\n0.005570341094459917\n\n\nthat is 5.6mm/s, a velocity so small that it may be achieved through the natural convection occurring within a French press (and especially so in the case of something heated from below like Turkish coffee), but is certainly the case when the French press is stirred.\nRegardless it is unlikely that \\(\\mathrm{Bi}&lt;0.001\\) and thus the simple exponential model is probably not a good fit, we turn instead to the model from Carslaw and Jaeger.16\n16 Conduction of Heat in Solids.\n\nBoundary conditions\nIn the above I casually disregarded boundary conditions, focusing instead on refining the model. Before we move forward we should take a moment to clarify what the boundary conditions are.\nFirst off the coffee starts with a set of initial concentrations q0 and c0, usually these would be the max concentration in the solid phase and zero respectively but they don’t have to be. By disregarding the transfer through the thin film we impose another boundary condition: that at r = b the solid-phase concentration is at equilibrium with the concentration in the bulk liquid qr=b = K c\n\nt = 0 : q = q0 and c = c0\nr = b : q = K c\nr = 0 : q is finite\n\n\n\nThe Carslaw and Jaeger model\nIt might, at first glance, appear that I have lost the thread, Carslaw and Jaeger17 is a book on heat transfer, this is a mass transfer problem. This is an example of the unreasonable effectiveness of treating transport phenomena as a unified subject. By putting the PDE into dimensionless form we find that the PDE for the equivalent heat transfer problem (a solid sphere cooling in a liquid) has already been solved and we can just use that answer.\n17 Carslaw and Jaeger.First step, to put the PDE in dimensionless form we make the substitutions:\n\\[ \\xi = {r \\over b} \\]\n\\[ \\tau = { {\\mathscr{D}_s t} \\over b^2} \\]\n\\[ u = { { q - q^{*} } \\over { q_0 - q^{*} } }\\]\n\\[ u_f = { {c - c^{*} } \\over { c_0 - c^{*} } }\\]\nAfter which the PDE becomes\n\\[ { {\\partial u} \\over {\\partial \\tau} } = \\frac{1}{\\xi^2} { \\partial \\over {\\partial \\xi} } \\left( \\xi^2 {\\partial \\over {\\partial \\xi} } \\right) \\]\nWith boundary conditions\n\nτ = 0 : u = 0\nξ = 1 : u = uf\nξ = 0 : u is finite\n\nAnd the mass transfer into the liquid bulk becomes\n\\[ { {\\partial u_f} \\over {\\partial \\tau} } = -\\frac{3}{\\alpha} \\left. { {\\partial u} \\over {\\partial \\xi} } \\right|_{\\xi=1} \\]\nWith \\(\\alpha = { V_l \\over {K V_s} }\\) and boundary condition\n\nτ = 0 : uf = 1\n\nThis is the equivalent PDE (in dimensionless form) to the heat transfer case for a hot solid sphere cooling in a well mixed fluid,18 with the solution\n18 Carslaw and Jaeger, 240–41; Bird, Stewart, and Lightfoot, Transport Phenomena, 379–81.\\[ u_f = 6α (α+1) \\sum_{k=1}^{\\infty} { \\exp(-τ x_k^2 )\\over { 9(α+1) + (α x_k)^2 } } \\]\nWhere the xk s are the roots of the equation\n\\[ \\tan(x) = { {3 x} \\over { 3 + \\alpha x^2 } } \\]\n(the particular form shown here comes from Schwartzberg19)\n19 “Leaching – Organic Materials”.The first problem, when actually using this solution, is generating the roots of the equation. The original equation has a repeated singularity and, in my experience, off-the-shelf root finding algorithms have trouble with that and will find spurious zeros in the vicinity of the singularities.\nA better approach is to re-write it in a different way\n\\[ \\tan(x) = { \\sin(x) \\over \\cos(x) } \\]\n\\[ \\tan(x) - { {3 x} \\over { 3 + \\alpha x^2 } } = 0 \\Leftrightarrow \\left( 3 + \\alpha x^2 \\right) \\sin(x) - 3 x \\cos(x) = 0 \\]\nThis latter form is nice and continuous, with no singularities.\n\nusing IntervalRootFinding\nusing Roots\n\nα = Vₗ/(K*Vₛ)\n\nf(x) = tan(x) - 3x/(3+α*x^2)\ng(x) = (3 + α*x^2)*sin(x) - 3x*cos(x)\n\n# find the first 5 roots\nk=5\nxk = find_zeros(g, 0, (k+1/2)*π)\n\n6-element Vector{Float64}:\n  0.0\n  3.214656575481129\n  6.321030394109289\n  9.45018248676156\n 12.585470558898335\n 15.723260568418107\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: The roots of the equations f(x) and g(x), note the repeated singularities in f(x).\n\n\n\n\nSince α is fixed for a given problem we will end up using the same roots over and over again, so it would be nice to pre-calculate those roots. However, at this point, we don’t know how many we will need to get a reasonable answer. So my approach is to calculate as many as we need dynamically: if we need more roots than have already been calculated, calculate those ones and append them to the list of already calculated roots.\n\nfunction getroots(n)\n    if n ≤ length(xk)\n        return xk[2:n]\n    else\n        new_roots = find_zeros(x -&gt; g(x), xk[end], (n+1/2)*π)\n        append!(xk, new_roots)\n        return xk[2:end]\n    end\nend\n\nThe standard approach to calculating an infinite series is to use Richardson extrapolation as this accelerates convergence and allows for an error estimate.\n\nusing Richardson:extrapolate\n\nfunction u_f(τ)\n    val, err = extrapolate(1, x0=Inf) do N\n                xk = getroots(Int(N))\n                6α*(α+1)*sum( exp.(-τ.*(xk.^2))./((9*(α+1)).+(α.*xk).^2) )\n            end\n    return val\nend\n\nNow we can put together a bulk concentration function\n\nfunction c(t)\n    τ = (𝒟ₛ*t)/b^2\n    c = (c₀ - c_max)*u_f(τ) + c_max\n    return c\nend\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: The concentration of solubles in the extract over time.\n\n\n\n\nExtraction is simply concentration over dose\n\nextraction(t) = c(t)*Vₗ/mₛ\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: The extraction of coffee solubles over time.\n\n\n\n\n\n\nPackaging the final result\nAt this point we have enough to put together a struct to contain the parameters needed for the Carslaw and Jaeger model\n\nstruct CarslawSolution{T}\n    α::T\n    τ₁::T\n    xk::Vector{T}\n    ib::InfusionBrew{T}\nend\n\nfunction CarslawSolution(ib::InfusionBrew)\n    α  = ib.Vₗ/(ib.K*ib.Vₛ)\n    τ₁ = ib.𝒟ₛ/ib.b^2\n    xk = find_zeros( x -&gt; (3 + α*x^2)*sin(x) - 3x*cos(x) , 0, (10.5)*π)\n    return CarslawSolution(α, τ₁, xk, ib)\nend\n\nand update our code to add some methods for calculating the concentration and extraction based on a Carslaw and Jaeger model for the infusion brew.\n\nfunction getroots(n, model::CarslawSolution)\n    if n ≤ length(model.xk)\n        return model.xk[2:n]\n    else\n        new_roots = find_zeros(x -&gt; (3 + model.α*x^2)*sin(x) - 3x*cos(x), \n                               model.xk[end], (n+1/2)*π)\n        append!(model.xk, new_roots)\n        return model.xk[2:end]\n    end\nend\n\nfunction c(t, model::CarslawSolution)\n    τ = model.τ₁*t\n    α = model.α\n    u_f, err = extrapolate(1, x0=Inf) do N\n                  xk = getroots(Int(N), model)\n                  6α*(α+1)*sum( exp.(-τ.*(xk.^2))./((9*(α+1)).+(α.*xk).^2) )\n               end\n    c = (c₀ - c_max)*u_f + c_max\n    return c\nend\n\nextraction(t, model::CarslawSolution) = c(t, model)*model.ib.Vₗ/model.ib.mₛ\n\n\nsol = CarslawSolution(brew);\n\nThe advantage of packaging code like this is that is now easy to explore the impact of changes to individual parameters, for example below is the impact that changing grind size has on the extraction curve. It follows our general intuition that smaller grind sizes extract faster. It also shows a major weakness of this model: there is only one particle size in the model, which is average over the range of actual particle sizes. This model works well if the grind is quite uniform, however if there is a wide range of particle sizes the actual coffee will be a mix of over extracted coffee (from the small particles) and under extracted coffee (from the large particles).\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: The evolution of coffee extraction over time for several grind sizes. Note that the smallest grind sizes extract faster, achieving equilibrium, whereas the largest grind sizes extract more slowly."
  },
  {
    "objectID": "posts/engineering_a_cup_of_coffee/index.html#final-thoughts",
    "href": "posts/engineering_a_cup_of_coffee/index.html#final-thoughts",
    "title": "Engineering a Cup of Coffee",
    "section": "Final thoughts",
    "text": "Final thoughts\nI think this shows that making coffee can be an interesting exploration of how one would go about building a mass-transfer model for an extraction operation, and going through the stages of simplifying the model by, for example, assuming simpler geometries, limiting cases and such. I think you could also take this as an example of how very often chasing down appropriate model parameters is the limiting step when building an engineering model (at least in chemical engineering). Often the exact chemical process that you want to model has not been explored, experimentally, over the entire range of your process variables (if at all).\nThe next obvious step with this model is to build some datasets and fit some of these models to actual observed extractions. This could be a jumping off point for exploring how changes in different parameters impact the overall extraction or required brew time."
  },
  {
    "objectID": "posts/engineering_a_cup_of_coffee/index.html#references",
    "href": "posts/engineering_a_cup_of_coffee/index.html#references",
    "title": "Engineering a Cup of Coffee",
    "section": "References",
    "text": "References\n\n\nBatali, Mackenzie E., Lik Xian Lim, Jiexin Liang, Sara E. Yeager, Ashley N. Thompson, Juliet Han, William D. Ristenpart, and Jean-Xavier Guinard. “Sensory Analysis of Full Immersion Coffee: Cold Brew Is More Floral, and Less Bitter, Sour, and Rubbery Than Hot Brew.” Foods 11, no. 16 (2022): 2440. https://doi.org/10.3390/foods11162440.\n\n\nBatali, Mackenzie E., William D. Ristenpart, and Jean‑Xavier Guinard. “Brew Temperature, at Fixed Brew Strength and Extraction, Has Little Impact on the Sensory Profile of Drip Brew Coffee.” Scientific Reports 10 (2020): 16450. https://doi.org/10.1038/s41598-020-73341-4.\n\n\nBird, R. Byron, Warren E. Stewart, and Edwin N. Lightfoot. Transport Phenomena. 2nd ed. Hoboken, NJ: John Wiley & Sons, 2007.\n\n\nCarslaw, Horatio S., and John C. Jaeger. Conduction of Heat in Solids. 2nd ed. London: Oxford University Press, 1959.\n\n\nGreen, Don W., ed. Perry’s Chemical Engineers’ Handbook. 8th ed. New York: McGraw Hill, 2008.\n\n\nHottel, Hoyt C., James J. Noble, Adel F. Sarofim, Geoffrey D. Silcox, Phillip C. Wankat, and Kent S. Knaebel. “Heat and Mass Transfer.” In Perry’s Chemical Engineers’ Handbook, edited by Don W. Green, 8th ed. New York: McGraw Hill, 2008.\n\n\nMoroney, Kevin M., William T. Lee, Stephen B. G. O’Brien, Freek Suijver, and Johan Marra. “Modelling of Coffee Extraction During Brewing Using Multiscale Methods: An Experimentally Validated Model.” Chemical Engineering Science 137 (2015): 216–34. https://doi.org/10.1016/j.ces.2015.06.003.\n\n\nPoling, Bruce E., John M. Prausnitz, and John P. O’Connell. The Properties of Gases and Liquids. 5th ed. New York: McGraw Hill, 2001.\n\n\nPoling, Bruce E., George H. Thomson, Daniel G. Friend, Richard L. Rowley, and W. Vincent Wilding. “Physical and Chemical Data.” In Perry’s Chemical Engineers’ Handbook, edited by Don W. Green, 8th ed. New York: McGraw Hill, 2008.\n\n\nRodrigues, Melissa A. A., Maria Lúcia A. Borges, Adriana S. Franca, Leandro S. Oliveira, and Paulo C. Corrêa. “Evaluation of Physical Properties of Coffee During Roasting.” Agricultural Engineering International: The CIGR Journal of Scientific Research and Development V (2003). https://www.researchgate.net/publication/267858074.\n\n\nRousseau, Ronald W., ed. Handbook of Seperation Process Technology. Hoboken, NJ: John Wiley & Sons, 1987.\n\n\nSchwartzberg, Henry G. “Leaching – Organic Materials.” In Handbook of Seperation Process Technology, edited by Ronald W. Rousseau. Hoboken, NJ: John Wiley & Sons, 1987.\n\n\nSeader, J. D., Ernest J. Henley, and D. Keith Roper. Separation Process Principles. 3rd ed. Hoboken, NJ: John Wiley & Sons, 2011."
  },
  {
    "objectID": "posts/building_infiltration_example/index.html",
    "href": "posts/building_infiltration_example/index.html",
    "title": "Building Infiltration Example",
    "section": "",
    "text": "A common part of emergency planning is the shelter in place. More often than not trying to outrun whatever calamity is happening at a chemical plant is more dangerous than finding a safe place to ride it out, which is usually the lunch room or somewhere like that. Sometimes facilities have different shelter in place locations depending on what the hazard is – for example a severe weather shelter location may not be where you go for a toxic gas release.\nNaturally, when evaluating the consequences of a release, one wants to evaluate the effectiveness of a shelter in place location. This usually involves looking at some building infiltration model.\nAnother situation in which building infiltration has come up in recent years is forest fires, and I think this presents a unique opportunity to validate the models and the selection of shelter in place locations. I’m not talking about sheltering in place because the forest fire is at the plant boundary, I’m talking about smoke days where forest fire smoke blows in and blankets the whole area in higher than usual airborne particulates."
  },
  {
    "objectID": "posts/building_infiltration_example/index.html#the-scenario",
    "href": "posts/building_infiltration_example/index.html#the-scenario",
    "title": "Building Infiltration Example",
    "section": "The Scenario",
    "text": "The Scenario\nAt least where I live, in Alberta, this is not an uncommon event. Every other year, it seems, there is a large forest fire either in the northern boreal forest or in the Rocky Mountains and the smoke from these enormous fires will blanket the entire province.\nThis is the view from my apartment of one of the last big smoke days, on May 30th of 2019\n\n\n\n\n\n\nFigure 1: The view from my apartment on May 30th, 2019, when the city experienced a significant air quality event due to wildfire smoke.\n\n\n\nThis haze is unsurprisingly bad for outdoor air quality, but what does it do for indoor air quality? Typically people shut-down air handling systems to minimize the smoke infiltration and wait it out (which is why I think it is a good proxy for a release scenario), so a model of how quickly smoke, or airborne particulates, can work its way into the building is useful to have."
  },
  {
    "objectID": "posts/building_infiltration_example/index.html#ambient-air-data",
    "href": "posts/building_infiltration_example/index.html#ambient-air-data",
    "title": "Building Infiltration Example",
    "section": "Ambient Air Data",
    "text": "Ambient Air Data\nThe outdoor concentration of airborne particulates, PM2.5, is needed. There are several air monitoring stations throughout Alberta, with the closest one to me being the Edmonton Central station. Hourly air quality data can be downloaded as a csv from Alberta’s Air Data Warehouse and imported into julia as a dataframe.\n\nusing CSV, DataFrames, Dates, Pipe\n\nI am using the Pipe.jl package to streamline the data manipulation process. It takes the output from what is on the left of |&gt; and puts it in where the _ is on the right, which is very convenient when chaining together several single-input-single-output functions.\n\ndata_file = \"data/Edmonton Central pm2.5.csv\"\n\n# import the csv file, remove missing data\n# insert a column for the time since the start of the dataset in hours\n\nambient_data = @pipe data_file |&gt;\n    CSV.File( _ ; \n             dateformat=\"mm/dd/yyyy HH:MM:SS\", \n             types=[DateTime, DateTime, Float64], \n             header=16) |&gt;\n    DataFrame(_) |&gt;\n    rename(_, \"MeasurementValue\" =&gt; \"conc\") |&gt;\n    rename(_, \"IntervalStart\" =&gt; \"date\") |&gt;\n    select(_, Not([2])) |&gt;\n    hcat(_, Dates.value.( _.date - _.date[1])/3.6e6) |&gt;\n    rename(_, \"x1\" =&gt; \"time\") |&gt;\n    dropmissing(_) \n\nfirst(ambient_data, 6)\n\n6 rows × 3 columnsdateconctimeDateTimeFloat64Float6412019-05-24T00:00:0016.00.022019-05-24T01:00:007.01.032019-05-24T02:00:009.02.042019-05-24T03:00:0013.03.052019-05-24T04:00:009.04.062019-05-24T05:00:0010.05.0\n\n\n\n\n\n\n\n\n\n\nFigure 2: Ambient pm2.5 concentrations from the Central Edmonton air quality monitoring station, May 24th - June 5th 2019.\n\n\n\n\n\nIn this case I restricted the dataset to just the PM2.5 concentration, since that is all I am interested in, and to a window of time around May 30th. The data clearly shows that May 30th had a big spike in airborne particulates, well in excess of the ambient air quality objectives. Though it was somewhat hazy in the days before and after too."
  },
  {
    "objectID": "posts/building_infiltration_example/index.html#infiltration-models",
    "href": "posts/building_infiltration_example/index.html#infiltration-models",
    "title": "Building Infiltration Example",
    "section": "Infiltration Models",
    "text": "Infiltration Models\nBuilding infiltration models can range from highly detailed CFD simulations of indoor airflow to simple “fully mixed” models that assume a single average indoor concentration. This second type is the easiest to use and a good start for screening scenarios. It is a simple differential equation that assumes the rate of infiltration is proportional to a ventilation rate, λ, and the concentration difference between the outside and inside air1\n1 Lees, Loss Prevention in the Process Industries, sec. 15.51.\\[ \\frac{d}{dt} c = f \\left( c, \\lambda, t \\right) = \\lambda \\cdot \\left( c_o(t) - c \\right)\\]\nThe outside concentration, \\(c_o\\), can be a constant, but it is more usefully thought of as a function of time. In practice the ventilation rate, λ, is usually taken to be a constant, but it is a function of ambient conditions and could be implicitly made a function of time as well.\nThis model is for for a single zone or single cell building, where the interior air is assumed to be well mixed and at a single uniform pressure and concentration. This works well for houses, non-segmented industrial buildings, and small open plan commercial buildings. For much larger buildings, with many zones, there are multiple zone models of various scales of complexity.\n\nusing Interpolations, OrdinaryDiffEq\n\nThe function below represents the right-hand-side of the differential equation in standard form, with the outside concentration as a generic function of time that is passed as a parameter. For convenience later on, I also created a function that takes the outside concentration and returns a callable with that pre-set.\nThe order of arguments is important here, OrdinaryDiffEq expects the arguments to be in the order unknowns, parameters, time, where both the unknowns and parameters can be vectors (if there’s more than one)\n\nf(c, λ, t; cₒ=zero) = λ*(cₒ(t) - c)\n\nf(g) = (c, λ, t) -&gt; f(c, λ, t; cₒ=g)\n\n\nNatural Ventilation\nAny structure, unless it is hermetically sealed, has some natural ventilation rate. This comes from leaks around doorframes, through ventilation systems (even when turned off), and other breaks in the building envelope. This natural ventilation rate, λ, is reported in air changes per hour (ACH) and is, in general, a function of ambient conditions inside and outside the structure.\n\n\n\n\n\n\nFigure 3: Natural ventillation mechanisms in a single zone building.\n\n\n\nThe following plot is for a building infiltration model showing a building with windows open versus closed, showing the functional relationship between temperature differences and windspeed and the ventillation rate. Note the first plot is against the square root of the temperature difference.\n\n\n\n\n\n\nFigure 4: Natural ventillation rates as a function of temperature difference and windspeed.\n\n\n\nASHRAE2 gives guidance on how to estimate the natural ventilation rate for single zone buildings, and a basic model of air leakage is\n2 2017 ASHRAE Handbook - Fundamentals (SI Edition), chap. 16.\\[ Q = A_L \\sqrt{ C_s \\vert \\Delta T \\vert + C_w u^2 }\\]\nWhere Q is the air leakage rate, in m³/s, \\(A_L\\) is the effective air leakage area, in m², \\(\\Delta T\\) is the temperature difference between indoors and outdoors, u the windspeed, and \\(C_s\\) and \\(C_w\\) constants tabulated based on building height and extent of shelter from the wind (due to other buildings in the vicinity).\nThe ventilation rate is then simply the leakage rate divided by the building volume\n\\[ \\lambda = {Q \\over V } \\]\nThis model effectively treats the leakage from the building like a leakage through a hole using Toricelli’s law, and the one big unknown that needs to be determined is the effective air leakage area. This can be determined by working through the different parts of the building and counting the elements such as windows and vents, or it can be determined experimentally for a given building, either by using a tracer (SF₆ is common) or by using the a blower system to pressurize the building and measuring how much flow is required to raise the internal pressure by a fixed amount (such as by 50Pa).\nThis is a lot of work for a simple building infiltration screening. However this may have already been done for the design of the building, as the air leakage is a critical component in a building’s overall thermal efficiency. If the data already exists for a given building, for other purposes, then why not use it, but if it isn’t readily available then it is more practical to use tabulated values for representative buildings.\nThis can be tricky, though, as most tabulated values are for houses and how “leaky” a house is depends very strongly on where that house was constructed (and when). Many references, such as Lees’ give values for British homes which are often much greater than equivalent values tabulated elsewhere for typical American and Canadian homes. Which could entirely be a function of local weather. The air tightness of a home where I live in Canada is probably a lot more important, especially on days when it is -30°C, than a similar home in the UK where such extremes are essentially unheard-of. Which is also putting aside the fact that commercial structures are quite different from houses and so these values may not be too representative either.\nTypical values for Canadian homes in urban areas\n\n\n\nLevel\nACH\n\n\n\n\nTight\n0.25\n\n\nAverage\n0.50\n\n\nLeaky\n1.0\n\n\n\nTypical values for houses in the US\n\n\n\nConditions\nTight\nTypical\nLeaky\n\n\n\n\nMild\n0.07\n0.1\n0.4\n\n\nModerate\n0.2\n0.3\n1.0\n\n\nSevere\n0.3\n0.5\n1.6\n\n\n\nIn this model of building infiltration the ASHRAE model could be used and, with a suitable dataset of outdoor temperature and windspeeds, the ventilation rate would be a function of time. But for simplicity I am going to assume that the change in windspeed and temperature from any given hour to the next is quite small and the ventillation rate can be assumed to be a constant.\nThis raises the obvious question of what impact windspeed has on the indoor concentration? At higher windspeeds the building ventilation rate is higher, and so more of what’s outside ends up inside, however at higher windspeeds there is more mixing and the outdoor concentration will generally be lower. I would expect the effect of mixing would dominate, but this might be worth investigating further.\n\n\nBuilding Infiltration\nArmed with some ideas of the building ventilation rate, the differential equation can be solved for different outdoor concentration scenarios. When defining the problem, above, I set the outdoor concentration as generic function of time that was passed as a parameter. This ODE is simple enough that it can be solved by hand for basic cases and easily numerically integrated for any well behaved set of initial conditions and outdoor concentrations.\nFor an example let’s consider a sudden pulse of a pollutant, say the outside air is 100%(vol) during the pulse and 0 otherwise, a square wave.\n\nfunction c_square(t; cmax=1, t1=5, t2=20)\n    if (t &gt;=t1) & (t &lt;= t2)\n        return cmax\n    else\n        return 0\n    end\nend\n\nAssuming the initial indoor concentration to be zero, and with a suitable ventillation rate, this can be solved numerically.\nThe problem is defined using the ODEProblem function, which creates a problem object which the solver then solves. In this case using the Tsit5 solver, the default for non-stiff problems. The solution returned is an object that contains both a vector of solutions, and times, but can also be called like a function to return an interpolated result. This way the solution acts like a continuous function of time.\n\nλ₀ = 0.25 # ventilation rate, 0.25 h⁻¹\nc0 = 0.0  # initial condition\ntspan = (0.0, 25.0)\nsys = f(c_square)\n\nprb = ODEProblem(sys, c0, tspan, λ₀)\nsln = solve(prb, Tsit5())\n\n\n\n\n\n\n\n\n\nFigure 5: Building infiltration for a step-change in outdoor concentration.\n\n\n\n\n\nSolving for the indoor concentration of pm2.5s using measured outdoor concentrations is essentially the same process, except instead of a simple square wave we have a timeseries of measured values.\nThe first step is to turn that timeseries into a continuous function, in this case I am using a simple linear interpolation.\n\ncₒᵤₜ = LinearInterpolation(ambient_data.time, ambient_data.conc)\n\nThe remaining steps are the same, since the model for building infiltration is the same (with the parameter and inital conditions as defined earlier). The only difference is the timespan is the full span of the timeseries and the outdoor concentration is the linear interpolation defined above.\n\ntspan = (0.0, ambient_data.time[end])\nsys = f(cₒᵤₜ)\n\nprb = ODEProblem(sys, c0, tspan, λ₀)\nsln = solve(prb, Tsit5())\n\n\n\n\n\n\n\n\n\nFigure 6: Building infiltration using measured outdoor concentrations.\n\n\n\n\n\nWe can see, much like in the square wave model, that the indoor concentration lags behind the outoor concentration but still rises significantly. Once the pulse in high pm2.5 ends the indoor concentration decays, but again with a delay. So there is a period after the smoke has blown over in which the pm2.5 concentration indoors can be higher than outdoors (a good time to open some windows and air the place out)\nFor some context I have added Alberta’s Ambient Air Quality Objective for pm2.5s, clearly bad smoke days exceed that target but also indoor air quality can exceed it as well. Interestingly the indoor air quality may have exceeded workplace limits for airborne particulates through the whole period.\n\n# max outdoor concentration\n\nmaximum(ambient_data.conc)\n\n867.0\n\n\n\n# max indoor concentration\n\nmaximum(sln.u)\n\n501.877462902824\n\n\n\n\nVentilation and Infiltration Time\nIf we assume a constant outdoor concentration, a constant ventilation rate, and an initial indoor concentration of zero, the model can be solved analytically to give\n\\[ { c_i \\over c_o } = 1 - e^{-\\lambda t}\\]\nWhich leads us to ask, how long does it take for the indoor concentration to reach some fraction x of the outdoor concentration?\n\\[ x = 1 - e^{-\\lambda t} \\]\n\\[ t = { -\\ln{\\left( 1 - x \\right)} \\over \\lambda } \\]\nFor simplicity’s sake let’s assume \\(x = 0.5\\).\n\nt_x(λ; x=0.5) = -log(1-x)/λ\n\n\n\n\n\n\n\n\n\nFigure 7: The time to reach 1/2 the outdoor concentration as a function of ventillation rates.\n\n\n\n\n\nThis gives us a sense of how building tightness - the natural ventilation rate - impacts how long a shelter in place would be effective for. If the emergency is lasting for several hours then a shelter in place location would have to be highly air tight to be effective."
  },
  {
    "objectID": "posts/building_infiltration_example/index.html#model-evaluation",
    "href": "posts/building_infiltration_example/index.html#model-evaluation",
    "title": "Building Infiltration Example",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nA realistic shelter in place location is not going to be well mixed with the rest of the building. It will be an enclosed space that can be isolated from the building (e.g. by closing doors), and with an air handling system that can be isolated (unlike, say, a cafeteria where often the vents for clearing the air in the kitchen cannot be easily sealed). In this case the effective ventillation rate for that enclosed space, during a shelter in place, should be smaller than the ventillation rate for the building overall – when the doors are open and a single zone model is perhaps more representative.\nIn this case we can use the outdoor smoke event as a test. Somewhat like a tracer test but in reverse and we are not controlling the tracer. If we knew in advance that a smoke day was coming, which given publicly available modeling such as FireSmoke is reasonable, we could close off the shelter in place location with some indoor monitoring set up in the middle of the room and watch what happens.\nIf things go like they have in the past, at least where I work, the air handling system is shutdown and people try and minimize their time outdoors (and thus time spent opening and closing outside doors). By tracking the indoor concentration as well as outdoor concentrations we can compare the model to reality – does a single zone model work? do we need to incorporate weather conditions? – and estimate an effective ventilation rate by fitting the ODE to the measured data.\nAt least that’s the theory. I don’t have measured indoor air data for the time in question so I am going to simulate some by assuming the model works and adding some random noise. In this case I am adding ±10% random noise to the results calculated earlier for λ=0.25.\n\nusing DiffEqParamEstim, Optim\n\nHere I make a copy of the ambient data dataframe, df, and create a new column called cin for indoor concentration. This is the solution found earlier times a random error ±10%, then for good measure any columns with the concentration below zero are chopped off at zero since that is unphysical.\n\n# Dummy data\n\ndf = deepcopy(ambient_data)\n\ndf.cin = sln.(df.time) .* ( 1 .+ 0.10*randn(size(df.time)) )\ndf.cin[ df.cin .&lt; 0 ] .= 0\n\n\n\n\n\n\n\n\n\nFigure 8: Actual measured outdoor data and generated indoor data.\n\n\n\n\n\nThe model can be fit to these two sets of data, essentially taking the outdoor concentration as a given and finding the best fit curve to the indoor concentration by solving the ODE repeatedly for different values of the parameter λ.\nThe major difference between this and simply solving the ODE is defining the loss function, in this case an L2 loss which is analogous to least squares, and then optimizing.\n\ntspan = (0, df.time[end])\nc0 = df.cin[1] # initial condition\n\nsys = f(cₒᵤₜ)\np = [0.5] #initial guess of λ=0.5\n\nprb = ODEProblem(sys, c0[1], tspan, p)\nlossfn = L2Loss(df.time, df.cin)\n\ncost_function = build_loss_objective(prb,Tsit5(),lossfn,\n                                     maxiters=10000,verbose=false)\n\nThe optimization is looking for the parameter that minimizes the cost function, which in this case is the least-squares difference between the model and the “measured” data for indoor concentration. The minimum is well defined and close to λ=0.25, which is what we would expect given that was how the data was generated.\nNote: the plot below is nice to look at but not something one would normally generate, since calculating each point involves solving the ODE and could be fairly resource intensive for any problem more complex than this simple model. It sort of defeats the point of using an optimization algorithm to find the minimum. It’s just a nice visualization of what is happening in the background.\n\n\n\n\n\n\n\n\nFigure 9: The cost landscape showing the optimal parameter λ\n\n\n\n\n\n\n# optimize the cost function for parameters between 0 and 1\n\nresult = optimize(cost_function, 0.0, 1.0)\n\nResults of Optimization Algorithm\n * Algorithm: Brent's Method\n * Search Interval: [0.000000, 1.000000]\n * Minimizer: 2.363617e-01\n * Minimum: 9.520426e+03\n * Iterations: 27\n * Convergence: max(|x - x_upper|, |x - x_lower|) &lt;= 2*(1.5e-08*|x|+2.2e-16): true\n * Objective Function Calls: 28\n\n\n\nλfit = result.minimizer[1]\n\n0.2363617224674848\n\n\n\nλfit/λ₀\n\n0.9454468898699392\n\n\nThe best fit ventillation rate is quite close to the actual ventillation rate used to generate the data, which is what we would expect.\nWith an effective ventilation rate we can generate a best fit line\n\nprb = ODEProblem(sys, c0[1], tspan, λfit)\nfit = solve(prb, Tsit5())\n\n\n\n\n\n\n\n\n\nFigure 10: The linear ventillation model fitted to the indoor concentration."
  },
  {
    "objectID": "posts/building_infiltration_example/index.html#a-control-systems-approach",
    "href": "posts/building_infiltration_example/index.html#a-control-systems-approach",
    "title": "Building Infiltration Example",
    "section": "A Control Systems Approach",
    "text": "A Control Systems Approach\nFor people with a background in control systems and process dynamics, an obvious alternative way of writing the problem is in term of Laplace transforms and transfer functions.\n\\[ \\frac{d}{dt} c = \\lambda \\cdot \\left( c_o(t) - c \\right)\\]\nwith the change of variables \\(y = c(t) - c(0)\\) and \\(u = c_o\\)\n\\[ y^\\prime = \\lambda u - \\lambda y \\]\nTaking the Laplace transform of both sides \\[ s Y = \\lambda U - \\lambda Y \\]\n\\[ Y = { \\lambda \\over { s + \\lambda} } U \\]\nThis can then be solved analytically for various inputs, \\(U\\), or numerically for a given timeseries. In Julia this can be done with the ControlSystems.jl and ControlSystemIdentification.jl packages. This lets you define a system in terms of transfer functions and solve them that way.\nI showed the more generic ODE approach at the start because this is more easily generalized to more complex models (e.g. by incorporating the functional dependence of λ on temperature and windspeed). Though the transfer function approach lends itself more simply to using different inputs to the same system, simply change \\(u\\) and you get a different result, whereas the generic ODE has the input as part of the system definition, which I think is sort of messy (though maybe there’s a better way of doing this that I don’t know about?).\n\nBuilding Infiltration Model\nThe building infilration model is set up by simply defining the transfer function for the system and then simulating the response to the given input (the outdoor concentration in this case)\n\nusing ControlSystems, ControlSystemIdentification\n\n\nu(x, t) = [cₒᵤₜ(t)]\nt = 0:1:ambient_data.time[end]\n\nsys = tf([λ₀], [1, λ₀])\n\nTransferFunction{Continuous, ControlSystems.SisoRational{Float64}}\n   0.25\n-----------\n1.0s + 0.25\n\nContinuous-time transfer function model\n\n\nUnlike the previous package, this returns the output, y, and time, t, as vectors with no interpolation or other information.\n\ny, t, x = lsim(sys, u, t)\n\n\n\n\n\n\n\n\n\nFigure 11: The linear ventillation model, using ControlSystems.jl\n\n\n\n\n\n\nmaximum(y)\n\n508.2191947911374\n\n\nThe two approaches produce almost identical answers, which is not too surprising as the same ODE package and solver (Tsit5) is being used under the hood of ControlSystems.jl to solve this (I believe the only difference is one is using a variable time-step and the other a fixed time step, but I could be wrong).\n\n\nModel Evaluation\nFor the purposes of fitting the model to the data, we note that the model is an ARX model, i.e. it is of the form\n\\[ A(s) Y = B(s) U + D \\]\nwhere \\(A(s)\\) and \\(B(s)\\) are polynomials in s, and use the ControlSystemIdentification.jl package to fit the model, generating a fitted transfer function.\nIn this simple approach the parameters of \\(A(s)\\) and \\(B(s)\\) are allowed to be different, and in general you could fit a variety of models and use this as a jumping off point to explore potentially better models of infiltration.\nIf you are more interested in an empirical model, fitted to timeseries data, this approach can be much simpler than the model driven approach taken above with fitting the ODE to the data. Especially when incorporating other elements like windspeed and temperature.\n\n# generates the discrete timeseries data with a sample time of 1\n\nd = iddata(df.cin, df.conc, 1)\n\nInputOutput data of length 311 with 1 outputs and 1 inputs\n\n\n\n# finds the best fit transfer function with numerator order 1 and denominator order 1\n\nmodel_tf = arx(d, 1, 1)\n\nTransferFunction{Discrete{Int64}, ControlSystems.SisoRational{Float64}}\n  0.22305776166128505\n------------------------\n1.0z - 0.738488961457766\n\nSample Time: 1 (seconds)\nDiscrete-time transfer function model\n\n\n\n# generate the best fit line\n\ny_fit, t_fit, _ = lsim(model_tf, u, t)\n\n\n\n\n\n\n\n\n\nFigure 12: The linear ventillation model fitted to indoor concentrations using ControlSystemIdentification.jl"
  },
  {
    "objectID": "posts/building_infiltration_example/index.html#references",
    "href": "posts/building_infiltration_example/index.html#references",
    "title": "Building Infiltration Example",
    "section": "References",
    "text": "References\n\n\n2017 ASHRAE Handbook - Fundamentals (SI Edition). Atlanta, GA: American Society of Heating, Refrigerating; Air-Conditioning Engineers, 2017.\n\n\nLees, Frank P. Loss Prevention in the Process Industries. 2nd ed. Oxford: Butterworth-Heinemann, 1996."
  },
  {
    "objectID": "posts/indoor_air_quality/index.html",
    "href": "posts/indoor_air_quality/index.html",
    "title": "Monitoring smoke infiltration",
    "section": "",
    "text": "A few years ago I mused about using wildfire smoke events to measure the infiltration rate of buildings, in the context of modeling the infiltration of air pollution into buildings. Well it is wildfire season again and this past weekend saw a thick haze descend upon Edmonton, with airborne particulate concentrations, pm2.5 specifically, exceeding 440 μg/m3 in my neighbourhood.\nIn anticipation I had ordered an Atmotube PRO as a relatively cheap and portable solution – something I can also hang from my backpack for when I travel. Just due to poor timing on my part, it did not arrive until part-way through the day on Saturday, May 20th, and I couldn’t use it to capture the impact of the smoke arriving as it was well and truly already here. That did not stop me from setting up two experiments, one to measure the rate of infiltration and another to demonstrate (to myself really) the effectiveness of a remedy."
  },
  {
    "objectID": "posts/indoor_air_quality/index.html#measuring-building-infiltration",
    "href": "posts/indoor_air_quality/index.html#measuring-building-infiltration",
    "title": "Monitoring smoke infiltration",
    "section": "Measuring building infiltration",
    "text": "Measuring building infiltration\nOn Sunday, May 21st, I ran a very simple experiment to measure the ventilation rate in my bedroom (i.e. the rate of air infiltration). I live in an older apartment building with radiant heat, which makes my bedroom somewhat perfect: It has no vents or other connections to adjacent rooms, heat comes only from the radiators which are turned off (it’s summer). My bedroom has an older aluminum frame window, of the sort common with other apartments of the same vintage in my neighbourhood.\nThe set-up was quite simple: I ran a portable HEPA filter in my bedroom, with the door closed, to maintain the particulate concentrations at a low level (more on that later) until 2:15pm. At that point I turned off the filter, left the room, and blocked off the gaps beneath the door with a wet towel. I left the atmotube sitting in the middle of the room, passively collecting at 15 minute intervals. A little over 10 hours later I returned and turned the HEPA filter back on, ending the experiment. I waited until Sunday to run the experiment as I had plans that day and knew I would be out of the apartment and thus not be tempted to go in the room for several hours.\nA house on the same block as my apartment building has a purple air outdoor air quality monitor mounted in their yard and the data is available at a 10-minute frequency through the purple air real-time air quality map. Using this and the data from the atmotube, I should be able to fit a simple building infiltration model.\n\nOutdoor particulate concentration\nThe purple air monitor can output the raw pm2.5 concentrations as a csv, which is easily imported into julia. As a first step I define when the experiment started such that I can also calculate how much time has elapsed – it is going to be easier to work with a time variable that is just a number starting at 0 when the experiment started than datetime objects. The default units of time, in julia, are milliseconds however the more convenient units for building ventilation are hours and so the time variable here is in hours.\n\nusing CSV, DataFrames, Dates, Pipe\n\n\nstart = DateTime(2023,5,21,14,15)\n\n2023-05-21T14:15:00\n\n\nThe purple air monitor has dual particle count sensors, labeled in this dataset as “Purple Air A” and “Purple Air B”, for convenience I take the average of the two as the outdoor concentration.\n\nusing Statistics: mean\n\noutdoor = @pipe \"data/22_May_2023_raw-pm25-gm.csv\" |&gt;\n                CSV.read( _ , DataFrame, dateformat=\"yyyy-mm-dd HH:MM:SS\") |&gt;\n                transform( _ , AsTable([\"Purple Air A\", \"Purple Air B\"]) =&gt; ByRow(mean) =&gt; :pm25) |&gt;\n                transform( _ , :DateTime =&gt; ByRow((x) -&gt; Dates.value(x - start)/(3600*1000)) =&gt; :time);\n\n\n\nIndoor particulate concentration\nThe atmotube outputs a whole bunch of stuff in one csv, including temperature, barometric pressure, VOCs, pm1, pm2.5 and pm10, I am only interested in the pm2.5s. That said, the csv has one serious issue: it implements a zero-order hold on data. There are pm2.5 values for every minute however the pm2.5 values are not sampled every minute, the atmotube holds the last value for all the minutes in between measurements. This is a problem as I am fitting a model to this data and I need the actual data at the times it was taken.\n\nraw_indoor = @pipe \"data/C22B42153089_22_May_2023_00_43_32.csv\" |&gt;\n                   CSV.read( _ , DataFrame; dateformat=\"yyyy-mm-dd HH:MM:SS\") |&gt;\n                   sort!( _ , :Date);\n\nTo retrieve only the actual measured data, and not the filled in rows, I create a new dataframe and walk through the raw data keeping a data point if it differs from the previous one or if more than 15 minutes have elapsed. Rows where the concentration value has not changed, and it has been less than 15 minutes from the last update, are assumed to be filled in rows and not “real”.\n\nlast_good_data = raw_indoor[!, \"PM2.5, ug/m3\"][1]\nlast_good_datetime = raw_indoor[!, :Date][1]\n\nindoor = DataFrame(datetime = DateTime[], meas = Float64[], time = Float64[])\n\nfor r in eachrow(raw_indoor)\n    dt = r[:Date]\n    meas = r[\"PM2.5, ug/m3\"]\n    time = Dates.value(dt - start)/(3600*1000)\n    \n    if meas != last_good_data\n        last_good_data = meas\n        last_good_datetime = dt\n        push!(indoor, [dt, meas, time])\n    elseif Dates.value(dt - last_good_datetime) &gt; 15*60*1000 # more than 15 minutes\n        last_good_data = meas\n        last_good_datetime = dt\n        push!(indoor, [dt, meas, time])\n    else\n        continue\n    end\nend\n\nPlotting the data looks encouraging (as far as fitting a model goes, not encouraging if one wanted to spend time in there breathing) as the particulates appear to be infiltrating with a rate proportional to the difference between the concentrations – the standard building infiltration model.\n\n\n\n\n\n\n\n\nFigure 1: Time series data for indoor and outdoor pm2.5 concentrations with the 1 hour AAQO indicated.\n\n\n\n\n\n\n\nFitting the model\nThe type of fit I am doing is quite simple: I am fitting a differential equation to the indoor concentration while taking the outdoor concentration as a parameter of the model. Thus I need the outdoor concentration as a continuous function of time and, for simplicity, I am using a linear interpolation of the measured outdoor concentration.\n\nusing Interpolations: linear_interpolation, Flat\n\ncₒ = linear_interpolation(outdoor.time, outdoor.pm25, extrapolation_bc=Flat());\n\nTo start, I define the differential equation that I am going to be fitting to the measured indoor concentration. This is the simple linear model for building infiltration\n\\[ {d \\over dt} c = \\lambda \\left( c\\_o - c \\right) \\]\nWhere c is the indoor concentration, co the outdoor concentration, and λ is the ventilation rate in units of h-1.\n\nusing OrdinaryDiffEq\n\n# the model\nf(c, λ, t) = λ*(cₒ(t) - c)\n\n# initial condition\nc0 = indoor.meas[1]\n\n# timespan\ntspan = (0, indoor.time[end])\n\n# parameters\np= [0.5] #initial guess of λ=0.5\n\nprb = ODEProblem(f, c0, tspan, p)\n\nNow I define the fit itself: with the cost function as the L2 loss between the measured indoor concentration and the predicted indoor concentration.\n\nusing DiffEqParamEstim: build_loss_objective, L2Loss\n\nlossfn = L2Loss(indoor.time, indoor.meas)\n\ncost_function = build_loss_objective(prb,Tsit5(),lossfn,\n                                     maxiters=10000,verbose=false);\n\nThen using the Optim package to find the parameter λ which minimizes the cost function.\n\nusing Optim: optimize\n\nresult = optimize(cost_function, 0.0, 1.0)\n\nResults of Optimization Algorithm\n * Algorithm: Brent's Method\n * Search Interval: [0.000000, 1.000000]\n * Minimizer: 7.848374e-02\n * Minimum: 1.517807e+03\n * Iterations: 35\n * Convergence: max(|x - x_upper|, |x - x_lower|) &lt;= 2*(1.5e-08*|x|+2.2e-16): true\n * Objective Function Calls: 36\n\n\nI can then retrieve the ventilation rate for my bedroom\n\nλfit = result.minimizer[1]\n\n0.07848373551388874\n\n\n\nprb = ODEProblem(f, c0, tspan, λfit)\nfit = solve(prb, Tsit5());\n\n\n\n\n\n\n\n\n\nFigure 2: Best fit curve for the simple linear building infiltration model.\n\n\n\n\n\nI think this simple linear model works relatively well, all things considered. A more fulsome model would have treated the ventilation rate as a function of air pressure, windspeed, and the difference between indoor and outdoor temperatures.\nThere are also a few weaknesses in the experimental design, beyond the quality of the sensors. For one I didn’t seal my door perfectly, and so there was some exchange with the rest of my apartment which had a much lower particulate concentration. I am also assuming that there is no deposition or adhesion of particulates when passing through the small leaks around my window. It’s possible that some particulates are being lost along the way, which would impact this. The placement of sensors could also be an issue, especially the outdoor ones: I live in a neighbourhood full of large apartment buildings and that creates complex wind patterns, I also live several stories up whereas the purple air monitor is at ground level. A better location would have been on my balcony, adjacent to the bedroom window.\nBut I think as a first pass, and especially for screening potential shelter in place locations, something as simple as this could work and the time investment is very minimal. It’s major weakness is that the key variable, the outdoor concentration, is not controlled and this whole exercise is dependent upon the whims of wildfire smoke and on the individuals responsiveness to smoke forecasts."
  },
  {
    "objectID": "posts/indoor_air_quality/index.html#using-a-hepa-filter",
    "href": "posts/indoor_air_quality/index.html#using-a-hepa-filter",
    "title": "Monitoring smoke infiltration",
    "section": "Using a HEPA Filter",
    "text": "Using a HEPA Filter\nAfter all that time measuring how rapidly the particulates infiltrated a room, what is to be done? The air quality in Edmonton has been poor for several days on end. Without any sort of mitigation my bedroom would be well above the limits and would be unhealthly to be in and yet that’s where I sleep. The solution is either installing furnace air filters with a high MERV rating or, in places like mine that lack central air, using a portable fan with a HEPA filter. I picked up a portable air filter from IKEA and similar ones are available from many places, and can be made by hand. Unfortunately, dangerously high levels of pm2.5s are invisible and generally undetectable to one’s senses, so without some sort of monitoring one is left merely trusting that the system is doing what it is supposed to be doing.\nI tested my IKEA unit on Saturday night in a similar manner to the building infiltration test: I turned off the unit, closed my bedroom door, and left the space to accumulate particulates for several hours. Then, before I went to bed, I went in and turned it on. Throughout this the atmotube was located in the middle of the room collecting data. From the plot below it is clear that the air filter works: the indoor particulate concentration drops rapidly and stays at a low level throughout the night, even as the outdoor concentration rises to very high levels.\n\nindoor_hepa = @pipe \"data/C22B42153089_21_May_2023_10_05_00.csv\" |&gt;\n                    CSV.read( _ , DataFrame; dateformat=\"yyyy-mm-dd HH:MM:SS\");\n\noutdoor_hepa = @pipe \"data/21_May_2023_raw-pm25-gm.csv\" |&gt;\n                     CSV.read( _ , DataFrame, dateformat=\"yyyy-mm-dd HH:MM:SS\") |&gt;\n                     transform( _ , AsTable([\"Purple Air A\", \"Purple Air B\"]) =&gt; ByRow(mean) =&gt; :pm25);\n\n\n\n\n\n\n\n\n\nFigure 3: Response of measured indoor particulate concentrations to air filtration. Note that the outdoor fine particulate concentration remains high throughout the measurement period."
  },
  {
    "objectID": "posts/indoor_air_quality/index.html#final-thoughts",
    "href": "posts/indoor_air_quality/index.html#final-thoughts",
    "title": "Monitoring smoke infiltration",
    "section": "Final thoughts",
    "text": "Final thoughts\nUsing wildfire smoke to measure the ventilation rates of different buildings, or rooms in buildings, is certainly a niche activity. I don’t imagine there are many places where it is important to screen for safe shelter-in-place locations that also experience significant wildfire smoke events regularly. That does describe the petrochemical industry in Alberta, wildfire smoke is a regular occurrence now, and perhaps locations along the west coast, but it’s not universal.\nThat said I wonder if this might be a more broadly useful activity when planning for how to manage indoor air quality beyond industry. The office building I work in struggles with indoor air quality during smoke events like this one whereas my home office does not because I have invested in HEPA filters and simple air monitoring. Perhaps schools, offices, and other places could use similar techniques to screen spaces for interventions. Rooms with high ventilation rates could benefit from interventions such as better sealing around windows. Perhaps the plastic sheeting used to seal drafty windows in the wintertime could find a second use during wildfire season. In this way the air filters themselves are being used more effectively: an air filter can manage a larger space if that space has a low ventilation rate.\nCurrently a lot of the advice is merely to stay indoors, with little acknowledgment that indoors is often severely polluted as well."
  },
  {
    "objectID": "posts/butane_leak_example/index.html",
    "href": "posts/butane_leak_example/index.html",
    "title": "Chemical Release Screening Example - Butane leak",
    "section": "",
    "text": "A routine practice of process safety is to model scenarios for different chemical hazards present at a plant. Often there are more plausible scenarios than there is the time or resources to model at the highest level of fidelity, the more complex models take time to set up and run and often there are only so many software licenses available. There needs to be some prioritization and screening. It’s fairly typical, especially for larger companies, to have screening tools that an engineer can use which incorporate simpler models and make conservative estimates to get a first guess at the impact of a given hazard, if this crosses a preset threshold then it is escalated to a more in depth level of modelling, drilling down to more and more detailed analysis as required.\nMore often than not I’ve seen these simple tools implemented as excel spreadsheets – which is fine, they do the job and everybody has excel on their computers – however overly involved spreadsheets can be rather opaque, it’s often not obvious what they are doing and what assumptions are being made in those calculations. So I am going to work through an example of how one could estimate the airborne quantity, and ultimately the consequences of, an example release of butane from a large storage sphere, while documenting the assumptions and models along the way."
  },
  {
    "objectID": "posts/butane_leak_example/index.html#the-scenario",
    "href": "posts/butane_leak_example/index.html#the-scenario",
    "title": "Chemical Release Screening Example - Butane leak",
    "section": "The Scenario",
    "text": "The Scenario\nAs a simple scenario suppose a leak from a butane storage sphere. These are a fairly common sight around refineries and facilities that process large quantities of hydrocarbons. This sphere is 40ft in diameter and operates under 250psig of pressure, containing primarily n-butane, which I will assume is entirely n-butane for simplicity1. As for the leak itself I am supposing a leak area equivalent to a 2in rupture2. The sphere doesn’t sit directly on the ground, it is supported 10ft above a concrete pad which has a diked area of 500ft². The leak itself at the bottom somewhere, suppose exactly at the bottom for simplicity3. Furthermore I am assuming the release occurs on a day with an ambient temperature of 25°C and that the tank contents and surroundings are at thermal equilibrium.\n1 If the vessel contained a mixture, for the purposes of screening, conservatively choosing the most volatile of the major components would be a reasonable assumption. These simplifications are suitable for screening purposes however if more in depth modeling is required then performing mixture flash calculations would have to be considered, which very quickly becomes a lot of work to set-up outside of a process simulator like Aspen2 There are lots of ways of generating leak scenarios, from the very specific leaks from particular propagating events to simple rules of thumb. The Chemical Exposure Index gives the following rules for determining a leak scenario for a vessel:\nA rupture based on the largest diameter process pipe attached to the vessel using the following:\n\nFor anything less than 2in a full bore rupture (i.e. the full diameter of the pipe)\nFor between 2 and 4in assume a rupture area equal to that of a 2in diameter pipe\nFor &gt;4in assume a rupture area equal to 20% of the pipe cross section area\n\n3 Picking the bottom also ensures the leak occurs at the highest pressure, which gives a larger release and is most conservative. Releases at higher elevations also tend to mix more thoroughly with the air and present less of a hazard to personnel on the ground, and possibly less of an explosion hazard depending on where one supposes the ignition sources are.Key Assumptions\n\nStorage sphere with 40ft diameter\nSphere located on a concrete pad with 500ft² diked area\nSphere contains ~100% n-butane\nLeak area equivalent to a 2in rupture\nLeak located at the bottom of the vessel for maximum release pressure\nVessel pressure is 250psig\nRelease temperature is 25°C\n\n\n\n\n\n\n\nFigure 1: A sketch of the release scenario, adapted from … somewhere\n\n\n\n\nusing Unitful: ustrip, @u_str\n\nft = ustrip(u\"m\", 1u\"ft\")     # unit conversion ft-&gt;m\ninch = ustrip(u\"m\", 1u\"inch\") # unit conversion inch-&gt;m\npsi = ustrip(u\"Pa\", 1u\"psi\")  # unit conversion psi-&gt;Pa\n\nDᵥ = 40ft    # Diameter of the vessel, in m\nAd = 500ft^2 # Dyked area, in m^2\ndₕ = 2inch   # Diameter of the hole, in m\nhₗ = 50ft    # height of liquid in the vessel\nhᵣ = 10ft    # height of release point\n\npₐ= 14.7psi     # atmospheric pressure in Pa absolute\np = 250psi + pₐ # pressure of the butane in Pa absolute\nTᵣ= 25 + 273.15; # the release temperature in K\n\nSome relevant thermodynamic properties of butane\n\n# From Perry's, 8th edition\n\nR = 8.31446261815324 # universal gas constant, J/mol/K\n\n# Air\nMWₐᵢᵣ = 28.960\nρa(T) = (pₐ*MWₐᵢᵣ)/(R*T)/1000\nμₐ(T) = (1.425e-6*T^0.5039)/(1 + 108.3/T)\n\n\n# Butane\nMw = 58.122        # molar mass of butane, kg/kmol\nTcr = 425.12       # critical temperature, K\nTb = -0.6 + 273.15 # the normal boiling point of butane, K\n\n# vapour pressure in Pa, T in K\npˢ(T) = exp(66.343 - (4363.2/T) - 7.046*log(T) + 9.4509e-6*T^2)\n\n# density in kg/m^3, T in K\nρₗ(T) = Mw*( 1.0677/0.27188^(1+ (1-T/425.12)^0.28688) )\n\n# heat capacity in J/kmol/K, T in K\ncₚ(T) = 191030 - 1675*T + 12.5*T^2 - 0.03874*T^3 + 4.6121e-5*T^4\n\n# latent heat in J/kmol, T in K\nΔHᵥ(T) = 3.6238e7*(1-(T/Tcr))^(0.8337 - 0.82274*(T/Tcr) + 0.39613*(T/Tcr)^2)\n\n# surface tension, N/m\nσ(T) = 0.05196*(1-(T/Tcr))^(1.2181);\n\nThe vapour pressure of butane at the release temperature is below the storage pressure, so the butane in the storage sphere will be a liquid.\n\npˢ(Tᵣ)&lt;p\n\ntrue"
  },
  {
    "objectID": "posts/butane_leak_example/index.html#the-release-rate",
    "href": "posts/butane_leak_example/index.html#the-release-rate",
    "title": "Chemical Release Screening Example - Butane leak",
    "section": "The Release Rate",
    "text": "The Release Rate\nSince the vapour pressure within the vessel is below the storage pressure, at ambient temperature, the butane within the storage sphere is a liquid. In general one would have to account for flashing and two-phase flow during the release, however for very short discharge distances (&lt;10cm) there is typically not enough time for the liquid to flash during discharge,4 over the thickness of a hole this especially true. The butane discharged from the tank will be a stream of liquid initially and the simple Bernoulli equation for a liquid jet can be used.5\n4 See AIChE/CCPS, Guidelines for Use of Vapour Cloud Dispersion Models, 2nd Ed., 37 for more of a disussion on two-phase discharge rates.5 This is also known as Toricelli’s equation and can be derived from a mechanical energy balance and is found in a lot of references (e.g. Perry’s), the form of it I’m using here comes from AIChE/CCPS, 29 equation 4-10. This is really a function of time as the liquid height \\(h_l\\) will decrease as it leaks out. Using the discharge rate at the start of the leak throughout the analysis is a conservative assumption, again for the purposes of a simplified screening case. For more detailed modeling one could make this explicitly a function of time and integrate over the release.\\[ Q_l = c_d \\rho_l A_h \\sqrt{ 2 \\left( p - p_a \\over \\rho_l \\right) + 2gh_l } = c_d \\rho_l { {\\pi \\over 4} d_h^2} \\sqrt{ 2 \\left( p - p_a \\over \\rho_l \\right) + 2gh_l } \\]\nWhere \\(Q_l\\) is the mass flow of liquid discharged through the hole (in kg/s), \\(c_d\\) is the discharge coefficient which can be assumed to be 0.61,6 \\(g\\) is the acceleration due to gravity \\(9.81 m/s^2\\) and the rest are as defined earlier. I am assuming, here, that the hole is circular for simplicity.\n6 From AIChE/CCPS, Guidelines for Consequence Analysis of Chemical Releases., 27, for sharp edged orifices and Reynolds numbers greater than 30,000 the discharge coefficient approaches 0.61, and the exit velocity is independent of the hole size. For a simple screening calculation one could also use a coefficient of 1.0, though that may be excessively conservative (large over-estimates end up wasting time modeling later).Key Assumptions\n\nLiquid release\nSharp edged hole with discharge coefficient of 0.61\n\n\ncd = 0.61\ng  = 9.81 # m/s^2\nQₗ = cd*ρₗ(Tᵣ)*(π/4)*(dₕ^2)*√( 2*(p - pₐ)/ρₗ(Tᵣ) + 2*g*hₗ )\n\n56.31092763613714"
  },
  {
    "objectID": "posts/butane_leak_example/index.html#flashing-fraction",
    "href": "posts/butane_leak_example/index.html#flashing-fraction",
    "title": "Chemical Release Screening Example - Butane leak",
    "section": "Flashing Fraction",
    "text": "Flashing Fraction\nSince the butane is significantly above it’s normal boiling point, as the liquid stream exits the storage sphere it will flash. However not all of it will flash into a vapour as the quantity that can vaporize is limited by the available energy. A simplified model of flashing is to assume the process is so rapid that it is effectively adiabatic and, from a simple steady-state energy balance, one arrives at the following7\n7 This can be easily derived, but the form given here is from AIChE/CCPS, Guidelines for Use of Vapour Cloud Dispersion Models, 2nd Ed., 31, equation 4-14.\\[f_v = {Q_v \\over Q_l} = { {c_p (T_r - T_b)} \\over {\\Delta H_v} }\\]\nwhere \\(f_v\\) is the mass fraction that flashes and \\(Q_v\\) is the mass flow of liquid that flashes (in kg/s) and recall that the heat capacity and latent heat are functions of temperature.\nKey Assumptions\n\nflashing occurs rapidly and is effectively adiabatic\nheat capacity and latent heat taken at the release temperature\n\n\nfᵥ = cₚ(Tᵣ)*(Tᵣ-Tb)/ΔHᵥ(Tᵣ) \n\n0.17128269541302374\n\n\n\nQᵥ(t) = fᵥ*Qₗ\n\nQᵥ (generic function with 1 method)"
  },
  {
    "objectID": "posts/butane_leak_example/index.html#aerosol-fraction",
    "href": "posts/butane_leak_example/index.html#aerosol-fraction",
    "title": "Chemical Release Screening Example - Butane leak",
    "section": "Aerosol Fraction",
    "text": "Aerosol Fraction\nAs the butane flashes into a gas, some of the liquid stream will be entrained as an aerosol. The presence of aerosolized droplets are a major contributor to the overall mass of a vapour cloud and it is important to include them. There is a wide array of methods for estimating the aerosolized fraction, from as simple as assuming it is 1-2x the flashed fraction to more detailed models that take into account the different mechanisms behind aerosolization and rain-out.\nThe aerosol fraction, \\(f_a\\), the fraction of the liquid remaining in the cloud after flashing, in the form of aerosolized droplets.\n\\[ f_a = {Q_a \\over { Q_l - Q_v } }\\]\nOne method is to estimate the droplet size and from that determine the degree of rain out (i.e. the liquid that does not remain in the cloud) through a model of droplet settling. I am going to use the RELEASE model8 of droplet settling to determine the aerosol fraction.\n8 Johnson and Woodward, RELEASE - a Model with Data to Predict Aerosol Rainout in Accidental Releases.Key Assumptions\n\nrain-out is the only significant mechanism by which liquid drops out to form a pool\nall droplets larger than a critical size drop out, all droplets below that size remain in the cloud\nevaporation is negligible\nthe RELEASE model is used to estimate the degree of rain-out\n\n\nMean droplet diameter\nThere are three important mechanisms of drop formation, capillary breakup which occurs when sub-cooled liquids are discharged through very small holes (&lt;2mm), aerodynamic breakup which occurs with larger holes with sub-cooled liquids or slightly super-heated liquids, and flashing breakup which occurs as super-heated liquids are discharged and flash to vapour in the form of bubbles which breakup the surrounding liquid.\nAerodynamic breakup is correlated with the Weber number, which is the ratio of shear forces on the surface of the liquid to the surface tension.\n\\[ We = { { \\rho_g u_d^2 d_p } \\over \\sigma } \\]\nWhere \\(\\rho_g\\) is the density of the gas, \\(u_d\\) the discharge velocity, \\(d_p\\) the mean droplet diameter, and \\(\\sigma\\) the surface tension. Experimentally, droplet breakup occurs at a critical Weber number between 12 and 22, and so the mean droplet size can be estimated by rearranging9\n9 Woodward, Estimating the Flammable Mass of a Vapour Cloud, 49.\\[ d_p = { { \\sigma We_c } \\over {\\rho_g u_d^2 } } \\]\nand solving at the critical Weber number \\(We_c = 12\\), assuming \\(\\rho_g = \\rho_a\\) to be the density of ambient air, and with the discharge velocity \\(u_d\\) given as:\n\\[ u_d = { Q_l \\over { c_d A_h \\rho_l} } = { Q_l \\over {c_d \\frac{\\pi}{4} d_h^2 \\rho_l} }\\]\n\n# the cloud temperature, assumed to be the boiling point\nTc = Tb\n\n# critical Weber number\nWe = 12                                  \n\n# release velocity, m/s\nud = Qₗ/( cd * (π/4)*dₕ^2 * ρₗ(Tᵣ)) \n\n# droplet size, m, due to aerodynamic breakup\nda = ( σ(Tc) * We)/(ρa(Tc) * ud^2)  \n\n2.188550597862162e-5\n\n\nThe diameter of droplets from flashing breakup can be calculated from the following empirical correlation10 and the mean droplet diameter is simply the smallest of either the aerodynamic or flashing diameter11 In almost all cases that are relevant for release modelling capillary breakup is not significant.\n10 Woodward, 50.11 Johnson and Woodward, RELEASE - a Model with Data to Predict Aerosol Rainout in Accidental Releases, 64.\\[ d_p = { {0.03} \\over {10 + 4.0 \\cdot (T - T_b) } }\\]\n\n\n\n\n\n\nFigure 2: A correlation for droplet size due to flashing breakup.\n\n\n\n\n# droplet size, m, due to flashing breakup\ndf = (0.03)/(10 + 4*(Tᵣ-Tb))                   \n\ndₚ = min(da, df)\n\n2.188550597862162e-5\n\n\n\n\nThe RELEASE model\nThe RELEASE model uses a distribution of droplet sizes to determine, based on a simple model of settling dynamics, the fraction of droplets that remain the cloud. The model assumes droplet diameter follows a log-normal distribution and that any droplet greater than a critical diameter, determined from a balance of drag and buoyancy, will rain out. The parameters of the model were fit to experimental data of rain out events.\n\nCritical diameter\nThe critical diameter is a function of a critical velocity which is calculated from a model of the spray jet with a tuning parameter \\(\\beta\\) which captures the expansion of the jet. The default value for \\(\\beta\\) is given to be 4.46°12\n12 Johnson and Woodward, 63.\\[ u_c = u_d \\tan \\beta \\]\n\n# the default value given by RELEASE is 4.46°\nβ = deg2rad(4.46)\nuc = ud * tan(β) # critical velocity, m/s\n\n6.197367132394693\n\n\nThe critical diameter is found by solving the balance of buoyant and drag forces on a droplet\n\\[ F_{buoyant} = F_{drag} \\]\n\\[ \\left( \\rho_l - \\rho_g \\right) g V_{droplet} = \\frac{1}{2} C_D \\rho_g u_c^2 A_{droplet} \\]\n\\[ \\left( \\rho_l - \\rho_g \\right) g \\cdot \\frac{\\pi}{6} d_c^3 = \\frac{1}{2} C_D \\rho_g u_c^2 \\cdot \\frac{\\pi}{4} d_c^2 \\]\n\\[ \\left( \\rho_l - \\rho_g \\right) g \\cdot d_c - \\frac{3}{4} C_D \\rho_g u_c^2 = 0\\]\nWhere \\(C_D\\) is the drag coefficient, which for a solid sphere in viscous flow is given by this correlation13\n13 White, Viscous Fluid Flow. This could be an opportunity for improvement to the RELEASE model as liquid droplets and bubbles do not experience drag in the same way as solids, due to internal flows that can dissipate energy.\\[ C_D = 0.4 + {24 \\over Re} + {6 \\over {1 - \\sqrt{Re} } } \\]\nwith the Reynolds number \\(Re\\) as\n\\[ Re = { {\\rho_g u_c d_c} \\over \\mu_a} \\]\nfor simplicity the gas density \\(\\rho_g\\) can be calculated assuming an ideal gas, and \\(\\mu_a\\) is the viscosity of air.\nThis relationship will have to be solved numerically to get the critical diameter, since the Reynolds number and thus drag coefficient is a function of the critical diameter. Which is fairly straight forward and in this case I use the bounds \\(0.1 \\cdot d_p \\le d_c \\le 10 \\cdot d_p\\) as a very broad starting point.\n\nusing Roots: find_zero\n\nρg(T) = (pₐ * Mw)/(R * T)/1000  # ideal gas law, kg/m^3\n\n# the Reynold's number at the release temperature\nRe(d) = ρg(Tc) * uc * d / μₐ(Tc)\n\n# the drag coefficient\nCD(d) = 0.4 + (24/Re(d)) + 6/(1-√(Re(d))) \n\n# critical diameter\ndc = find_zero( d -&gt;   (ρₗ(Tc) - ρg(Tc))*g*d - 0.75*CD(d)*ρg(Tc) * uc^2, (0.1*dₚ, 10*dₚ))\n\n0.00014250630981793824\n\n\n\n\nAerosol Fraction\nThe aerosol fraction, in the RELEASE model, is the mass fraction of droplets with a diameter less than the critical diameter:\n\\[ f_a = { {F_m \\left( d_c \\right)} \\over {F_m \\left( \\infty \\right)} } \\]\nWhere \\(F_m(d)\\) is the cumulative mass distribution function for droplets. This is based on a log-normal distribution and is given as\n\\[ F_m \\left( d \\right) = \\left( \\frac{\\pi}{6} \\rho_l d_p^3 \\right) \\int_0^t { t^2 \\over {\\sqrt{2 \\pi} \\log{\\sigma_G} } } {\\exp \\left( -\\frac{1}{2} \\left( \\log{t} \\over \\log{\\sigma_G} \\right)^2 \\right)} dt\\]\nWhere \\(t = d/d_p\\) and \\(\\sigma_G\\) is another tuning parameter (the default value given is 1.8).\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: The distribution of droplet sizes and rain-out region.\n\n\n\n\nWith a change of variables \\(z = \\log{t}\\) and \\(s = \\log{\\sigma_g}\\) the cumulative mass distribution function can be integrated:\n\\[ F_m \\left( d \\right) = \\left( \\frac{\\pi}{6} \\rho_l d_p^3 \\right) \\int_{-\\infty}^z { 1 \\over {\\sqrt{2 \\pi} s} }{\\exp \\left( 3z \\right) \\exp \\left(-\\frac{1}{2} \\left( z \\over s \\right)^2 \\right)} dz \\]\n\\[ = \\left( \\frac{\\pi}{6} \\rho_l d_p^3 \\right) \\frac{-1}{2} \\exp \\left( 9s^2 \\over 2 \\right) \\left[ \\mathrm{erf} \\left( {3s^2 - z} \\over {\\sqrt{2} s} \\right) \\right]_{-\\infty}^z\\]\n\\[= \\left( \\frac{\\pi}{6} \\rho_l d_p^3 \\right) \\exp \\left( 9 \\left( \\log{\\sigma_G} \\right)^2 \\over 2 \\right) \\times \\frac{1}{2} \\left[ 1 - \\mathrm{erf} \\left( {3 \\left( \\log{\\sigma_G} \\right)^2 - \\log{d} + \\log{d_p} } \\over {\\sqrt{2} \\log{\\sigma_G} } \\right) \\right] \\]\nwhere \\(\\mathrm{erf}\\left( x \\right)\\) is the error function. Finally the aerosol fraction is:14\n14 Johnson and Woodward, RELEASE - a Model with Data to Predict Aerosol Rainout in Accidental Releases, 58–60. The integration is not shown in the text, but the fortran code is included on the CD if one wants to verify the final result.\\[ f_a = { {F_m \\left( d_c \\right)} \\over {F_m \\left( \\infty \\right)} } = \\frac{1}{2} \\left[ 1 - \\mathrm{erf} \\left( {3 \\left( \\log{\\sigma_G} \\right)^2 - \\log{d_c} + \\log{d_p} } \\over {\\sqrt{2} \\log{\\sigma_G} } \\right) \\right]\\]\nThe RELEASE code uses this formula and also does a check for extreme cases, defaulting to either 1 or 0.\n\nusing SpecialFunctions: erf\n\nfunction RELEASE_fa(dc, dp; σG=1.8)\n    if (dp/dc) &gt;= exp(σG)\n        # checks for where erf(x) ~ 1\n        return 0.0\n    elseif (dc/dp) &gt;= 15*exp(σG)\n        # checks for where erf(x) ~ -1\n        return 1.0\n    else\n        return 0.5*( 1 - erf((3*log(σG)^2 -log(dc) + log(dp)) / (√(2) * log(σG)) ))\n    end\nend\n\nRELEASE_fa (generic function with 1 method)\n\n\n\nfₐ = RELEASE_fa(dc, dₚ)  # calculates the aerosol fraction using the RELEASE method\n\n0.9227949810754577\n\n\n\nQₐ(t) = fₐ*(Qₗ - Qᵥ(t));"
  },
  {
    "objectID": "posts/butane_leak_example/index.html#pool-evaporation",
    "href": "posts/butane_leak_example/index.html#pool-evaporation",
    "title": "Chemical Release Screening Example - Butane leak",
    "section": "Pool Evaporation",
    "text": "Pool Evaporation\nThe droplets that rain out of the cloud will form a pool and, depending on how long the release occurs, evaporation from the pool can be a significant contributor to the overall airborne quantity. In this case the liquid is assumed to be at the boiling point of butane, it cooled through evaporative cooling, and is cryogenic with respect to the ground. There are two major factors that impact the evaporation rate: the area of the pool and the heat transfer into the pool from the environment. Both of these, in general, can be quite complicated time-dependent phenomena with lots of different models capturing a wide array of scenarios.\nSince this is a simple screening calculation, I will be avoiding all of that and use some simple models for pool spread and evaporative flux.\nKey Assumptions\n\nSimple model of pool spread\nEvaporation of pool is driven by heat transferred from the ground by conduction\n\nA simple model of pool spread as a function of time is15\n15 Woodward, Estimating the Flammable Mass of a Vapour Cloud, 57.\\[ A_{pu}  = \\frac{\\pi}{4} \\sqrt{\\frac{2048}{81} {Q_{p} \\over \\rho_l} t^3} \\]\nWhere \\(A_{pu}\\) is the unconstrained pool area in m², \\(t\\) is the time since the start of the release in seconds, and \\(Q_{p}\\) is the mass flow of liquid to the pool in kg/s (the total release rate less what was lost to flashing and entrained in the cloud as an aerosol)\n\\[ Q_{p} = Q_l - Q_v - Q_a \\]\nIn practice the area of the pool will be limited to be at most the diked area. For large spills having a diked area is significant, both in the obvious containing of the spill, but also since it can significantly reduce the amount of pool evaporation.\n\n# the liquid temperature, taken to be the boiling point\nTₗ = Tb\n\n# mass flow to the pool, kg/s\nQₚ(t) = Qₗ - Qᵥ(t) - Qₐ(t)\n\n# unconstrained pool area, m^2\nAₚᵤ(t) = (π/4) * √((2048/81) * (Qₚ(t)/ρₗ(Tₗ)) * t^3 )\n\n# Pool area, restricted to at most dyked area, m^2\nAₚ(t) = min( Aₚᵤ(t) , Ad);\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Pool spreading as a function of time for constrained and unconstrained releases.\n\n\n\n\nIn general the evaporation rate is derived from a heat balance accounting for the heat transfer from the ground, from the ambient air, and from solar flux, however in this case a simplifying assumption is that the majority of the heat transferred to the liquid is from the ground.\nFor a cryogenic liquid spilled on land a simple model of the evaporative flux \\(G_e\\) in kg/s/m² is16\n16 AIChE/CCPS, Guidelines for Consequence Analysis of Chemical Releases., 63.\\[ G_e = { {Mw} \\over {\\Delta H_v} } { k \\left( T_s - T_l \\right) \\over \\sqrt{\\pi \\alpha t} } \\]\nWhere \\(k\\) is the thermal conductivity of the surface (ground) in W/m/K, \\(T_s\\) is the temperature of the surface in K, \\(T_l\\) the temperature of the liquid in K, and \\(\\alpha\\) the thermal diffusivity of the surface in m²/s\nThe overall evaporation rate is the product of the pool area and the evaporative flux\n\\[ Q_e \\left( t \\right) = G_e \\left( t \\right) \\cdot A_p \\left( t \\right) \\]\nIt’s worth taking a moment to note that the evaporative flux will decrease with time. This is because the ground under the spill cools down over time. The overall evaporation rate will grow as the pool grows – in this model the pool grows \\(\\propto t^{3/2}\\) while the flux decreases \\(\\propto t^{-1/2}\\), so the evaporation rate should grow \\(\\propto t\\) – but once it hits the limit of the diked area the overall evaporation rate will decrease over time.\nOne thing worth noting is that the pool area equation does not take into account a mass balance. As time goes on the unconstrained pool only grows, even if the evaporation rate were to exceed the rate of new liquid being added to the pool. This limitation probably doesn’t matter for short duration leaks in which it is expected that the pool evaporation rate is strictly lower than the rate at which new liquid is added to the pool, however for long duration spills or instantaneous spills this not appropriate and a more complex model of pool growth and evaporation should be considered.\n\n# Thermal properties of concrete \n# A. Bejan, Kraus, A. D., Heat Transfer Handbook, John Wiley & Sons, 2003\nk = 1.28     # W/m/K\nα = 6.6e-7   # m^2/s\n\n# surface temperature, taken to be the ambient temperature\nTₛ = Tᵣ      \n\n# evaporative flux, kg/s/m^2\nGₑ(t) = (Mw/ΔHᵥ(Tₗ)) * k * (Tₛ - Tₗ) / √(π*α*t)\n\n# evaporation rate, kg/s\nQₑ(t) = min( Gₑ(t)*Aₚ(t), Qₚ(t));\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: The pool evaporation rate as a function of time for a constrained release."
  },
  {
    "objectID": "posts/butane_leak_example/index.html#airborne-quantity",
    "href": "posts/butane_leak_example/index.html#airborne-quantity",
    "title": "Chemical Release Screening Example - Butane leak",
    "section": "Airborne Quantity",
    "text": "Airborne Quantity\nThe total airborne quantity is the sum of the flashed vapour, the aerosolized droplets, and the vapour from pool evaporation. So far the calculation has been in terms of rates, but the total airborne quantity depends upon the release duration, \\(t_d\\). There are lots of different ways of deciding on a release duration, in general the release duration of interest is the time it would take for a vapour cloud to find an ignition source – for vapour cloud explosion scenarios – or, more optimistically, the time it would take for the plant to respond and take some action to mitigate the hazard. A common default release duration is 10 minutes[#ccps-1999 page 22]\nOne common simplification is to take the vapour and aerosol rates to be a constant, and the pool evaporation rate as a constant at the final time \\(t_d\\), then multiply by the total duration. An alternative is to integrate over time from 0 to \\(t_d\\).\n\\[ Q_{aq} \\left( t \\right) = Q_v \\left( t \\right) + Q_a \\left( t \\right) + Q_e \\left( t \\right)\\]\n\\[ m_{aq} = \\int_0^{t_d} Q_{aq} \\left( t \\right) dt = \\int_0^{t_d} Q_v \\left( t \\right) + Q_a \\left( t \\right) + Q_e \\left( t \\right) dt \\]\nOne could try to integrate this analytically, but for re-useability of code it’s a better idea to integrate numerically – then different models for each of the rates can be swapped in and out with ease.\n\nusing QuadGK: quadgk\n\n# release duration of 10 minutes, seconds\ntd = 10*60 \n\n# total airborne release rate is the sum of the individual \n# mechanism release rates, in kg/s\nQaq(t) = Qᵥ(t) + Qₐ(t) + Qₑ(t) \n\n# total airborne quantity is the integral over time\nmaq, err = quadgk(Qaq, 0, td)  \n\n(31737.218210630548, 0.00014433872246399915)\n\n\nA quick sanity check is to make sure that the total airborne quantity is less than the total quantity released, i.e. \\(Q_l \\cdot t_d\\).\n\nmaq &lt;= Qₗ*td\n\ntrue\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: The total airborne quantity as a function of time.\n\n\n\n\nIt is often insightful to compare the airborne quantity to the case where there was no secondary containment, i.e. the pool could expand without bound.\n\nQₑᵤ(t) = min( Gₑ(t)*Aₚᵤ(t), Qₚ(t))\nQaqu(t) = Qᵥ(t) + Qₐ(t) + Qₑᵤ(t)\n\nmaqu, err = quadgk(Qaqu, 0, td)\n\n(33426.49125139247, 0.0003172098610093599)\n\n\n\n\nWith secondary containment    31.737 t \nWithout secondary containment 33.426 t \n\n\nIn this case the secondary containment reduced the overall airborne quantity by ~5%, and we wouldn’t expect it to be hugely important for this example as most of the mass of the vapour cloud came from the flashing of the liquid immediately upon release and from entrained droplets."
  },
  {
    "objectID": "posts/butane_leak_example/index.html#closing-remarks",
    "href": "posts/butane_leak_example/index.html#closing-remarks",
    "title": "Chemical Release Screening Example - Butane leak",
    "section": "Closing Remarks",
    "text": "Closing Remarks\nThere are always trade offs to be made with model accuracy, model complexity, and the shear amount of data required to run the models (often overlooked unless you’re the flunky tasked with finding all of these constants). This notebook aimed at creating a simple screening model and made several simplifications along the way. One thing I tried to avoid, though, is the use of gross “rules of thumb” and any pre-calculating of constants. I see this fairly often in older works because, likely, the calculations were being done by hand and this greatly speeds that up. I don’t think the justification for it is still valid, though, for a few reasons. For one many very rough rules of thumb were developed to avoid iterative solutions, but with modern computers there’s really no reason to, the numerical solutions in this notebook took fractions of a second to calculate on my laptop. For another much of the work collecting constants and pre-calculating things simply makes it harder to validate formulas. With a notebook like this not only can I properly typeset the formula more-or-less as presented in the reference (while keeping a consistent nomenclature) but I can also fairly transparently type that into Julia, making it very clear what the code is doing, step by step, and where those formulae came from. This should make it very easy to verify that I haven’t made a typo, for example. In my experience with some older excel tools that used a lot of pre-calculated rearranged equations, it was often entirely not obvious how the reference (if there is one given at all) lead to the final equations in the spreadsheet and verifying that the spreadsheet worked as intended without some written down derivation could take hours.\nOne feature that I didn’t use, but could be a nice addition, is the unit-aware library Unitful, I included it at the beginning for some unit conversions. However it can be used to track the units for each number throughout, ensuring that results are in the appropriate units and that there are no unit mismatches. I did not use that part of things because there are quite a few correlations and figuring out the appropriate units for the various constants in those correlations such that all the units match properly can be a pain in the butt. In general, though, using Unitful is a very powerful tool when working with physical modelling.\nThat said, this notebook is far from perfect. Probably the biggest simplification that could be changed is the assumption that the liquid release rate is a constant and is at the highest rate. This could be made a function of time – as the vessel empties there is less hydro-static pressure – and the rates of flashing and aerosol entrainment made explicitly time dependent as well. Like all things this would take some time to implement – though not much – and would improve model performance for long duration releases. The assumption made is on the conservative side and for short duration, and large vessels, is appropriate for screening purposes. Though, like all things with code, you only have to put the effort in once…\nFor re-useability the lowest hanging fruit for changes would be to link this to a database of substance properties. Probably the most tedious part of using this notebook is finding and filling in all of the correlations at the beginning for the various temperature dependent material properties. There’s no reason why a small database couldn’t be set up, containing everything in a given plant’s inventory, and some code added so the notebook can look up the properties for you.\nThere are also lots of opportunities for embedding some of the decision logic into the notebook, I set up the notebook to do a liquid discharge because I knew what the scenario was. Furthermore I knew that the boiling point of butane is less than ambient and so the pool evaporation would be for a cryogenic liquid spill. There’s no reason why the logic behind those decisions, and others, couldn’t be generalized and the notebook setup to choose which model was appropriate in a clear and transparent way."
  },
  {
    "objectID": "posts/butane_leak_example/index.html#references",
    "href": "posts/butane_leak_example/index.html#references",
    "title": "Chemical Release Screening Example - Butane leak",
    "section": "References",
    "text": "References\n\n\nAIChE/CCPS. Dow’s Chemical Exposure Index Guide. New York: American Institute of Chemical Engineers, 1998.\n\n\n———. Guidelines for Consequence Analysis of Chemical Releases. New York: American Institute of Chemical Engineers, 1999.\n\n\n———. Guidelines for Use of Vapour Cloud Dispersion Models, 2nd Ed. New York: American Institute of Chemical Engineers, 1996.\n\n\nJohnson, David W., and John L. Woodward. RELEASE - a Model with Data to Predict Aerosol Rainout in Accidental Releases. New York: American Institute of Chemical Engineers, 1999.\n\n\nWhite, F. M. Viscous Fluid Flow. New York: McGraw-Hill, 1974.\n\n\nWoodward, John L. Estimating the Flammable Mass of a Vapour Cloud. New York: American Institute of Chemical Engineers, 1998."
  },
  {
    "objectID": "posts/dynamic_mode_decomposition/index.html",
    "href": "posts/dynamic_mode_decomposition/index.html",
    "title": "Dynamic Mode Decomposition",
    "section": "",
    "text": "Recently I’ve been playing around with Dynamic Mode Decomposition (DMD) and this notebook compiles my notes and julia code in one place for later reference.\nVery generally DMD is an approach to system identification problems that is well suited for high dimensional data and systems with coherent spatio-temporal structures. In particular DMD finds the “best fit” linear approximation to the dynamical system, i.e. it finds the matrix A such that\n\\[ \\mathbf{\\dot{x} } = \\mathbf{A x} \\]\nWhere x is the high dimensional state vector for the system. One key strength of DMD is that it allows one to calculate x(t) without explicitly calculating A. This may not seem like a particularly useful property on its face unless one notes that the matrix A is n×n and, for systems with a very large n (i.e. very high dimensionality) that can be huge. Context for huge is also important: a matrix that fits easily in memory on my laptop may be infeasibly huge for an embedded system. For control applications, such as MPC, DMD may be a good method for generating approximations that are both good and space efficient."
  },
  {
    "objectID": "posts/dynamic_mode_decomposition/index.html#example-flow-past-a-cylinder",
    "href": "posts/dynamic_mode_decomposition/index.html#example-flow-past-a-cylinder",
    "title": "Dynamic Mode Decomposition",
    "section": "Example: Flow Past a Cylinder",
    "text": "Example: Flow Past a Cylinder\nAs a motivating example, I am going to use the flow past a cylinder dataset from Data-Driven Science and Engineering, specifically the matlab dataset. This dataset is the simulated vorticity for fluid flow past a cylinder. The vector x in this case is the vorticity at every point in the discretized flow field at a particular time; a two dimensional array of 89,351 pixels reshaped into a column vector. The data is a sequence of equally spaced snapshots of the flow field, and ultimately we wish to generate a linear system that best approximates this.\nThe MAT package allows us to import data from matlab data files directly into julia\n\nusing MAT\n\nfile = matopen(\"data/CYLINDER_ALL.mat\")\n\n# import the data set\ndata = read(file, \"VORTALL\");\n\n# the orinal dimensions of each snapshot\nnx = Int(read(file, \"nx\"))\nny = Int(read(file, \"ny\"))\n\n# the final dimensions of the data matrix\nn, m = size(data)\n\n(89351, 151)\n\n\nThe data set, data, has already been processed into the form we need: each column represents a “frame” of the animation. We can walk through the matrix, taking each column and re-shaping it back into a 2D array, and recover the original flow as a movie.\n\n\n\n\n\n\nFigure 1: Original data, vorticity of flow past a cylinder.\n\n\n\nThe data set has the property that the number of data points at each time step, n, is much greater than the number of time steps, m. In fact n is large enough that the n×n matrix A might be unwieldy to store: If we assume it is a dense matrix of 64-bit floats, 8 bytes each, we would need ~64GB of memory just to store it.\n\nsize_A_naive = n*n*8\n\n63868809608"
  },
  {
    "objectID": "posts/dynamic_mode_decomposition/index.html#exact-dmd",
    "href": "posts/dynamic_mode_decomposition/index.html#exact-dmd",
    "title": "Dynamic Mode Decomposition",
    "section": "Exact DMD",
    "text": "Exact DMD\nDMD provides us a method to both find a best fit approximation for A while also being more space (and computation) efficient. To get there we first need to define what a best fit means.\n\nBest Fit Matrix\nConsider the general linear system Y = AX, where Y is a n × m matrix of outputs, X is a n × m matrix of inputs and A is an n × n linear transformation matrix. We say that the best fit matrix A is the matrix that minimizes\n\\[ \\| \\mathbf{ A X } - \\mathbf{Y} \\|_{F} \\]\nwhere \\(\\| \\cdots \\|_{F}\\) is the Frobenius norm.\nThe solution to which is\n\\[ \\mathbf{A} = \\mathbf{YX}^{\\dagger} \\]\nwhere X† is the Moore-Penrose pseudoinverse of X.1\n1 I think this can be shown fairly easily by starting with the definition of the Frobenius norm \\(\\| \\mathbf{ A X } - \\mathbf{Y} \\|_{F}^{2} = \\mathrm{Tr}\\left( \\left(\\mathbf{ A X } - \\mathbf{Y}\\right)\\left(\\mathbf{ A X } - \\mathbf{Y} \\right)^{T} \\right)\\) and finding the matrix A that minimizes that using standard matrix calculus, and some properties of the pseudoinverse.\n\nSingular Value Decomposition\nThe conventional way of calculating the Moore-Penrose pseudoinverse is to use the Singular Value Decomposition: for a matrix X with SVD \\(\\mathbf{X}=\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{*}\\), the pseudoinverse is \\(\\mathbf{X}^{\\dagger} = \\mathbf{V} \\mathbf{\\Sigma}^{-1} \\mathbf{U}^{*}\\). Returning to the best fit matrix A we find\n\\[ \\mathbf{A} = \\mathbf{Y} \\mathbf{V} \\mathbf{\\Sigma}^{-1} \\mathbf{U}^{*} \\]\nWe can calculate a projection of A onto the space of the upper singular vectors U\n\\[ \\tilde{ \\mathbf{A} } = \\mathbf{U}^{*} \\mathbf{A} \\mathbf{U} = \\mathbf{U}^{*} \\mathbf{Y} \\mathbf{V} \\mathbf{\\Sigma}^{-1} \\mathbf{U}^{*} \\mathbf{U} = \\mathbf{U}^{*} \\mathbf{Y} \\mathbf{V} \\mathbf{\\Sigma}^{-1} \\]\nWhich then allows us to reconstruct the matrix A on demand while only needing to store the matrices Ã and U, by the following \\[ \\mathbf{A} = \\mathbf{U} \\tilde{ \\mathbf{A} } \\mathbf{U}^{*} \\]\nThis is useful when n &gt; &gt; m as U is n×m and Ã is m×m. For this example this has reduced the memory requirement to ~108MB, a &gt;99.8% reduction\n\nsize_A_exact = (n*m + m*m)*8\n\n108118416\n\n\n\nsize_A_exact/size_A_naive\n\n0.0016928202774340957\n\n\nReturning to the original problem, we have a sequence of discrete snapshots arranged in a matrix such that each column, k, is the vector xk. Our aim, then, is to find the best fit matrix A for the linear system\n\\[ \\mathbf{x}_{k+1} = \\mathbf{A} \\mathbf{x}_k \\]\nfor all xk in our data set. Or in other words, to find the best fit matrix A for the system\n\\[ \\mathbf{X}_{2} = \\mathbf{A} \\mathbf{X}_{1} \\]\nwhere X1 is the matrix of all of the vectors xk and X2 is the matrix of the corresponding xk+1’s.\nThough, using DMD, we will instead calculate Ã and U, leaving us with\n\\[ \\mathbf{x}_{k+1} = \\mathbf{U} \\mathbf{ \\tilde{A} } \\mathbf{U}^{*} \\mathbf{x}_k \\]\nTo start, we divide the data set into X1 and X2\n\nusing LinearAlgebra\n\n\n# dividing into past and future states\nX₁ = data[:, 1:end-1];\nX₂ = data[:, 2:end];\n\nThen compute the SVD of X1.2\n2 The svd function in julia returns the singular values in a Vector, but for later on it will be more convenient have this as a Diagonal matrix.\n# SVD\nU, Σ, V = svd(X₁)\nΣ = Diagonal(Σ);\n\nThen calculate the projection Ã (I am pre-computing YVΣ-1 as that will come in handy later)\n\n# projection\nYVΣ⁻¹ = X₂*V*Σ^-1\nÃ = U'*YVΣ⁻¹\n\nsize(Ã)\n\n(150, 150)\n\n\nWe can then calculate the predicted xk+1’s, without ever having to actually compute (or store) A\n\nX̂₂_exact = (U*(Ã*(U'*X₁)));\n\nAs before, we can step through the matrix, extract each frame of the 2D flow field, and animate them, giving us a general sense of how well this worked\n\n\n\n\n\n\nFigure 2: Original flow field (top) and reconstructed flow field (bottom).\n\n\n\n\n\nDynamic Modes\nOf course this only solves the problem in the discrete case (for control applications that may be all you need). Consider again the system \\(\\mathbf{\\dot{x} } = \\mathbf{A x}\\), the solution to this differential equation is\n\\[ \\mathbf{x}\\left( t \\right) = e^{\\mathbf{A}t} \\mathbf{x}_{0} \\]\nwhere x0 is the initial conditions. If the matrix A has eigendecomposition ΦΛΦ-1 then this can be written as\n\\[ \\mathbf{x}\\left( t \\right) = \\mathbf{\\Phi} e^{\\mathbf{\\Lambda}t} \\mathbf{\\Phi}^{-1} \\mathbf{x}_{0} \\]\nSo it would be very convenient if we could get those eigenvalues and eigenvectors, preferably without having to actually compute A.\nRecall, by definition, the projection matrix Ã is unitarily similar to A, which means the eigenvalues are identical. The eigenvectors of A can also be recovered from properties of Ã: Suppose Ã has the eigendecomposition WΛW-1\n\\[ \\mathbf{ \\tilde{A} } \\mathbf{W} = \\mathbf{W} \\mathbf{\\Lambda} \\]\n\\[ \\mathbf{U}^{*} \\mathbf{A} \\mathbf{U} \\mathbf{W} = \\mathbf{W} \\mathbf{\\Lambda} \\]\n\\[ \\mathbf{U} \\mathbf{U}^{*} \\mathbf{A} \\mathbf{U} \\mathbf{W} = \\mathbf{U} \\mathbf{W} \\mathbf{\\Lambda} \\]\n\\[ \\mathbf{A} \\mathbf{\\Phi} = \\mathbf{\\Phi} \\mathbf{\\Lambda} \\]\nwhere\n\\[ \\mathbf{\\Phi} = \\mathbf{U} \\mathbf{W} \\]\nThis is what is given in the original DMD, however more recent work recommends using\n\\[ \\mathbf{\\Phi} = \\mathbf{Y} \\mathbf{V} \\mathbf{\\Sigma}^{-1} \\mathbf{W} \\]\n\n# calculate eigenvectors and eigenvalues\n# of projection Ã\nΛ, W = eigen(Ã)\n    \n# reconstruct eigenvectors of A\nΦ = YVΣ⁻¹*W;\n\nWhether or not the ultimate goal is to generate the continuous system, the eigenvectors and eigenvalues are useful to examine as they represent the dynamic modes of the system.\n\n\n\n\n\n\n\n\nFigure 3: The first and tenth dymanic mode of the system.\n\n\n\n\n\nI’ve played somewhat fast and loose with variables: the A for the discrete system is not the same A as the continuous system. Specifically the eigenvalues of the continuous system, ω are related to the eigenvalues of the discrete system, λ by the following\n\\[ \\omega_{i} = {\\log{ \\lambda_{i} } \\over \\Delta t} \\]\nwhere Δt is the time step. The eigenvectors are the same, though. So we can generate a function x(t) pretty easily:\n\n# calculate the eigenvalues for \n# the continuous system\nΔt = 1\nΩ  = Diagonal(log.(Λ)./Δt)\n\n# precomputing this\nΦ⁻¹x₀ = Φ\\X₁[:,1]\n\n# continuous system\nx̂(t) = real( Φ*exp(Ω .* t)*Φ⁻¹x₀ )\n\n\n\n\n\n\n\nFigure 4: Original flow field (top) and reconstructed flow field (bottom), using the continuous time vector function."
  },
  {
    "objectID": "posts/dynamic_mode_decomposition/index.html#refactoring",
    "href": "posts/dynamic_mode_decomposition/index.html#refactoring",
    "title": "Dynamic Mode Decomposition",
    "section": "Refactoring",
    "text": "Refactoring\nThrough taking the SVD, the eigenvalue decomposition, and projections, DMD involves generating a whole bunch of matrices, which can be really unwieldy to manage without some structure. The low hanging fruit for refactoring is to introduce a struct to store those matrices.\n\nstruct DMD\n    r::Integer  # Dimension\n    U::Matrix   # Upper Singular Vectors\n    Ã::Matrix   # Projection of A\n    Λ::Diagonal # Eigenvalues of A\n    Φ::Matrix   # Eigenvectors of A\nend\n\nThen we can introduce a method that takes an input matrix X and output matrix Y and returns the corresponding DMD object. We can take advantage of multiple dispatch to to add further methods, such as for the case where we have a single data matrix X and wish to calculate the DMD on the “future” and “past” matrices.\n\nfunction DMD(Y::Matrix, X::Matrix)\n    # dimension\n    r = rank(X)\n    \n    # Full SVD\n    U, Σ, V = svd(X)\n    Σ = Diagonal(Σ)\n    \n    # projection\n    YVΣ⁻¹ = Y*V*Σ^-1\n    Ã = U'*YVΣ⁻¹\n    \n    # calculate eigenvectors and eigenvalues\n    # of projection Ã\n    Λ, W = eigen(Ã)\n    Λ = Diagonal(Λ)\n    \n    # reconstruct eigenvectors of A\n    Φ = YVΣ⁻¹*W\n    \n    return DMD(r,U,Ã,Λ,Φ)\nend\n\nfunction DMD(X::Matrix)\n    X₁ = X[:, 1:end-1]\n    X₂ = X[:, 2:end]\n    return DMD(X₂, X₁)\nend\n\nWe can check that this is doing what it is supposed to be doing by comparing with what we have already done\n\nd = DMD(data)\n\n# This produces the same result as before\nd.Φ == Φ && d.Λ == Diagonal(Λ)\n\ntrue\n\n\nIf you were to build this into a larger project, it would be worthwhile to define some actual unit tests to validate that the DMD is working properly.\n\nDiscrete System\nSince we have a DMD type to work with, we can also refactor how discrete systems are generated. In this case I have defined a struct for the discrete system, and then added a method such that any discrete system acts as a callable xk+1=f(xk)\n\nstruct DiscreteSys\n    Ã::Matrix\n    U::Matrix\nend\n\nfunction DiscreteSys(d::DMD)\n    return DiscreteSys(d.Ã,d.U)\nend\n\nfunction (ds::DiscreteSys)(xₖ)\n    return (ds.U*(ds.Ã*(ds.U'*xₖ)))\nend\n\n\nds = DiscreteSys(d)\n\n# This produces the same result as before\nX̂₂_exact == ds(X₁)\n\ntrue\n\n\n\n\nContinuous System\nSimilarly we can refactor the generation of continuous systems, first by defining a struct for the continuous system, then by adding a method xt=f(t). This requires a little more information: we need to keep track of the initial state of the system x0 as well as the step size Δt\n\nstruct ContinuousSys\n    Φ⁻¹x₀::Vector\n    Ω::Diagonal\n    Φ::Matrix\nend\n\nfunction ContinuousSys(d::DMD, x₀, Δt=1)\n    Φ⁻¹x₀ = d.Φ\\x₀\n    Ω = Diagonal(log.(d.Λ.diag)./Δt)\n    return ContinuousSys(Φ⁻¹x₀, Ω, d.Φ)\nend\n\nfunction (cs::ContinuousSys)(t)\n    return real( cs.Φ*exp(cs.Ω .* t)*cs.Φ⁻¹x₀ )\nend\n\n\ncs = ContinuousSys(d, X₁[:,1]);\n\n# This produces the same result as before\nx̂(150) == cs(150)\n\ntrue\n\n\n\n\nLarge Systems\nI have been using the default tools in julia, which work well for small matrices. If you are planning on doing DMD on enormous matrices then it is worth investigating packages such as IterativeSolvers.jl, Arpack.jl, KrylovKit.jl and others to find better ways than vanilla svd and eigen. It also may be worth thinking about refactoring the problem to be matrix-free, though that is way beyond the scope of these notes."
  },
  {
    "objectID": "posts/dynamic_mode_decomposition/index.html#reduced-dmd",
    "href": "posts/dynamic_mode_decomposition/index.html#reduced-dmd",
    "title": "Dynamic Mode Decomposition",
    "section": "Reduced DMD",
    "text": "Reduced DMD\nWhenever a problem involves computing the SVD of a matrix, dimensionality reduction lurks about in the shadows, winking suggestively. By the Eckart-Young theorem we know that the best rank r approximation to a matrix X=UΣVT is the truncated SVD Xr=UrΣrVrT, i.e. the SVD truncated to the r largest singular values (and corresponding singular vectors). So an obvious step for dimensionality reduction in DMD is substitute a truncated SVD for the full SVD.\n\nfunction DMD(Y::Matrix, X::Matrix, r::Integer)   \n    # full SVD\n    U, Σ, V = svd(X)\n    \n    # truncating to rank r\n    @assert r ≤ rank(X)\n    U = U[:, 1:r]\n    Σ = Diagonal(Σ[1:r])\n    V = V[:, 1:r]\n    \n    # projection\n    YVΣ⁻¹ = Y*V*Σ^-1\n    Ã = U'*YVΣ⁻¹\n    \n    # calculate eigenvectors and eigenvalues\n    # of projection Ã\n    Λ, W = eigen(Ã)\n    Λ = Diagonal(Λ)\n    \n    # reconstruct eigenvectors of A\n    Φ = YVΣ⁻¹*W\n    \n    return DMD(r,U,Ã,Λ,Φ)\nend\n\nfunction DMD(X::Matrix, r::Integer)\n    X₁ = X[:, 1:end-1]\n    X₂ = X[:, 2:end]\n    return DMD(X₂, X₁, r)\nend\n\nOne consequence of truncation, however, is that the resulting matrix Ur is only semi-unitary, in particular\n\\[ \\mathbf{U}_{r}^{*} \\mathbf{U}_{r} = \\mathbf{I}_{r \\times r} \\]\nbut\n\\[ \\mathbf{U}_{r} \\mathbf{U}_{r}^{*} \\ne \\mathbf{I}_{n \\times n} \\]\nThis leads to a complication as the matrix U is required to be unitary, in particular when recovering A from the projection matrix Ã, and also when recovering the eigenvalues and eigenvectors of A from Ã.\nBut, supposing that this at least approximately works, we are still left with the problem of picking an appropriate value for r. One could look at the singular values and pick one based on structure. For this problem it looks like an elbow happens at r=45.\n\n\n\n\n\nThe singular values of the system showing a significant elbow at r=45\n\n\n\n\nWe can then generate a set of predictions for the reduced DMD, with r=45, and compare with the exact DMD\n\nds_45 = DiscreteSys(DMD(data, 45))\nX̂₂_45 = ds_45(X₁)\n\nnorm(X₂ - X̂₂_45) # Frobenius norm\n\n0.005459307491383062\n\n\n\nnorm(X₂ - X̂₂_exact)\n\n0.0005597047465277092\n\n\nAn alternative is to specify how much of the variance in the original data set needs to be captured. The singular values are a measure of the variance in the data, and so keeping the top p percent of the total variance equates to keeping the top p percent of the sum of all of the singular values.\nThat is to say we calculate the r such that\n\\[ { {\\sum_{i}^{r} \\sigma_i} \\over {\\sum_{i}^{m} \\sigma_i} } \\le p  \\]\nwhere σi is the ith singular value (in order of largest to smallest).\n\nfunction DMD(Y::Matrix, X::Matrix, p::AbstractFloat)\n    @assert p&gt;0 && p≤1\n    \n    # full SVD\n    U, Σ, V = svd(X)\n    \n    # determine required rank\n    r = minimum( findall( &gt;(p), cumsum(Σ)./sum(Σ)) )\n    \n    # truncate\n    @assert r ≤ rank(X)\n    U = U[:, 1:r]\n    Σ = Diagonal(Σ[1:r])\n    V = V[:, 1:r]\n    \n    # projection\n    YVΣ⁻¹ = Y*V*Σ^-1\n    Ã = U'*YVΣ⁻¹\n    \n    # calculate eigenvectors and eigenvalues\n    # of projection Ã\n    Λ, W = eigen(Ã)\n    Λ = Diagonal(Λ)\n    \n    # reconstruct eigenvectors of A\n    Φ = YVΣ⁻¹*W\n    \n    return DMD(r,U,Ã,Λ,Φ)\nend\n\nfunction DMD(X::Matrix, p::AbstractFloat)\n    X₁ = X[:, 1:end-1]\n    X₂ = X[:, 2:end]\n    return DMD(X₂, X₁, p)\nend\n\nCapturing 99% of the variance, in this case, requires only keeping the first 14 singular values.\n\n\n\n\n\n\n\n\nFigure 5: The Frobenius norm of the difference between the original and reconstructed flow field as a function of reduced DMD rank, the point where 99% of the variance has been captured is indicated.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Original flow field (top) and reconstructed flow field (bottom), using reduced DMD capturing 99% of the variance.\n\n\n\nThere are also methods for finding the optimal rank for truncated SVD for a data set that involves gaussian noise which I am not going to go into here.\nSo, supposing that p=0.99 works for us, how much further have we reduced the size of our matrices?\n\n# for p=0.99, r=14\nr = 14\nsize_A_reduced = (n*r + r*r)*8\n\n10008880\n\n\nTo recover the (approximate) A matrix we only need to store 10MB, a ~91% reduction over the exact DMD\n\nsize_A_reduced/size_A_exact\n\n0.09257331331972159\n\n\nand a &gt;99.98% reduction of the naive case (recall the naive approach of storing the entire A matrix would take ~64GB)\n\nsize_A_reduced/size_A_naive\n\n0.00015670998193688458\n\n\n\nTruncated SVD and Large Systems\nIn the above code I simply calculated the full SVD and then truncated it after the fact. If m (the rank of X) is particularly large, then this can be hilariously inefficient. In those cases it may be worth writing a method that uses TSVD.jl to efficiently calculate only the first r singular values – as opposed to calculating all m singular values and then chucking out most of them."
  },
  {
    "objectID": "posts/dynamic_mode_decomposition/index.html#compressed-dmd",
    "href": "posts/dynamic_mode_decomposition/index.html#compressed-dmd",
    "title": "Dynamic Mode Decomposition",
    "section": "Compressed DMD",
    "text": "Compressed DMD\nCompressed DMD attempts to tackle the slowest step in the DMD algorithm: calculating the SVD. An SVD on full data is \\(\\mathcal{O}\\left( n m^2 \\right)\\) if we instead compress the data from n dimensions to k dimensions then the cost of the SVD is reduced to either \\(\\mathcal{O}\\left( k m^2 \\right)\\) (when k&gt;m) or \\(\\mathcal{O}\\left( m k^2 \\right)\\) (when k &lt; m), which for large n can be a dramatic speed-up.\nSuppose we have some k×n unitary matrix C which compresses our input matrix X into the compressed input matrix Xc and our output matrix Y into the compressed output matrix Yc\n\\[ \\mathbf{X}_c = \\mathbf{C} \\mathbf{X} \\\\ \\mathbf{Y}_c = \\mathbf{C} \\mathbf{Y} \\]\nWe suppose again that X has the SVD X=**UΣV***, then\n\\[ \\mathbf{X}_c = \\mathbf{C} \\mathbf{X} = \\mathbf{C} \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^{*} \\]\nand, since C is unitary, the SVD of Xc is\n\\[ \\mathbf{X}_c = \\mathbf{U}_c \\mathbf{\\Sigma} \\mathbf{V}^{*} \\]\nwhere Uc=CU is the upper singular values of the compressed input matrix.\nThe projection matrix Ãc of the compressed input matrix is\n\\[ \\mathbf{ \\tilde{A} }_c = \\mathbf{U}^{*}_c \\mathbf{Y}_c \\mathbf{V} \\mathbf{\\Sigma}^{-1} \\]\n\\[ = \\left( \\mathbf{CU} \\right)^{*} \\mathbf{C} \\mathbf{Y} \\mathbf{V} \\mathbf{\\Sigma}^{-1} \\]\n\\[ = \\mathbf{U}^{*} \\mathbf{C}^{*} \\mathbf{C} \\mathbf{Y} \\mathbf{V} \\mathbf{\\Sigma}^{-1} \\]\n\\[ = \\mathbf{U}^{*} \\mathbf{C}^{*} \\mathbf{C} \\mathbf{Y} \\mathbf{V} \\mathbf{\\Sigma}^{-1} \\]\n\\[ = \\mathbf{U}^{*} \\mathbf{Y} \\mathbf{V} \\mathbf{\\Sigma}^{-1} = \\mathbf{ \\tilde{A} } \\]\nand so we should recover the same eigenvalues and eigenvectors as from the uncompressed data.\n\nusing SparseArrays\n\nfunction cDMD(Y::Matrix, X::Matrix, C::AbstractSparseMatrix)   \n    # determining dimensionality\n    r = rank(X)\n       \n    # compress the X and Y\n    Xc = C*X\n    Yc = C*Y\n    \n    # singular value decomposition\n    Uc, Σc, Vc = svd(Xc)\n    Σc = Diagonal(Σc)\n       \n    # projection\n    Ã = Uc'*Yc*Vc*inv(Σc)\n    U = C'*Uc\n    \n    # calculate eigenvectors and eigenvalues\n    # of projection Ã\n    Λ, W = eigen(Ã)\n    Λ = Diagonal(Λ)\n    \n    # reconstruct eigenvectors of A\n    Φ = Y*Vc*inv(Σc)*W\n    \n    return DMD(r,U,Ã,Λ,Φ)\nend\n\nfunction cDMD(X::Matrix, C::AbstractSparseMatrix)\n    X₁ = X[:, 1:end-1]\n    X₂ = X[:, 2:end]\n    return cDMD(X₂, X₁, C)\nend\n\nThe giant caveat is: how do we generate a unitary compression matrix? In fact we can relax this condition if we simply want to recover the eigenvalues and eigenvectors of A. It is enough that the data is sparse in some basis and that the compression matrix is incoherent with respect to that basis.\nWe can think of C as a set of k (1×n)-row vectors that project an n dimensional vector x onto a k dimensional space. There are several ways of finding the basis for this projection – e.g. a uniform random projection or a gaussian projection – but by far the simplest is to pick a random subset of k single pixels and only take the measurements from those pixels.\n\nfunction cDMD(Y::Matrix, X::Matrix, k::Integer)\n    n, m = size(X)\n    @assert k≤n\n    \n    # build (sparse) compression matrix\n    C = spzeros(k, n)\n    for i in 1:k\n        C[i,rand(1:n)] = 1\n    end\n\n    return cDMD(Y, X, C)\nend\n   \nfunction cDMD(X::Matrix, k::Integer)\n    X₁ = X[:, 1:end-1]\n    X₂ = X[:, 2:end]\n    return cDMD(X₂, X₁, k)\nend\n\nSuppose we sample at 300 randomly chosen points in the flow field to form the compression matrix\n\nk = 300\n\nC = spzeros(k, n)\nfor i in 1:k\n    C[i,rand(1:n)] = 1\nend\n\nThat is to say we are only sampling the vorticity at the green dots. This reduces the dimensionality of the data going in to the DMD algorithm from 89351 to 300.\n\n\nOriginal flow field with randomly generated sample points for compressed DMD.\n\nWe can generate a few different compressed DMDs to get a sense of how this impacts the overall performance (in terms of the Frobenius norm) and, much like we saw with reduced DMD, there are diminishing returns.\n\n\n\n\n\n\n\n\nFigure 7: Compressed DMD performance, as measured by the Frobenius norm of the difference between the original flow field and the reconstructed field, over a range of sample sizes.\n\n\n\n\n\nUsing the compression matrix from above, we can generate a compressed DMD3\n3 While we can reconstruct the eigenvalues and eigenvectors quite successfully, I don’t believe we adequately reconstruct U, and so this really only works for the continuous system. The reconstruction of U strongly depends on C being unitary and I don’t think that condition can be relaxed.\n\n\n\n\n\nFigure 8: Original flow field (top) and reconstructed flow field (bottom), using compressed DMD and sampling at 300 points.\n\n\n\nThe compressed DMD does not actually reduce the storage size of any of the matrices, it is more a technique to speed up the calculation of the SVD. Compressed DMD and reduced DMD can be combined: first by compressing the n×m matrix X to a k×m matrix Xc and then finding the best rank r approximation to the compressed matrix by truncating the SVD to the r largest singular values. The reduction step reduces the memory requirements and, if truncated SVD is used as well, this could significantly improve performance for enormous systems.\nThere is a related approach called compressed sensing DMD, in which the full state vector is not available in the first place. A much smaller dimension set of measurements is sampled and the full state DMD generated using the same general idea as compressed DMD. It isn’t that much of a leap from what is above, just with a convex optimization step added to reconstruct the actual state matrix for a given set of measurements."
  },
  {
    "objectID": "posts/dynamic_mode_decomposition/index.html#physics-informed-dmd",
    "href": "posts/dynamic_mode_decomposition/index.html#physics-informed-dmd",
    "title": "Dynamic Mode Decomposition",
    "section": "Physics Informed DMD",
    "text": "Physics Informed DMD\nThe idea behind physics informed DMD is that the physics of the system imposes structure upon the solution, which we can build into the DMD algorithm. This way we generate results that are consistent with physical reality. Which is to say that we are not merely finding the best fit matrix A, we are finding the best fit matrix A subject to some constraints on its structure. The paper I am using as a reference gives a nice table of different types of flow problems and the sort of structure one might want to impose upon the solution,4\n4 Baddoo et al., “Physics-Informed Dynamic Mode Decomposition (piDMD)” fig 3.\n\n\n\n\n\nFigure 9: A comparison of models trained with exact DMD and with piDMD, also showing the matrix structure of the corresponding piDMD method.5\n\n5 Baddoo et al., fig. 3.\n\nConveniently the flow past a cylinder example is on that table (that definitely wasn’t a motivating factor for choosing it as the example in the first place, nope, not at all) and what we want to impose on the solution is conservation of energy. Conservation of energy in this case equates to requiring that A be unitary, which is the standard procrustes problem\nWe modify the best fit such that we are looking for the A matrix that minimizes\n\\[ \\| \\mathbf{ A X } - \\mathbf{Y} \\|_{F} \\]\n\\[ \\textrm{ subject to } \\mathbf{A}^{*} \\mathbf{A} = \\mathbf{I} \\]\nFor which the standard solution is to define a matrix M\n\\[ \\mathbf{M} = \\mathbf{Y} \\mathbf{X}^{*} \\]\nsupposing M has SVD\n\\[ \\mathbf{M} = \\mathbf{U}_{M} \\mathbf{\\Sigma}_{M} \\mathbf{V}_{M}^{*} \\]\nthen the solution is\n\\[ \\mathbf{A} = \\mathbf{U}_{M} \\mathbf{V}_{M}^{*} \\]\nOf course we can’t directly compute M in many cases for the same reason that we can’t directly compute A : it would be a n×n matrix and for large n that would be enormous. So instead we project X and Y onto the upper singular values of X and solve the procrustes problem in that smaller space:\n\\[ \\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^{*} \\]\n\\[ \\mathbf{ \\tilde{X} } = \\mathbf{U}^{*} \\mathbf{X} \\]\n\\[ \\mathbf{ \\tilde{Y} } = \\mathbf{U}^{*} \\mathbf{Y} \\]\n\\[ \\mathbf{ \\tilde{M} } = \\mathbf{ \\tilde{Y} } \\mathbf{ \\tilde{X} }^{*} = \\mathbf{U}^{*} \\mathbf{Y} \\mathbf{X}^{*} \\mathbf{U} = \\mathbf{U}^{*} \\mathbf{M} \\mathbf{U}\\]\nsince SVD is invariant to left and right unitary transformations, the SVD of the projected \\(\\mathbf{ \\tilde{M} }\\) is\n\\[ \\mathbf{ \\tilde{M} } = \\mathbf{U}_{ \\tilde{M} } \\mathbf{\\Sigma}_{M} \\mathbf{V}_{ \\tilde{M} }^{*} \\]\nwhere\n\\[ \\mathbf{U}_{ \\tilde{M} } = \\mathbf{U}^{*} \\mathbf{U}_{ M } \\textrm{ and } \\mathbf{V}_{ \\tilde{M} } = \\mathbf{U}^{*} \\mathbf{V}_{M} \\]\nand the A matrix which solves the projected procrustes problem is\n\\[ \\mathbf{ \\tilde{A} } = \\mathbf{U}_{ \\tilde{M} } \\mathbf{V}_{ \\tilde{M} }^{*} = \\mathbf{U}^{*} \\mathbf{U}_{ M } \\mathbf{V}_{M}^{*} \\mathbf{U} = \\mathbf{U}^{*} \\mathbf{A} \\mathbf{U} \\]\nwhich is exactly the projected A matrix we need to proceed with reconstructing the eigenvalues and eigenvectors as per the standard DMD algorithm.\n\n# this is piDMD *only* for the case where A must be unitary\n# see arXiv:2112.04307 for details on the alternative cases\nfunction piDMD(Y::Matrix, X::Matrix)\n    # dimension\n    r = rank(X)\n    \n    # Full SVD\n    U, _, _ = svd(X)\n    \n    # projection\n    Ỹ = U'*Y\n    X̃ = U'*X\n    M̃ = Ỹ*X̃'\n    \n    # solve procrustes problem\n    Uₘ, _, Vₘ = svd(M̃)\n    Ã = Uₘ*Vₘ'\n    \n    # calculate eigenvectors and eigenvalues\n    # of projection Ã\n    Λ, W = eigen(Ã)\n    Λ = Diagonal(Λ)\n    \n    # reconstruct eigenvectors of A\n    Φ = U*W\n    \n    return DMD(r,U,Ã,Λ,Φ)\nend\n\nfunction piDMD(X::Matrix)\n    X₁ = X[:, 1:end-1]\n    X₂ = X[:, 2:end]\n    return piDMD(X₂, X₁)\nend\n\n\n\n\n\n\n\nFigure 10: Original flow field (top) and reconstructed flow field (bottom), using physics informed DMD.\n\n\n\nWe can compare the Frobenius norm of the actual data versus the predicted, and it’s clear the physics informed DMD does not generate as good of a fit as exact DMD. Though it could equally be the case that the exact DMD is over-fitting.\n\nnorm(X₂ - X̂₂_pi, 2)\n\n18.35684111920036\n\n\n\nnorm(X₂ - X̂₂_exact, 2)\n\n0.0005597047465277092\n\n\nThe main reason why you would pursue physics informed DMD, though, is not necessarily to generate a better fit as much as to generate better (or more physically realistic) dynamic modes.\nSimilarly to compressed DMD, physics informed DMD can also be combined with reduced DMD. In this case there are two SVD steps but only the upper singular values of X, the U matrix, needs to be truncated. The second SVD proceeds without truncation."
  },
  {
    "objectID": "posts/dynamic_mode_decomposition/index.html#references",
    "href": "posts/dynamic_mode_decomposition/index.html#references",
    "title": "Dynamic Mode Decomposition",
    "section": "References",
    "text": "References\n\n\nBaddoo, Peter J., Benjamin Herrmann, Beverley J. McKeon, J. Nathan Kutz, and Steven L. Brunton. “Physics-Informed Dynamic Mode Decomposition (piDMD),” December 8, 2021. https://doi.org/10.48550/arXiv.2112.04307.\n\n\nBai, Zhe, Eurika Kaiser, Joshua L. Proctor, J. Nathan Kutz, and Steven L. Brunton. “Dynamic Mode Decomposition for Compressive System Identification.” AIAA Journal 58 (2020): 561–74. https://doi.org/10.2514/1.J057870.\n\n\nBrunton, Steven L., and J. Nathan Kutz. Data Driven Science and Engineering. Cambridge: Cambridge University Press, 2019. http://databookuw.com.\n\n\nBrunton, Steven L., Joshua L. Proctor, and J. Nathan Kutz. “Compressive Sampling and Dynamic Mode Decomposition,” December 18, 2013. https://doi.org/10.48550/arXiv.1312.5186.\n\n\nBrunton, Steven L., Joshua L. Proctor, Jonathan H. Tu, and J. Nathan Kutz. “Compressed Sensing and Dynamic Mode Decomposition.” Journal of Computational Dynamics 2 (2015): 165–91. https://doi.org/10.3934/jcd.2015002.\n\n\nSchmid, Peter J. “Dynamic Mode Decomposition of Numerical and Experimental Data.” Journal of Fluid Mechanics 656 (2010): 5–28. https://doi.org/10.1017/S0022112010001217.\n\n\nTu, Jonathan H., Clarence W. Rowley, Dirk Martin Luchtenburg, Steven L. Brunton, and J. Nathan Kutz. “On Dynamic Mode Decomposition: Theory and Applications.” Journal of Computational Dynamics 1 (2014): 391–421. https://doi.org/10.3934/jcd.2014.1.391."
  },
  {
    "objectID": "posts/vessel_blowdown_real_gases/index.html",
    "href": "posts/vessel_blowdown_real_gases/index.html",
    "title": "Vessel Blowdown - Real Gases",
    "section": "",
    "text": "Continuing on from where I left off previously, examining vessel blowdown, it is time to implement real gases. I left the ideal gas case promising that implementing a real gas was easy, well now is the time to prove it. Instead of implementing real gas equations of state myself, I am going to use Clapeyron.jl but, as a first step, it is worthwhile to consider how the problem can be divided up into sub-problems and what data structures would be the most useful. I would like to write code that is general enough that any equation of state can be used with minimal changes. With that in mind, I am going to consider the problem as being composed of three distinct subsets: the vessel, the fluid model, and the ambient conditions."
  },
  {
    "objectID": "posts/vessel_blowdown_real_gases/index.html#data-structures",
    "href": "posts/vessel_blowdown_real_gases/index.html#data-structures",
    "title": "Vessel Blowdown - Real Gases",
    "section": "Data Structures",
    "text": "Data Structures\nThe properties of the vessel form a natural data structure containing the valve properties, the vessel volume, and the initial conditions. It can also be divided into two distinct sub-problems: the gas expansion within the vessel and the gas expansion across the valve. The gas expansion within the vessel will be governed by the ODE or DAE for the particular expansion type – isothermal, adiabatic, &c. – whereas the expansion across the valve will always be isentropic. These sub-problems can then be solved in a way that is agnostic to the equation of state.\nAn important decision must be made regarding which subset of the state variables, \\(P, v, T\\), will be used to define the system. The remaining variable will be defined by the equation of state. Equations of state are typically given in relation to the Helmholtz free energy, \\(A\\), a function of molar volume and temperature, which makes those a natural choice. The pressure vessel can then be instantiated with the total volume, total mass of material contained, and the vessel temperature. The pressure then varies with the equation of state. Alternatively, the pressure and temperature of the vessel could be chosen as the state variables. But then the total mass in the vessel depends on the particular equation of state, which strikes me as weird.\nbegin\n\nstruct PressureVessel{F &lt;: Number}\n    c::F # valve discharge coefficient\n    A::F # valve flow area\n    V::F # vessel volume\n    T::F # vessel temperature\n    m::F # total mass of material\nend\n\nPressureVessel(c, A, V, T, m) = \n    PressureVessel(promote(c, A, V, T, m)...)\n\nend\nAbstracting the fluid properties – the P-v-T relationship, entropy, enthalpy, and the like – allows the vessel blowdown model to be re-used easily. Using Julia’s multiple dispatch no code even needs to change, just add new methods for a new fluid model and everything works. This leads naturally to a way of checking that the vessel model is working by comparing an ideal gas model to the known analytic solution. Verifying that it works with an ideal gas then gives confidence that the model is working with a real gas, for which the analytic solution is unknown.\nCollecting the ambient conditions into a data structure does not lead to any spectacular improvements or insights, it is just neat and tidy.\nbegin\n\nstruct Environment{F &lt;: Number}\n    P::F\n    T::F\nend\n\nEnvironment(P, T) = Environment(promote(P,T)...)\n\nend\nFinally, a data structure for blowdown solutions is useful for dispatch.\nstruct Blowdown{S}\n    pv::PressureVessel\n    env::Environment\n    sol::S\nend\nBase.length(::Blowdown) = 1\nBase.iterate(b::Blowdown, state=1) = state &gt; length(b) ? nothing : (b,state+1)"
  },
  {
    "objectID": "posts/vessel_blowdown_real_gases/index.html#equations-of-state",
    "href": "posts/vessel_blowdown_real_gases/index.html#equations-of-state",
    "title": "Vessel Blowdown - Real Gases",
    "section": "Equations of State",
    "text": "Equations of State\n\nIdeal gases\nThe first fluid model worth creating is the ideal gas model that corresponds to the known, analytic, solution. Specifically an ideal gas with constant heat capacities such that \\(c_p - c_v = R\\). Starting with the following data structure.\nbegin\n\nconst R = 8.31446261815324 # m³⋅Pa/K/mol\n\nstruct IdealGas{F &lt;: Number}\n    cᵥ::F # J/kg/K\n    cₚ::F # J/kg/K\n    k::F\n    R::F  # J/kg/K\n    MW::F # kg/mol\nend\n\nfunction IdealGas(cᵥ,MW; R=R)\n    cᵥ, MW = promote(cᵥ,MW)\n    cₚ = cᵥ + R\n    k = cₚ/cᵥ\n    return IdealGas(cᵥ,cₚ,k,R,MW)\nend\n\nfunction IdealGas(model::Clapeyron.EoSModel; \n                  P=101325, T=288.15, z=[1.])\n    MW = Clapeyron.molecular_weight(model, z) # kg/mol\n    cᵥ = Clapeyron.isochoric_heat_capacity(model, P, T, z) # J/mol/K\n    return IdealGas(cᵥ, MW)\nend\n    \nend\nA good practice, when solving ODEs, is to use NaNMath.jl for roots, logarithms, and the like. These versions return NaN when results are outside of the function domain – for example \\(\\sqrt{-1}\\) – instead of throwing a DomainError. Returning NaNs makes it easier for the ODE solver to detect when it has left the domain of a valid solution.\nbegin \n\nusing NaNMath\n\n√ = NaNMath.sqrt\nlog = NaNMath.log\n\nend\nThe equation of state is implemented as a series of high-level functions, dispatching on the fluid model and returning the relevant fluid properties. Extending the blowdown model to use a different equation of state involves merely overloading these to dispatch on a different fluid type.\npressure(model::IdealGas, v, T) = model.R*T/v\nvolume(model::IdealGas, P, T) = model.R*T/P\nmolecular_weight(model::IdealGas) = model.MW\nmolar_enthalpy(model::IdealGas, v, T) = model.cₚ*T\nmolar_entropy(model::IdealGas, v, T) =\n model.cᵥ*log(T) + model.R*log(v)\nmolar_internal_energy(model::IdealGas, v, T) = model.cᵥ*T\nspeed_of_sound(model::IdealGas, v, T) =\n √(model.k*model.R*T/model.MW)\n\n\nReal Gases with Clapeyron.jl\nThe high-level functions defined above are mapped to the corresponding Clapeyron.jl functions. And that’s it. Everything is ready to use for whichever equation of state your heart desires.\nimport Clapeyron\npressure(model::Clapeyron.EoSModel, v, T) =\n Clapeyron.pressure(model, v, T)\nvolume(model::Clapeyron.EoSModel, P, T; v0=nothing) = \n Clapeyron.volume(model, P, T; phase=:vapor, vol0=v0)\nmolecular_weight(model::Clapeyron.EoSModel) =\n Clapeyron.molecular_weight(model)\nmolar_enthalpy(model::Clapeyron.EoSModel, v, T) =\n Clapeyron.VT_enthalpy(model, v, T)\nmolar_entropy(model::Clapeyron.EoSModel, v, T) =\n Clapeyron.VT_entropy(model, v, T)\nmolar_internal_energy(model::Clapeyron.EoSModel, v, T) =\n Clapeyron.VT_internal_energy(model, v, T)\nspeed_of_sound(model::Clapeyron.EoSModel, v, T) =\n Clapeyron.VT_speed_of_sound(model, v, T)"
  },
  {
    "objectID": "posts/vessel_blowdown_real_gases/index.html#isentropic-nozzle-flow",
    "href": "posts/vessel_blowdown_real_gases/index.html#isentropic-nozzle-flow",
    "title": "Vessel Blowdown - Real Gases",
    "section": "Isentropic Nozzle Flow",
    "text": "Isentropic Nozzle Flow\nThe vessel blowdown relies on a good model of isentropic nozzle flow. This involves finding the pressure and temperature in the throat of the nozzle which maximizes the mass flux, G, while satisfying the constraints that the path from a stagnation point in the vessel through the nozzle is isentropic and that the enthalpy is conserved. The flow is further constrained to be either sonic or subsonic, i.e. the Mach number is less than or equal to one.\n\\[\ns_1 = s_t\n\\]\n\\[\nh_1 = h(v_t, T_t) + \\frac{1}{2} M u_t^2\n\\]\nFor almost all of the blowdown the flow will be sonic and the pressure in the throat of the nozzle will be greater than atmospheric, this is called choked flow. The entropy and enthalpy balances can be solved for the throat conditions, \\(v_t, T_t\\), assuming the velocity is the local speed of sound. I do this here using NonlinearSolve.jl, where the objective function, choked_nozzle_balance!, is in-place.\nusing NonlinearSolve\nfunction choked_nozzle_balance!(obj, y, prms)\n    # y = [v; T]\n    obj .= [ prms.entropy - molar_entropy(prms.model, y[1], y[2])\n             prms.enthalpy - molar_enthalpy(prms.model, y[1], y[2]) - 0.5*molecular_weight(prms.model)*speed_of_sound(prms.model, y[1], y[2])^2 ]\n    return nothing\nend\nchoked_nozzle_prob = NonlinearProblem(choked_nozzle_balance!, [0.0; 0.0], \n                                      (model=nothing, env=nothing,\n                                       entropy=0.0, enthalpy=0.0))\nIn the case where the flow is subsonic, the pressure in the throat of the nozzle is atmospheric and the entropy and enthalpy balances are solved for gas velocity and temperature, \\(u_t, T_t\\).\nfunction non_choked_nozzle_balance!(obj, y, prms)\n    # y = [u; T]\n    v = volume(prms.model, prms.env.P, y[2])\n    obj .= [ prms.entropy - molar_entropy(prms.model, v, y[2])\n             prms.enthalpy - molar_enthalpy(prms.model, v, y[2]) - 0.5*molecular_weight(prms.model)*y[1]^2 ]\n    return nothing\nend\nnon_choked_nozzle_prob = NonlinearProblem(non_choked_nozzle_balance!,\n                                             [0.0; 0.0],\n                                             (model=nothing, env=nothing,\n                                              entropy=0.0, enthalpy=0.0))\nThe most obvious and direct way of solving the entropy and energy balances is to solve the optimization problem. However, I could not get that to work reliably. Using the same constraints on entropy and enthalpy as well as constraining the Mach number to be less than or equal to one, I could get it to work but only with very good guesses of the initial conditions. Using Optimization.jl, it would either get stuck in a local maximum or, depending on the solver, sometimes return results that simply did not satisfy the constraints (but came with return code “Success”). Given that this is going to be wrapped in an ODE and executed, potentially, hundreds of times, that is not good.\nMy completely stupid but it works approach is to solve the choked flow nonlinear system first and, if the nozzle pressure is below atmospheric, solve the non-choked flow system instead. This works perfectly though, presumably, is not nearly as efficient as solving the optimization problem directly would be if I could get it to work properly.\nfunction mass_flow(model, pv, env, v, T)\n    # calculate the molar entropy and molar enthalpy\n    # at vessel conditions\n    s₁ = molar_entropy(model, v, T)\n    h₁ = molar_enthalpy(model, v, T)\n\n    # solve the choked flow energy balance for\n    # an isentropic nozzle\n    params = (model=model, env=env, entropy=s₁, enthalpy=h₁)\n    y₀ = [v; T]\n    prob = remake(choked_nozzle_prob, u0=y₀, p=params)\n    sol = solve(prob, NewtonRaphson())\n    vₜ, Tₜ = sol.u\n    Pₜ = pressure(model, vₜ, Tₜ)\n    if Pₜ &gt; env.P\n        # flow is choked, we're done\n        uₜ = speed_of_sound(model, vₜ, Tₜ)\n    else\n        # flow is not choked, solve the non-choked problem\n        v₀ = volume(model, env.P, T)\n        y₀ = [ speed_of_sound(model, v₀, T); T ]\n        prob = remake(non_choked_nozzle_prob, u0=y₀, p=params)\n        sol = solve(prob, NewtonRaphson())\n        uₜ, Tₜ = sol.u\n        vₜ = volume(model, env.P, Tₜ)\n    end\n\n    ρₜ = molecular_weight(model)/vₜ\n    return pv.c*pv.A*ρₜ*uₜ\nend"
  },
  {
    "objectID": "posts/vessel_blowdown_real_gases/index.html#adiabatic-blowdown",
    "href": "posts/vessel_blowdown_real_gases/index.html#adiabatic-blowdown",
    "title": "Vessel Blowdown - Real Gases",
    "section": "Adiabatic Blowdown",
    "text": "Adiabatic Blowdown\n\nThe Pressure Equation\nThe general adiabatic blowdown solution proceeds in the same way as the ideal gas case (solved previously). Here the isentropic path is not directly available, so the problem is rewritten as a Differential Algebraic Equation (DAE), where the vessel state is constrained to be isentropic.\nThe first step is to define a basic type, PressureODE; which will allow functions like blowdown_pressure to dispatch on solution type.\nstruct PressureODE{S}\n    ode_sol::S\nend\nThe governing equations are the ODE as defined before, plus the constraints that the P-v-T behaviour follows the equation of state and the entropy is constant.\n\\[\n\\frac{dP}{dt} = -\\frac{c_D A}{V} a^2 G\n\\]\n\\[\n0 = v - volume(P, T)\n\\]\n\\[\n0 = s_0 - entropy(v, T)\n\\]\nThe equation of state does not need to be pulled into the DAE like this. It could be incorporated into the right hand side of the ODE. However, it is often convenient to have all of the state variables directly accessible in the solution.\nusing OrdinaryDiffEq, DiffEqCallbacks\nfunction adiabatic_vessel!(dy, y, prms, t)\n    P, v, T = y\n    \n    a² = speed_of_sound(prms.model, v, T)^2\n    w = mass_flow(prms.model, prms.pv, prms.env, v, T)\n\n    dy .= [-w*a²/prms.pv.V\n            v - volume(prms.model, P, T)\n            prms.init - molar_entropy(prms.model, v, T) ]\n    return nothing\nend\nabd_rhs = ODEFunction(adiabatic_vessel!, mass_matrix = [1 0 0\n                                                        0 0 0\n                                                        0 0 0])\nA callback function is used to terminate the integration once the vessel is within a given tolerance of atmospheric pressure. Without this the blowdown would continue forever, or until the limits of machine precision (whichever came first). Technically, this blowdown model predicts the pressure in the vessel will get arbitrarily close to atmospheric pressure but never actually achieve it.\ndepressured_callback(y, t, I; reltol=0.001) =\n    y[1] - (1+reltol)*I.p.env.P\nThe entire model is packaged into a function which takes a fluid, pressure vessel, and environment and returns a Blowdown solution. By splitting the problem up like this, different fluid models, vessels or ambient conditions can be swapped around while reusing what has already been defined.\nfunction adiabatic_blowdown(model, pv::PressureVessel, \n                            env::Environment;\n                            solver=Rodas5(), \n                            tspan=(0.0, 600.0))\n\n    # vessel initial conditions\n    V, T₀, m = pv.V, pv.T, pv.m\n    n₀ = m/molecular_weight(model)\n    v₀ = V/n₀\n    P₀ = pressure(model, v₀, T₀)\n    \n    # defining the parameters\n    s₀ = molar_entropy(model, v₀, T₀)\n    params = (model=model, pv=pv, env=env, init=s₀)\n\n    # callbacks\n    dpcb = ContinuousCallback(depressured_callback, terminate!)\n\n    # set up the ODEProblem and solve\n    y₀ = [P₀; v₀; T₀]\n    prob = ODEProblem(abd_rhs, y₀, tspan, params)\n    sol = solve(prob, solver; callback=dpcb)\n\n    return Blowdown(pv,env,PressureODE(sol))\nend\nFrom the ODE solution the blowdown time, pressure curve, and temperature can be recovered.\nblowdown_time(bd::Blowdown{&lt;:PressureODE}) =\n    bd.sol.ode_sol.t[end]\nfunction blowdown_pressure(bd::Blowdown{&lt;:PressureODE}, t)\n    bdt = blowdown_time(bd)\n    t = min(t, bdt)\n    return bd.sol.ode_sol(t; idxs=1)\nend\nfunction blowdown_temperature(bd::Blowdown{&lt;:PressureODE}, t)\n    bdt = blowdown_time(bd)\n    t = min(t, bdt)\n    return bd.sol.ode_sol(t; idxs=3)\nend\n\nThe Ideal Gas Choked Flow Model\nThe entire model, including all of the sub-models, is complicated and could easily have typos and hard to notice errors in it. An easy way to check this is to compare the results against the known analytic solution for the case where the gas is an ideal gas and the flow through the nozzle is always choked.\nstruct IdealGasChoked{F &lt;: Number}\n    P₀::F\n    k::F\n    τ::F\nend\nfunction adiabatic_choked_blowdown(model::IdealGas, pv::PressureVessel,\n                                   env::Environment)\n    # vessel parameters\n    c, A = pv.c, pv.A\n    \n    # vessel initial conditions\n    V, T₀, m = pv.V, pv.T, pv.m\n    n₀ = m/molecular_weight(model)\n    v₀ = V/n₀\n    P₀ = pressure(model, v₀, T₀)\n\n    k, R, MW = model.k, model.R, model.MW\n    τ = 1/( (c*A/V)*√(k*R*T₀/MW)*(2/(k+1))^((k+1)/(2*(k-1))) )\n    return Blowdown(pv,env,IdealGasChoked(P₀,k,τ))\nend\nfunction blowdown_time(bd::Blowdown{&lt;:IdealGasChoked})\n    P₀, Pₐ, k, τ = bd.sol.P₀, bd.env.P, bd.sol.k, bd.sol.τ\n    return (2τ/(1-k))*(1 - (Pₐ/P₀)^((1-k)/2k))\nend\nfunction blowdown_pressure(bd::Blowdown{&lt;:IdealGasChoked}, t)\n    P₀, k, τ = bd.sol.P₀, bd.sol.k, bd.sol.τ\n    t = min(t, blowdown_time(bd))\n    return P₀*( 1 + 0.5*(k-1)*(t/τ))^((2*k)/(1-k))\nend\nfunction blowdown_temperature(bd::Blowdown{&lt;:IdealGasChoked}, t)\n    T₀, P₀, k = bd.pv.T, bd.sol.P₀, bd.sol.k\n    t = min(t, blowdown_time(bd))\n    P = blowdown_pressure(bd, t)\n    return T₀*(P/P₀)^((k-1)/k)\nend\n\n\nChecking our work\nThe same situation as the previous post on ideal gas blowdown is used here, a gas cylinder at 3000psia blowing down through a valve into the air. In this case the gas is nitrogen, instead of air, as having a single species is simpler than a mixture (though not by much).\natm = Environment(101325,288.15)\nvessel = let\n    c = 0.85\n    D = 0.005 # m\n    A = 0.25*π*D^2 # m²\n    V = 0.01111 # m³\n    m = 2.743 # kg\n    T = 288.15 # K\n    PressureVessel(c, A, V, T, m)\nend\nThe real gas is modelled using a volume translated Peng-Robinson equation of state.\nusing Clapeyron:PR, ReidIdeal, RackettTranslation\nnitrogen = PR([\"nitrogen\"]; idealmodel=ReidIdeal, \n              translation=RackettTranslation);\nig_nitrogen = IdealGas(nitrogen);\nchoked_model = adiabatic_choked_blowdown(ig_nitrogen, vessel, atm);\nideal_gas = adiabatic_blowdown(ig_nitrogen, vessel, atm);\nreal_gas = adiabatic_blowdown(nitrogen, vessel, atm);\n\n\n\n\n\n\nFigure 1: The adiabatic blowdown curve for a tank of nitrogen, showing the fully choked ideal gas model model and the DAE solutions for both the ideal gas and real gas case.\n\n\n\nThe blowdown using an ideal gas equation of state matches the known solution for the entire domain where flow is actually choked. This gives some assurance that the general model is working properly. The real gas model, VTPR, appears to work well and is not too far from the ideal case, as expected.\nWhen I first played around with this I assumed flow through the nozzle was always choked (as a test) and this led to numerical difficulties near the end of the integration. I had to manually stop the integration at around 20s. Each subsequent time step would end up venting a physically unrealistic amount of material and the thermodynamic models would start to suffer from domain errors. Pleasingly, once a better model for the valve was swapped in, these problems went away.\n\n\n\n\n\n\nFigure 2: The vessel temperature for the nitrogen blowdown, showing the fully choked ideal gas model model and the DAE solutions for both the ideal gas and real gas case.\n\n\n\nAnother problem that can happen, depending upon the equation of state, is that the vessel may be outside the range where the equation of state can return a physically real gas. The temperature in the vessel, and especially within the nozzle, drops dramatically and in this case it drops below the boiling point of nitrogen by the time the vessel is fully depressured. This is a real result. It is not a consequence of some numerical error. It is a consequence of assuming a perfectly adiabatic vessel.\n\n\n\nThe Energy Equation\nAnother way of approaching this problem is to perform a mass and energy balance. This is a more general approach and is what underlies more complex blowdown simulators (such as BLOWDOWN). Starting with a mass balance:\n\\[\n\\frac{dm}{dt} = -w\n\\]\nThe energy balance is that the change in the internal energy within the vessel is equal to the rate of heat in, through the walls of the vessel, minus the rate of heat lost due to flow out of the vessel.\n\\[\n\\frac{dU}{dt} = Q_i - Q_o\n\\]\n\\[\n\\frac{dU}{dt} = Q_i - w \\bar{h}\n\\]\nWhere \\(\\bar{h}\\) is the specific enthalpy, note this is at vessel conditions. The boundary for the energy balance is around the vessel, not including the valve.\nThe total internal energy is the product of the mass remaining in the vessel and the specific internal energy, \\(U=m \\bar{u}\\). Applying the chain rule:\n\\[\n\\frac{dU}{dt} = m \\frac{d \\bar{u}}{dt} + \\bar{u} \\frac{dm}{dt} = m \\frac{d \\bar{u}}{dt} - \\bar{u} w\n\\]\nCombining these two expressions:\n\\[\n\\frac{d \\bar{u}}{dt} = \\frac{1}{m} \\left( Q_i + \\left(\\bar{u} - \\bar{h} \\right) w \\right)\n\\]\nThe specific internal energy, \\(\\bar{u}\\), is related to the molar internal energy, \\(u\\), by the molar weight, \\(M \\bar{u} = u\\), similarly for the specific and molar enthalpy. Substituting and multiplying through by the molar weight gives:\n\\[\n\\frac{d u}{dt} = \\frac{1}{m} \\left( M Q_i + \\left(u - h \\right) w \\right)\n\\]\nThe remaining mass, \\(m\\), can be written in terms of the molar volume, \\(v\\):\n\\[\n\\frac{d u}{dt} = \\frac{v}{M V} \\left( M Q_i + \\left(u - h \\right) w \\right)\n\\]\nThe mass balance can also be written in terms of the molar volume:\n\\[\n\\frac{dv}{dt} = \\frac{w v^2}{M V}\n\\]\nThe full system of equations, in terms of \\(u, v, T\\) is then:\n\\[\n\\frac{d u}{dt} = \\left( M Q_i + \\left(u - h \\right) w \\right) \\frac{v}{M V}\n\\]\n\\[\n\\frac{dv}{dt} = \\frac{w v^2}{M V}\n\\]\n\\[\n0 = u - internal\\_energy(v, T)\n\\]\nThe adiabatic case is the special case where \\(Q_i = 0\\).\n\nThe Adiabatic Ideal Gas Case\nIt is not immediately clear that this is the same model as the adiabatic pressure equation. The adiabatic pressure equation assumes the expansion within the vessel is isentropic, but that condition is not explicitly applied in the energy equation. One hint this is the same model is that the ideal gas solution can be derived from the energy balance.\nConsider an ideal gas with constant heat capacities such that \\(u = c_v T\\) and \\(h = c_p T\\). For the adiabatic case the energy balance becomes:\n\\[\nc_v \\frac{d T}{dt} = \\frac{1}{m} \\left( c_v T - c_p T \\right) w\n\\]\nIsentropic choked flow of an ideal gas occurs with:\n\\[\nw = c_d A \\rho_t \\sqrt{ \\frac{k R T_t}{M} }\n\\]\nWith nozzle density and temperature related to the vessel conditions by:\n\\[\n\\rho_t = \\rho \\left( 2 \\over {k+1} \\right)^{\\frac{1}{k-1}}\n\\]\n\\[\nT_t = T \\left( 2 \\over {k+1} \\right)\n\\]\nSubstituting all of this into the energy equation and dividing by \\(c_v\\) gives:\n\\[\n\\frac{dT}{dt} = \\frac{c_d A}{V} \\left( 1 - k \\right) \\left(2 \\over {k+1} \\right)^{\\frac{k+1}{2(k-1)}} \\sqrt{ \\frac{k R T}{M} } T\n\\]\nWhere \\(k = \\frac{c_p}{c_v}\\). The time constant \\(\\tau\\) is defined such that:\n\\[\n\\frac{1}{\\tau} = \\frac{c_d A}{V} \\left(2 \\over {k+1} \\right)^{\\frac{k+1}{2(k-1)}} \\sqrt{ \\frac{k R T_0}{M} }\n\\]\nWhich simplifies the ODE to:\n\\[\n\\frac{dT}{dt} = \\frac{1-k}{\\tau} T \\sqrt{\\frac{T}{T_0}}\n\\]\nThis is a separable equation and can be integrated to give:\n\\[\n\\frac{T}{T_0} = \\left( 1 + \\frac{k-1}{2} \\frac{t}{\\tau} \\right)^{-2}\n\\]\nFor an adiabatic expansion of an ideal gas:\n\\[\n\\frac{P}{P_0} = \\left( \\frac{T}{T_0} \\right)^{\\frac{k}{k-1}}\n\\]\nWhich recovers the original solution:\n\\[\n\\frac{P}{P_0} = \\left( 1 + \\frac{k+1}{2} \\frac{t}{\\tau} \\right)^{\\frac{2k}{1-k}}\n\\]\n\n\nImplementing the DAE\nThe governing equations for the vessel blowdown can be implemented as a DAE though, as the state variables, \\(u, v, T\\), no longer include pressure, determining when the vessel has fully depressured is slightly more complicated. The callback function must first calculate the pressure in the system. Previously, the callback function was a ContinuousCallback, which adjusts the final time step to exactly depressurize the vessel. Here the callback is a DiscreteCallback which terminates once a time step has crossed the threshold.\nfunction energy_eqn!(dy, y, prms, t)\n    u, v, T = y\n\n    h = molar_enthalpy(prms.model, v, T)\n    w = mass_flow(prms.model, prms.pv, prms.env, v, T)\n    M = molecular_weight(prms.model)\n    V = prms.pv.V\n\n    dy .= [ (M*prms.Qᵢ(T) + (u-h)*w)*v/(M*V)\n            (w*v^2)/(M*V)\n            u - molar_internal_energy(prms.model, v, T) ]\n    return nothing\nend\nueqn_rhs = ODEFunction(energy_eqn!, mass_matrix = [ 1 0 0\n                                                    0 1 0\n                                                    0 0 0 ])\ndepressured_callback_2(y, t, I; reltol=0.001) =\n    pressure(I.p.model, y[2], y[3]) &lt; (1+reltol)*I.p.env.P\nTo generate the blowdown curve the pressure must be calculated, as it is no longer an output of the ODE. This could be done on demand, retrieving the molar volume and temperature for a given time and calculating the pressure. Another approach is to calculate the pressure at each time step and interpolate. This is implemented here as a SavingCallback, which calculates and saves the pressure after each time step. A cubic interpolation of the pressure is created from the results and used to generate the blowdown curve. The solution type contains two pieces: the ode solution and the pressure-time interpolation.\nusing DataInterpolations\nstruct EnergyODE{S,I}\n    ode_sol::S\n    p_interp::I\nend\nfunction energy_eqn_blowdown(model, pv::PressureVessel, \n                             env::Environment;\n                             Qi=(T)-&gt;0.0, \n                             solver=Rodas5(), \n                             tspan=(0.0, 600.0))\n\n    # vessel initial conditions\n    V, T₀, m = pv.V, pv.T, pv.m\n    Mw = molecular_weight(model)\n    v₀ = Mw*V/m\n    u₀ = molar_internal_energy(model, v₀, T₀)\n    \n    # defining the parameters\n    params = (model=model, pv=pv, env=env, Qᵢ=Qi)\n\n    # callbacks\n    svs = SavedValues(Float64, Float64)\n    svcb = SavingCallback((y, t, I) -&gt; pressure(I.p.model,y[2],y[3]), svs)\n    dpcb = DiscreteCallback(depressured_callback_2, terminate!)\n    cbs = CallbackSet(svcb,dpcb)\n\n    # set up the ODEProblem and solve\n    y₀ = [u₀; v₀; T₀]\n    prob = ODEProblem(ueqn_rhs, y₀, tspan, params)\n    sol = solve(prob, solver; callback=cbs)\n\n    # set up pressure interpolation\n    pi = AkimaInterpolation(svs.saveval, svs.t)\n\n    return Blowdown(pv,env,EnergyODE(sol,pi))\nend\nThe methods for blowdown time, pressure, and temperature are easily implemented.\nblowdown_time(bd::Blowdown{&lt;:EnergyODE}) =\n    bd.sol.ode_sol.t[end]\nfunction blowdown_pressure(bd::Blowdown{&lt;:EnergyODE}, t)\n    bdt = blowdown_time(bd)\n    t = min(t, bdt)\n    return bd.sol.p_interp(t)\nend\nfunction blowdown_temperature(bd::Blowdown{&lt;:EnergyODE}, t)\n    bdt = blowdown_time(bd)\n    t = min(t, bdt)\n    return bd.sol.ode_sol(t; idxs=3)\nend\nThe results from the energy model can be compared to the pressure model, they are functionally identical.\nideal_gas_energybd = energy_eqn_blowdown(ig_nitrogen, vessel, atm);\nreal_gas_energybd = energy_eqn_blowdown(nitrogen, vessel, atm);\n\n\n\n\n\n\nFigure 3: The blowdown curve for the nitrogen blowdown, showing the DAE solutions for both the pressure model and the energy balance model (ideal gas and real gas cases). The curves for the pressure model and energy balance model overlap.\n\n\n\n\n\n\n\n\n\nFigure 4: The vessel temperature for the nitrogen blowdown, showing the DAE solutions for both the pressure model and the energy balance model (ideal gas and real gas cases). The curves for the pressure model and energy balance model overlap.\n\n\n\n\n\n\nPerformance\nI did not put a lot of effort into making exceptionally performant code. Firstly, the model for isentropic flow through the valve could be improved. Presumably this could also be incorporated into the governing equations of the ODEs, at a cost to model simplicity and reusability, which might unlock some performance opportunities.\nGiven those limitations, the performance of the two models can be compared using BenchmarkTools.jl.\n@benchmark adiabatic_blowdown(nitrogen, vessel, atm)\nBenchmarkTools.Trial: 42 samples with 1 evaluation.\n Range (min … max):  111.588 ms … 149.424 ms  ┊ GC (min … max): 0.00% … 20.45%\n Time  (median):     120.515 ms               ┊ GC (median):    6.11%\n Time  (mean ± σ):   120.226 ms ±   5.882 ms  ┊ GC (mean ± σ):  4.39% ±  4.00%\n\n    ▂          ██ ▅█ █                                           \n  ▅██▁▅█▅▁▁▁▅▅███▅████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅ ▁\n  112 ms           Histogram: frequency by time          149 ms &lt;\n\n Memory estimate: 59.87 MiB, allocs estimate: 948090.\nThe adiabatic blowdown using the energy model is about 35% faster than the pressure model. Partly this is due to the choice of terminating callbacks. Whether or not integration is terminated with a DiscreteCallback or a ContinuousCallback does not meaningfully change the performance for the pressure model. However, this choice dramatically changes the performance of the energy model. Changing to a ContinuousCallback erases the difference between the two models.\n@benchmark energy_eqn_blowdown(nitrogen, vessel, atm)\nBenchmarkTools.Trial: 56 samples with 1 evaluation.\n Range (min … max):  82.811 ms … 122.534 ms  ┊ GC (min … max): 0.00% … 28.08%\n Time  (median):     89.420 ms               ┊ GC (median):    0.00%\n Time  (mean ± σ):   89.381 ms ±   6.032 ms  ┊ GC (mean ± σ):  4.05% ±  5.62%\n\n        ▂ ▂  ▂     ▂                        █▂                  \n  █▅▅▅▁▅█▅█▅▅█▅▁▅▅██▅▁▁▁▁▁▁▁▁▁▅▁▁▁▁▅▁▁▁▁▅▅█▅██▁██▅█▁▁▅▅▁▅▁█▁▁▅ ▁\n  82.8 ms         Histogram: frequency by time         95.7 ms &lt;\n\n Memory estimate: 44.30 MiB, allocs estimate: 727470.\nThe pressure model performance at the end of the blowdown is strongly dependent on whether molar volume is used as a state variable. When used as a state variable there is a major performance hit compared to moving the volume into the RHS, nearly double the compute time. Removing it as a state variable comes with a cost to the accuracy near the termination of the blowdown. It is not obvious to me why this is the case (maybe using volume as a system variable forces the solver to take smaller time steps?), but it hints that there are opportunities to improve the pressure model by tweaking how molar volume is incorporated.\n\n\n\n\n\n\nFigure 5: The blowdown curve for the pressure model when molar volume is moved to the RHS of the ODE. The pressure model curves have a weird bump at the end.\n\n\n\nA big caveat to the kind of loose performance comparison I did here is that I did not define a metric for performance. If you wanted to more rigorously benchmark these two approaches defining what constitutes “good enough” in terms of the blowdown curve is necessary. You can always make a model faster by making it less precise."
  },
  {
    "objectID": "posts/vessel_blowdown_real_gases/index.html#conclusions",
    "href": "posts/vessel_blowdown_real_gases/index.html#conclusions",
    "title": "Vessel Blowdown - Real Gases",
    "section": "Conclusions",
    "text": "Conclusions\nExtending the ideal gas blowdown to real gases using Clapeyron.jl is straightforward. Though the adiabatic case immediately calls into question the point in doing so. Even for a system as simple as a cylinder of nitrogen, the adiabatic assumption is too extreme to be plausible: it predicts the blowdown of a room temperature cylinder will result in a spray of liquid nitrogen. Really, though, the model breaks down once it results in the gas inside the vessel dropping below the boiling point while remaining a gas.\nRapid blowdowns often lead to cryogenic conditions where the assumption that the fluid in the vessel remains a gas becomes increasingly unlikely. The energy model given here can already accommodate variable heat transfer, for example \\(Q = k \\left( T - T_a \\right)\\), and it could be extended to include phase change by performing an isothermal flash calculation at each time step (and adjusting the enthalpy and internal energy calculations to account for the multiple phases). For a more realistic SCUBA tank model, this level of complexity isn’t needed, once a realistic heat transfer model is added the liquefaction problem would go away.\nSlower blowdowns, relative to the volume of the vessel, make more sense to model as always a gas. In these cases however, modelling the vessel as having no internal flow may be a serious limitation. Modelling the blowdown of pipeline segments, for example, without accounting for the frictional losses from internal flows leads to a significant error. I didn’t include an example of isothermal blowdowns here, but it is even easier to implement than the adiabatic case (for the pressure equation).\nI think there is a limited space between the pure ideal gas blowdown model and a full real fluid model with heat transfer &c. Most real situations either don’t require meticulously accounting for fluid non-ideality, and the ideal gas model works well enough, or are complex enough that a realistic model that includes phase change and heat transfer is required. However, building up from the ideal gas case step by step offers multiple points where the intermediate steps can be checked against known solutions. This is a useful exercise when building complex models, which can otherwise be difficult to test and troubleshoot."
  },
  {
    "objectID": "projects/unitfulcorrelations_jl/index.html",
    "href": "projects/unitfulcorrelations_jl/index.html",
    "title": "UnitfulCorrelations.jl",
    "section": "",
    "text": "A simple macro for working with empirical correlations and Unitful."
  },
  {
    "objectID": "projects/unitfulcorrelations_jl/index.html#installation",
    "href": "projects/unitfulcorrelations_jl/index.html#installation",
    "title": "UnitfulCorrelations.jl",
    "section": "Installation",
    "text": "Installation\nUnitfulCorrelations.jl can be installed using Julia’s built-in package manager. In a Julia session, enter the package manager mode by hitting ], then run the command\npkg&gt; add https://github.com/aefarrell/UnitfulCorrelations.jl"
  },
  {
    "objectID": "projects/unitfulcorrelations_jl/index.html#examples",
    "href": "projects/unitfulcorrelations_jl/index.html#examples",
    "title": "UnitfulCorrelations.jl",
    "section": "Examples",
    "text": "Examples\nSuppose you have an empirical correlation \\(f(x) = 0.92 x^{0.2}\\), where it is given that \\(x\\) is in meters and \\(f\\) is in seconds. You could figure out the units that the constants must have to make everything work out, or write a function that uses ustrip() to manage units, but that can get tedious if there are a lot of these.\nThe macro @ucorrel does this for you, adding another method for the case where the function is called with units. The arguments are: function (or function block), input units, output units.\n\nf(x) = 0.92*x^0.2\n@ucorrel f u\"m\" u\"s\"\n\n# f(2) == 1.0568024865972723\n# f(2 u\"m\") == 1.0568024865972723 u\"s\"\nthis can also be done with a function block\n\n@ucorrel function f(x)\n    return 0.92*x^0.2\nend u\"m\" u\"s\"\n\n# f(2) == 1.0568024865972723\n# f(2 u\"m\") == 1.0568024865972723 u\"s\"\nSo far it only supports one dimensional correlations, because I have basically just copied over a macro that I use frequently and have not added anything to its functionality."
  },
  {
    "objectID": "projects/gas_dispersion_jl/index.html",
    "href": "projects/gas_dispersion_jl/index.html",
    "title": "GasDispersion.jl",
    "section": "",
    "text": "GasDispersion.jl aims to bring together several models for dispersion modelling of chemical releases with a consistent interface. Currently it implements several Gaussian dispersion models, the Britter-McQuaid dense gas dispersion model, a subset of the SLAB dense gas dispersion model, and others."
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "A Chemical Engineer's Notebook",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nThe Ooms Plume Model\n\n\n\njulia\n\ndispersion modelling\n\nintegral plume models\n\n\n\nAn integral plume model for buoyant plumes.\n\n\n\nAllan Farrell\n\n\nJun 15, 2025\n\n\n\n\n\n\n\n\n\n\n\nLogging data from an Atmotube PRO over Bluetooth\n\n\n\npython\n\nair quality\n\natmotube\n\n\n\nHaving fun with data logging.\n\n\n\nAllan Farrell\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nMapping Pollen Dispersion\n\n\n\njulia\n\ndispersion modelling\n\npollen\n\n\n\nCalculating how far the wind blows.\n\n\n\nAllan Farrell\n\n\nMay 10, 2025\n\n\n\n\n\n\n\n\n\n\n\nVessel Blowdown - Real Gases\n\n\n\njulia\n\ncompressible flow\n\nblowdown\n\nequations of state\n\n\n\nModelling vessel blowdowns using equations of state.\n\n\n\nAllan Farrell\n\n\nMar 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nVessel Blowdown - Ideal Gases\n\n\n\njulia\n\ncompressible flow\n\nblowdown\n\n\n\nEvaluating approaches to ideal gas blowdowns.\n\n\n\nAllan Farrell\n\n\nJan 24, 2025\n\n\n\n\n\n\n\n\n\n\n\nRelief Valve Sizing with Real Gases\n\n\n\njulia\n\npressure relief\n\ncompressible flow\n\nequations of state\n\n\n\nCompressible orifice flow calculations using equations of state.\n\n\n\nAllan Farrell\n\n\nOct 28, 2024\n\n\n\n\n\n\n\n\n\n\n\nModelling Hydrogen Releases Using HyRAM+\n\n\n\npython\n\nhydrogen\n\ndispersion modelling\n\n\n\nHydrogen plume modelling and indoor accumulation.\n\n\n\nAllan Farrell\n\n\nSep 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlastics Recycling and Microplastics\n\n\n\nplastic\n\n\n\nIs plastic recycling a huge source of microplastics?\n\n\n\nAllan Farrell\n\n\nJul 14, 2024\n\n\n\n\n\n\n\n\n\n\n\nEngineering a Cup of Coffee Part Two: Espresso\n\n\n\njulia\n\ncoffee\n\nmass transfer\n\n\n\nModelling espresso bed extraction.\n\n\n\nAllan Farrell\n\n\nMar 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating the impact of fugitive emissions\n\n\n\njulia\n\nhydrogen\n\ncompressible flow\n\n\n\nEvaluating the zero emissions fuel.\n\n\n\nAllan Farrell\n\n\nJan 3, 2024\n\n\n\n\n\n\n\n\n\n\n\nImpossible bowling\n\n\n\npython\n\nbowling\n\n\n\nLooking for impossible bowling games.\n\n\n\nAllan Farrell\n\n\nNov 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nMessing around with model parameters\n\n\n\njulia\n\ndispersion modelling\n\n\n\nThe importance of choosing the right references.\n\n\n\nAllan Farrell\n\n\nOct 30, 2023\n\n\n\n\n\n\n\n\n\n\n\nEngineering a Cup of Coffee\n\n\n\njulia\n\ncoffee\n\nmass transfer\n\n\n\nBetter coffee through chemical engineering.\n\n\n\nAllan Farrell\n\n\nSep 15, 2023\n\n\n\n\n\n\n\n\n\n\n\nMonitoring smoke infiltration\n\n\n\njulia\n\nair quality\n\natmotube\n\nbuilding infiltration\n\n\n\nBetter indoor air quality through data.\n\n\n\nAllan Farrell\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nTaking a second look at the Britter-McQuaid model\n\n\n\njulia\n\ndispersion modelling\n\n\n\nRe-evaluating plume extents and determining the explosive mass\n\n\n\nAllan Farrell\n\n\nMar 12, 2023\n\n\n\n\n\n\n\n\n\n\n\nIntegrating a Gaussian puff - mistakes were made\n\n\n\njulia\n\ndispersion modelling\n\n\n\nSuccessive approximations to … an integrated gaussian puff model.\n\n\n\nAllan Farrell\n\n\nJan 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nDynamic Mode Decomposition\n\n\n\njulia\n\ndynamical systems\n\n\n\nDynamic mode decomposition of fluid flow problems.\n\n\n\nAllan Farrell\n\n\nDec 18, 2022\n\n\n\n\n\n\n\n\n\n\n\nHydrogen Blending\n\n\n\njulia\n\nhydrogen\n\ncompressible flow\n\n\n\nBlending hydrogen into natural gas.\n\n\n\nAllan Farrell\n\n\nNov 10, 2022\n\n\n\n\n\n\n\n\n\n\n\nAdiabatic Compressible Flow in a Pipe\n\n\n\njulia\n\ncompressible flow\n\n\n\nEvaluating different models of adiabatic pipe flow.\n\n\n\nAllan Farrell\n\n\nSep 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nBetween a puff and a plume\n\n\n\njulia\n\ndispersion modelling\n\n\n\nAn integrated Gaussian puff model\n\n\n\nAllan Farrell\n\n\nJun 10, 2022\n\n\n\n\n\n\n\n\n\n\n\nMore on Turbulent Jets\n\n\n\njulia\n\ndispersion modelling\n\nturbulent jets\n\n\n\nCalculating concentrations, temperatures, and flow rates.\n\n\n\nAllan Farrell\n\n\nMay 8, 2022\n\n\n\n\n\n\n\n\n\n\n\nTurbulent Jets\n\n\n\njulia\n\ndispersion modelling\n\nturbulent jets\n\n\n\nNotes on turbulent jets and velocity profiles.\n\n\n\nAllan Farrell\n\n\nApr 8, 2022\n\n\n\n\n\n\n\n\n\n\n\nThe 2021 Canadian Federal Election\n\n\n\njulia\n\nelections\n\n\n\nAn analysis of how exceptionally little changed.\n\n\n\nAllan Farrell\n\n\nSep 22, 2021\n\n\n\n\n\n\n\n\n\n\n\nSmoke Days\n\n\n\njulia\n\nair quality\n\n\n\nFrequency of forest fire smoke events.\n\n\n\nAllan Farrell\n\n\nJul 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Infiltration Example – Chlorine Release\n\n\n\njulia\n\ndispersion modelling\n\nbuilding infiltration\n\nhazard screening\n\n\n\nSingle zone building infiltration model with an instantaneous release\n\n\n\nAllan Farrell\n\n\nJun 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Infiltration Example\n\n\n\njulia\n\nair quality\n\nbuilding infiltration\n\n\n\nSingle zone building infiltration of forest fire smoke.\n\n\n\nAllan Farrell\n\n\nMay 22, 2021\n\n\n\n\n\n\n\n\n\n\n\nTurbulent Jet Example - Acetylene Leak\n\n\n\njulia\n\nchemical releases\n\nhazard screening\n\ndispersion modelling\n\nturbulent jets\n\n\n\nEstimating the explosive mass.\n\n\n\nAllan Farrell\n\n\nApr 10, 2021\n\n\n\n\n\n\n\n\n\n\n\nVCE Example - Butane Vapour Cloud\n\n\n\njulia\n\nchemical releases\n\nhazard screening\n\ndispersion modelling\n\nexplosions\n\n\n\nUsing the Baker-Strehlow-Tang model for a vapour cloud explosion.\n\n\n\nAllan Farrell\n\n\nJan 9, 2021\n\n\n\n\n\n\n\n\n\n\n\nWorst Case Meterological Conditions\n\n\n\njulia\n\ndispersion modelling\n\n\n\nThe worst case weather conditions for air dispersion modeling.\n\n\n\nAllan Farrell\n\n\nDec 12, 2020\n\n\n\n\n\n\n\n\n\n\n\nAir Dispersion Example - Gaussian Dispersion Model of Stack Emissions\n\n\n\njulia\n\nhazard screening\n\ndispersion modelling\n\n\n\nEstimating the airborne quantity.\n\n\n\nAllan Farrell\n\n\nDec 5, 2020\n\n\n\n\n\n\n\n\n\n\n\nCompressible Flow Example - Sizing a Goose Neck Vent\n\n\n\njulia\n\ncompressible flow\n\npressure relief\n\n\n\nCalculating the minimum diameter in incompressible, isothermal, and adiabatic flow situations.\n\n\n\nAllan Farrell\n\n\nNov 28, 2020\n\n\n\n\n\n\n\n\n\n\n\nChemical Release Screening Example - Butane leak\n\n\n\njulia\n\nchemical releases\n\nhazard screening\n\n\n\nEstimating the airborne quantity.\n\n\n\nAllan Farrell\n\n\nNov 20, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/atmotube_data_logging/index.html",
    "href": "posts/atmotube_data_logging/index.html",
    "title": "Logging data from an Atmotube PRO over Bluetooth",
    "section": "",
    "text": "I have had an Atmotube Pro for a few years, mostly using it during the summer to keep an eye on poor air quality during wildfire smoke events. I often export data from it, as a csv, to noodle around, but I haven’t really looked at how to log data directly from it with my laptop. Atmotube provides documentation on the bluetooth API and a guide for how to set up an MQTT router. But I couldn’t really find anything on just logging data from it directly, using python.\nThus my project for the Victoria Day long weekend was to figure out how to collect data from my atmotube using python. This works on my laptop but could, presumably, be ported to something like a raspberry pi easily enough."
  },
  {
    "objectID": "posts/atmotube_data_logging/index.html#requesting-data-with-gatt",
    "href": "posts/atmotube_data_logging/index.html#requesting-data-with-gatt",
    "title": "Logging data from an Atmotube PRO over Bluetooth",
    "section": "Requesting data with GATT",
    "text": "Requesting data with GATT\nThe Atmotube documentation gives two main ways of getting data from the device: using GATT or just passively from the advertising data the Atmotube broadcasts when it isn’t connected to anything (the BLE advertising packet). The most straightforward, to retrieve something specific, is via GATT.\nI am going to be using Bleak to scan and connect to BLE devices. To start I need a BleakScanner to scan for devices and, once I have found the one I want, connect to it as a BleakClient. Then, to make the various requests, I need the corresponding UUIDs – these correspond to specific packets of data as described in the docs\n\nimport time\n\n\nfrom bleak import BleakScanner, BleakClient\n\n\n# some constants\nATMOTUBE      = \"C2:2B:42:15:30:89\" # the mac address of my Atmotube\nSGPC3_UUID    = \"DB450002-8E9A-4818-ADD7-6ED94A328AB4\"\nBME280_UUID   = \"DB450003-8E9A-4818-ADD7-6ED94A328AB4\"\nSPS30_UUID    = \"DB450005-8E9A-4818-ADD7-6ED94A328AB4\"\nSTATUS_UUID   = \"DB450004-8E9A-4818-ADD7-6ED94A328AB4\"\n\nThe function scan_and_connect scans for the device which matches the mac address of my Atmotube, then proceeds to request each of the four packets of data. This simply returns a tuple with the data and the timestamp.\n\nasync def scan_and_connect(address):\n    device = await BleakScanner.find_device_by_address(address)\n    if not device:\n        print(\"Device not found\")\n        return None\n\n    async with BleakClient(device) as client:\n        stat = await client.read_gatt_char(STATUS_UUID)\n        bme = await client.read_gatt_char(BME280_UUID)\n        sgp = await client.read_gatt_char(SGPC3_UUID)\n        sps = await client.read_gatt_char(SPS30_UUID)\n        ts = time.time()\n        return (ts, stat, bme, sgp, sps)\n\nI can connect and get a single data point, but what I have is a timestamp and a collection of bytes. It is not cleaned up and readable in any way.\n\nres = await scan_and_connect(ATMOTUBE)\n\nThe easiest way to unpack a sequence of bytes is to use the struct standard library. But there are two exceptions:\n\nThe info byte is 8-bits where each bit corresponds to a particular flag. I could pull out each bit one by one using bit-shifting or something, but using a ctype struct lets me map the whole two-byte status characteristic into the various info flags and the battery state in one clean step.\n\n\nimport struct\n\n\nfrom ctypes import LittleEndianStructure, c_uint8, c_int8\n\nclass InfoBytes(LittleEndianStructure):\n    _fields_ = [\n                (\"pm_sensor\",    c_uint8, 1),\n                (\"error\",        c_uint8, 1),\n                (\"bonding\",      c_uint8, 1),\n                (\"charging\",     c_uint8, 1),\n                (\"charge_timer\", c_uint8, 1),\n                (\"bit_6\",        c_uint8, 1),\n                (\"pre_heating\",  c_uint8, 1),\n                (\"bit_8\",        c_uint8, 1),\n                (\"batt_level\",   c_uint8, 8),\n    ]\n\n\nThe PM characteristic is a 12-byte sequence where each set of 3-bytes is a 24-bit integer. This is not an integer type that is natively supported by python. I thought I could do the same thing as the Status characteristic and map it onto a ctype struct, but that didn’t work. As a work-around I collect each 3-byte sequence as arrays and convert each to an int as a two-step process. I could also have used int.from_bytes() directly, but I think this is a little neater and easier to read.\n\n\nclass PMBytes(LittleEndianStructure):\n    _fields_ = [\n        ('_pm1',   c_int8*3),\n        ('_pm2_5', c_int8*3),\n        ('_pm10',  c_int8*3),\n        ('_pm4',   c_int8*3), \n    ]\n    _pack_ = 1\n\n    @property\n    def pm1(self):\n        return int.from_bytes(self._pm1, 'little', signed=True)\n\n    @property\n    def pm2_5(self):\n        return int.from_bytes(self._pm2_5, 'little', signed=True)\n\n    @property\n    def pm10(self):\n        return int.from_bytes(self._pm10, 'little', signed=True)\n\nWith those two pieces out of the way, I define the actual variables I want – these are the column names I want to have in the final dataframe – and process the bytes. The first step is to use the InfoByte struct I defined above to pull out the flags and battery status, I add this to the results more for my own interest. Then I use struct.unpack() to unpack the integers from each byte-string and store the results.\nFinally I use the PMBytes class to process the PM data. If the sensor isn’t on the results are -1 and so I clean those out. The idea is to leave any blank readings as None, since that is easy to filter out with pandas later on.\n\nHEADERS = [\"Timestamp\", \"VOC\", \"RH\", \"T\", \"P\", \"PM1\", \"PM2.5\", \"PM10\"]\n\n\ndef process_gatt_data(data):\n    result = dict.fromkeys(HEADERS)\n    if res is not None:\n        ts, stat, bme, sgp, sps = data\n        result[\"Timestamp\"] = ts\n\n        # Info and Battery data\n        inf_bits = InfoBytes.from_buffer_copy(stat)\n        for (fld, _, _) in inf_bits._fields_:\n            result[f\"INFO.{fld}\"] = getattr(inf_bits, fld)\n        \n        # SGPC3 data format\n        tvoc, _ = struct.unpack('&lt;hh', sgp)\n        result[\"VOC\"] = tvoc/1000\n\n        # BME280 data format\n        rh, T, P, T_plus = struct.unpack('&lt;bblh', bme)\n        result[\"RH\"] = rh\n        result[\"T\"] = T_plus/100\n        result[\"P\"] = P/1000\n\n        # SPS30 data format\n        pms = PMBytes.from_buffer_copy(sps)\n        result[\"PM1\"] = pms.pm1/100 if pms.pm1 &gt; 0 else None\n        result[\"PM2.5\"] = pms.pm2_5/100 if pms.pm2_5 &gt; 0 else None\n        result[\"PM10\"] = pms.pm10/100 if pms.pm10 &gt; 0 else None\n\n    return result\n\nNow I can process the result I collected earlier.\n\nprocess_gatt_data(res)\n\n{'Timestamp': 1747673644.60206,\n 'VOC': 0.223,\n 'RH': 32,\n 'T': 21.3,\n 'P': 93.37,\n 'PM1': 1.0,\n 'PM2.5': 2.18,\n 'PM10': 3.27,\n 'INFO.pm_sensor': 1,\n 'INFO.error': 0,\n 'INFO.bonding': 0,\n 'INFO.charging': 0,\n 'INFO.charge_timer': 1,\n 'INFO.bit_6': 0,\n 'INFO.pre_heating': 1,\n 'INFO.bit_8': 0,\n 'INFO.batt_level': 63}\n\n\nThe results are what I expect for my apartment. In addition to the air quality data, we can see that the PM sensor was on and that the Atmotube had been charging recently.1 The pre-heat flag indicates that the device has completed any pre-heating and is ready. So everything looks good.\n1 I unplugged it before charging was done so it wouldn’t interfere with any temperature readings when I tested this code, that’s why the battery was only at 63%I could, at this point, just start a service or cron job to poll the device every so often and log the results. It will only return PM results when the atmotube is actively sampling, which could present some issues with timing. If the device is set to sample, for example, every 15 minutes and the script doesn’t make a request during that window, it will never return results. For everything that follows I set my atmotube to sample continuously."
  },
  {
    "objectID": "posts/atmotube_data_logging/index.html#collecting-broadcast-data",
    "href": "posts/atmotube_data_logging/index.html#collecting-broadcast-data",
    "title": "Logging data from an Atmotube PRO over Bluetooth",
    "section": "Collecting broadcast data",
    "text": "Collecting broadcast data\nThe other way of logging data from the atmotube is to pull it out of the advertising packet the atmotube broadcasts as a bluetooth device. In this case I don’t actually connect to the device, the scanner runs continuously and sends back any advertising data it finds using the adv_cb() callback function. This checks if the data came from my atmotube and, if it did, adds it to the results.\nThe scanner runs inside an event loop which starts the scanner, waits until the collection_time has elapsed, then shuts down and returns the results.\n\nimport asyncio\n\n\nasync def collect_data(device_mac, collection_time=600):\n    def adv_cb(device, advertising_data):\n        if device.address == device_mac:\n            results.append((time.time(), device, advertising_data))\n        else:\n            pass\n        return None\n    \n    async def receiver(event):\n        async with BleakScanner(adv_cb, scanning_mode='active') as scanner:\n            await event.wait()\n    \n    results = []\n    loop = asyncio.Event()\n    task = asyncio.create_task(receiver(loop))\n    await asyncio.sleep(collection_time)\n    loop.set()\n    _ = await asyncio.wait([task])\n    return results\n\nRunning this for 10 seconds lets me collect some example data to play with.\n\nbroadcasts = await collect_data(ATMOTUBE, 10)\n\nProcessing the advertising packet is similar to what was done with the GATT data, except that it comes in two flavours: the broadcast packet has the basic temperature, pressure, VOC, device status and the scan response packet contains the PM data and is shorter. Here the PM data is at a lower resolution – 16-bit integers – and so they can be unpacked using struct.unpack(). The GATT data returns the PM data to 2 decimal places (and the temperature to 1 decimal place), whereas the advertising packet data is rounded to the nearest whole number.\n\ndef process_adv_data(full_data, company_id=int(0xFFFF)):\n    result = dict.fromkeys(HEADERS)\n    if full_data is None:\n        return result\n    else:\n        timestamp, device, advertising_data = full_data\n        result[\"Timestamp\"] = timestamp\n\n        # process advertising data\n        data = advertising_data.manufacturer_data.get(company_id)\n        if len(data) == 12:\n            tvoc, devid, rh, T, P, inf, batt = struct.unpack(\"&gt;hhbblbb\", data)\n            result[\"VOC\"] = tvoc/1000\n            result[\"RH\"] = rh\n            result[\"T\"] = T\n            result[\"P\"] = P/1000\n        elif len(data) == 9:\n            pm1, pm2_5, pm10, fw_maj, fw_min, fw_bld = struct.unpack(\"&gt;hhhbbb\", data)\n            result[\"PM1\"] = pm1 if pm1 &gt; 0 else None\n            result[\"PM2.5\"] = pm2_5 if pm2_5 &gt; 0 else None\n            result[\"PM10\"] = pm10 if pm10 &gt; 0 else None\n        else:\n            pass\n        return result\n\nI can process this and look at examples of the two types of advertising packet\n\nprocess_adv_data(broadcasts[0])\n\n{'Timestamp': 1747673646.9507601,\n 'VOC': None,\n 'RH': None,\n 'T': None,\n 'P': None,\n 'PM1': 1,\n 'PM2.5': 2,\n 'PM10': 3}\n\n\n\nprocess_adv_data(broadcasts[5])\n\n{'Timestamp': 1747673647.2869163,\n 'VOC': 0.208,\n 'RH': 36,\n 'T': 21,\n 'P': 93.357,\n 'PM1': None,\n 'PM2.5': None,\n 'PM10': None}\n\n\nThe way I have this set up is very wasteful of memory if the atmotube is set-up to only sample periodically. In those cases there will be a lot of packets with no PM data that are being dutifully logged in results. By processing the data as it is retrieved, I can collect only the packets that had measurements in them.\n\nasync def better_collect_data(device_mac, collection_time=600):\n    def adv_cb(device, advertising_data):\n        if device.address == device_mac:\n            row = process_adv_data((time.time(), device, advertising_data))\n            if len( [ val for key, val in row.items() if val is not None ]) &gt;1:\n                # only collect results when we actually have a measurement\n                results.append(row)\n        else:\n            pass\n        return None\n    \n    async def receiver(event):\n        async with BleakScanner(adv_cb) as scanner:\n            await event.wait()\n    \n    results = []\n    loop = asyncio.Event()\n    task = asyncio.create_task(receiver(loop))\n    await asyncio.sleep(collection_time)\n    loop.set()\n    _ = await asyncio.wait([task])\n    return results\n\nWhich I let collect for 5 minutes\n\nnew_broadcasts = await better_collect_data(ATMOTUBE, 300)"
  },
  {
    "objectID": "posts/atmotube_data_logging/index.html#processing-the-broadcast-data",
    "href": "posts/atmotube_data_logging/index.html#processing-the-broadcast-data",
    "title": "Logging data from an Atmotube PRO over Bluetooth",
    "section": "Processing the broadcast data",
    "text": "Processing the broadcast data\nAt this point we want to actually look at the results and maybe do some stats. By logging the data as a list of dicts, transforming this into a dataframe is very straightforward.\n\nimport pandas as pd\n\n\ndf = pd.DataFrame(new_broadcasts)\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nTimestamp\nVOC\nRH\nT\nP\nPM1\nPM2.5\nPM10\n\n\n\n\ncount\n4.100000e+02\n57.000000\n57.000000\n57.0\n57.000000\n353.0\n353.000000\n353.000000\n\n\nmean\n1.747674e+09\n0.203228\n35.105263\n21.0\n93.351193\n1.0\n2.005666\n3.039660\n\n\nstd\n8.914660e+01\n0.002797\n0.450564\n0.0\n0.004576\n0.0\n0.184550\n0.246825\n\n\nmin\n1.747674e+09\n0.199000\n34.000000\n21.0\n93.343000\n1.0\n1.000000\n2.000000\n\n\n25%\n1.747674e+09\n0.201000\n35.000000\n21.0\n93.348000\n1.0\n2.000000\n3.000000\n\n\n50%\n1.747674e+09\n0.203000\n35.000000\n21.0\n93.351000\n1.0\n2.000000\n3.000000\n\n\n75%\n1.747674e+09\n0.204000\n35.000000\n21.0\n93.355000\n1.0\n2.000000\n3.000000\n\n\nmax\n1.747674e+09\n0.210000\n36.000000\n21.0\n93.361000\n1.0\n3.000000\n4.000000\n\n\n\n\n\n\n\nThis shows a real asymmetry in quantity of data found and what was in it – of 410 packets received 353 were PM data and 57 contained the VOC, temperature, etc. data.\n\ndf['Time'] = df['Timestamp'] - df.iloc[0]['Timestamp']\n\n\n\n\n\n\n\n\n\nFigure 1: Time series data of indoor VOC and PM concentrations, a 5 minute sample of BLE advertising data\n\n\n\n\n\nPlotting the timeseries data shows the PM data is very noisy – largely because it is rounding to the nearest whole integer. I also suspect that I should be cleaning up the scan responses better. Probably a lot of those are duplicates – it is not actually a fresh reading just rebroadcast of what had been read last. I’m not really sure."
  },
  {
    "objectID": "posts/atmotube_data_logging/index.html#logging-to-a-csv",
    "href": "posts/atmotube_data_logging/index.html#logging-to-a-csv",
    "title": "Logging data from an Atmotube PRO over Bluetooth",
    "section": "Logging to a CSV",
    "text": "Logging to a CSV\nIf you are only collecting 5 minutes of data, reading directly into memory like this is reasonable. But probably you want to log the data over a longer stretch of time, and it makes more sense to log the data to a csv – saving it more permanently. The following creates a new csv with the given filename then, for every valid packet processed, appends the results to the csv.\n\nimport csv\n\n\nasync def log_to_csv(device_mac, collection_time=600, file=\"atmotube.csv\"):\n    def adv_cb(device, advertising_data):\n        if device.address == device_mac:\n            row = process_adv_data((time.time(), device, advertising_data))\n            if len( [ val for key, val in row.items() if val is not None ]) &gt;1:\n                # only collect results when we actually have a measurement\n                with open(file, 'a', newline='') as csvfile:\n                    writer = csv.DictWriter(csvfile, fieldnames=HEADERS)\n                    writer.writerow(row)\n        else:\n            pass\n        return None\n    \n    async def receiver(event):\n        async with BleakScanner(adv_cb) as scanner:\n            await event.wait()\n    \n    # prepare csv file\n    with open(file, 'w', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=HEADERS)\n        writer.writeheader()\n\n    # start scanning\n    loop = asyncio.Event()\n    task = asyncio.create_task(receiver(loop))\n\n    # wait until the collection time is up\n    await asyncio.sleep(collection_time)\n    loop.set()\n    _ = await asyncio.wait([task])\n    \n    return True\n\n\n\n\n\n\n\nWarning\n\n\n\nThe callback function is doing a lot of work and blocking to write to a csv. This is, in general, not a good idea. When I put this together, I figured that the rate of new data from the Atmotube is significantly slower than the time required to process data and write it to a csv that it would be fine, and it was, but it isn’t really robust. A better solution might be to have the callback put the data into a queue and have a seperate worker process results into the csv.\n\n\nTo get this going, I just created a csv with the current timestep in the filename – so if I stop and start I don’t clobber previous data – and leave it to run for an hour. I just left this running in jupyter while I switched to a different desktop and went about my life, but a longer-term solution would be in a script that runs in the background.\n\nimport math\n\n\nnow = math.floor(time.time())\ntimestamped_file = f\"atmotube-{now}.csv\"\nresult = await log_to_csv(ATMOTUBE, 3600, timestamped_file)\n\nprint(\"Success!\") if result else print(\"Boo\")\n\nSuccess!\n\n\nWhile it is running, you can check on the progress with tail -f %filename, and watch the results come in live on the terminal. Once it is done, the csv can be read into pandas and plotted like before\n\nlogged_data = pd.read_csv(timestamped_file)\n\n\nlogged_data.describe()\n\n\n\n\n\n\n\n\nTimestamp\nVOC\nRH\nT\nP\nPM1\nPM2.5\nPM10\n\n\n\n\ncount\n4.823000e+03\n835.000000\n835.000000\n835.0\n835.000000\n3988.0\n3988.000000\n3988.000000\n\n\nmean\n1.747676e+09\n0.226522\n34.810778\n21.0\n93.320590\n1.0\n1.919007\n2.945587\n\n\nstd\n1.037307e+03\n0.012884\n0.711420\n0.0\n0.018019\n0.0\n0.328726\n0.325025\n\n\nmin\n1.747674e+09\n0.195000\n34.000000\n21.0\n93.283000\n1.0\n1.000000\n2.000000\n\n\n25%\n1.747675e+09\n0.217000\n34.000000\n21.0\n93.303000\n1.0\n2.000000\n3.000000\n\n\n50%\n1.747676e+09\n0.230000\n35.000000\n21.0\n93.325000\n1.0\n2.000000\n3.000000\n\n\n75%\n1.747677e+09\n0.237000\n35.000000\n21.0\n93.337000\n1.0\n2.000000\n3.000000\n\n\nmax\n1.747678e+09\n0.249000\n37.000000\n21.0\n93.355000\n1.0\n3.000000\n4.000000\n\n\n\n\n\n\n\n\nlogged_data['Time'] = logged_data['Timestamp'] - logged_data.iloc[0]['Timestamp']\n\n\n\n\n\n\n\n\n\nFigure 2: Time series data of indoor VOC and PM concentrations, a 1-hr sample of BLE advertising data\n\n\n\n\n\nThe atmotube is also logging data to its internal memory, so I exported that and plotted it against what was broadcast.\n\nexport_data = pd.read_csv('atmotube-export-data.csv')\nexport_data.describe()\n\n\n\n\n\n\n\n\nVOC, ppm\nAQS\nAir quality health index (AQHI) - Canada\nTemperature, °C\nHumidity, %\nPressure, kPa\nPM1, ug/m3\nPM2.5, ug/m3\nPM2.5 (avg 3h), ug/m3\nPM10, ug/m3\nPM10 (avg 3h), ug/m3\nLatitude\nLongitude\n\n\n\n\ncount\n66.000000\n66.000000\n66.0\n66.0\n66.000000\n66.000000\n66.0\n66.000000\n66.000000\n66.000000\n66.000000\n0.0\n0.0\n\n\nmean\n0.239985\n85.045455\n1.0\n21.0\n34.484848\n93.316364\n1.0\n1.530303\n1.559175\n2.545455\n2.861027\nNaN\nNaN\n\n\nstd\n0.018563\n1.156012\n0.0\n0.0\n0.769464\n0.019817\n0.0\n0.502905\n0.036129\n0.501745\n0.041909\nNaN\nNaN\n\n\nmin\n0.212000\n82.000000\n1.0\n21.0\n33.000000\n93.280000\n1.0\n1.000000\n1.466667\n2.000000\n2.722222\nNaN\nNaN\n\n\n25%\n0.228250\n85.000000\n1.0\n21.0\n34.000000\n93.300000\n1.0\n1.000000\n1.550000\n2.000000\n2.866667\nNaN\nNaN\n\n\n50%\n0.238000\n85.000000\n1.0\n21.0\n34.500000\n93.320000\n1.0\n2.000000\n1.561111\n3.000000\n2.877778\nNaN\nNaN\n\n\n75%\n0.244750\n86.000000\n1.0\n21.0\n35.000000\n93.337500\n1.0\n2.000000\n1.583333\n3.000000\n2.888889\nNaN\nNaN\n\n\nmax\n0.295000\n87.000000\n1.0\n21.0\n36.000000\n93.350000\n1.0\n2.000000\n1.616667\n3.000000\n2.888889\nNaN\nNaN\n\n\n\n\n\n\n\n\nfrom datetime import datetime\n\n\nexport_data['Timestamp'] = export_data[['Date']].apply(\n    lambda str: datetime.strptime(str.iloc[0], \"%Y/%m/%d %H:%M:%S\").timestamp(), axis=1)\n\n\nexport_data['Time'] = export_data['Timestamp'] - logged_data.iloc[0]['Timestamp']\n\n\n\n\n\n\n\n\n\nFigure 3: Time series data of indoor temperature and pressure, a 1-hr sample of BLE advertising data and data exported from the Atmotube app\n\n\n\n\n\nThe basic atmospheric data like temperature, pressure, and relative humidity appear to be the same. But there is something weird going on with the VOC measurements.\n\n\n\n\n\n\n\n\nFigure 4: Time series data of indoor VOC concentrations, a 1-hr sample of BLE advertising data and data exported from the Atmotube app\n\n\n\n\n\nI think the atmotube is actually exporting the rolling average of the VOC results over a fairly broad window, whereas the broadcast reading is more direct from the sensor. I would have to run this for much longer to see if that’s the case.\n\n\n\n\n\n\n\n\nFigure 5: Time series data of indoor PM2.5 concentrations, a 1-hr sample of BLE advertising data and data exported from the Atmotube app\n\n\n\n\n\nThe PM data shows the results are closer, but still have issues. The exported data is (I believe) a by-the-minute average, rounded to the nearest integer. There is a single data point for each minute in the dataset, giving 66 overall. Whereas the raw PM broadcast data has 3988 data points, and I think most of those are just rebroadcasts and are not “real”.\nOne thing I was thinking of doing was to capture only the first scan response packet after an advertising packet then ignore all the rest until the next advertising packet. I have also been ignoring the info flags since, when I was just noodling around, they didn’t seem to change at all (with the device always sampling), they might actually be telling me things that I’ve been ignoring."
  },
  {
    "objectID": "posts/atmotube_data_logging/index.html#final-thoughts",
    "href": "posts/atmotube_data_logging/index.html#final-thoughts",
    "title": "Logging data from an Atmotube PRO over Bluetooth",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nHopefully this helps you get set-up collecting data from your atmotube (I don’t know why else you would read this far). From here to building a simple dashboard or datalogger should be an easy weekend project. I think for applications where you want higher fidelity data over a long stretch of time, periodically requesting data using GATT makes the most sense. The PM data comes with more decimal places of precision, and you don’t need it more frequently than every minute or so.\nThe BLE advertising data could be an easy way of building a passive dashboard, continuously listening and updating the air quality statistics. Though some effort would need to be put in cleaning up the data, or perhaps just presenting a rolling average of some kind to smooth out the noise.\nThere is also a whole section of the documentation on connecting to an atmotube and downloading data from it, which I didn’t bother to investigate. It looked overly complicated for what I wanted to do. If you figure that out, please let me know!"
  },
  {
    "objectID": "posts/atmotube_data_logging/index.html#update",
    "href": "posts/atmotube_data_logging/index.html#update",
    "title": "Logging data from an Atmotube PRO over Bluetooth",
    "section": "Update",
    "text": "Update\nI was thinking about this more and there was one avenue I neglected to explore: subscribing to GATT notifications from the atmotube. Instead of requesting a single data point, like I did above, one can subscribe to the feed and the atmotube will just send packets whenever an update occurs. That is what I do below.\nTo get started I decided to make cytpe structs for each of the bytestrings that can be returned. I don’t think this is necessary, but I like how it seperates the logic of decoding the response on an aesthetic level. It also makes it very clear how the bytestrings are structured.\n\nfrom ctypes import LittleEndianStructure, c_ubyte, c_byte, c_short, c_int\n\nclass StatusData(LittleEndianStructure):\n    _fields_ = [\n                (\"pm_sensor\",          c_ubyte, 1),\n                (\"error\",              c_ubyte, 1),\n                (\"bonding\",            c_ubyte, 1),\n                (\"charging\",           c_ubyte, 1),\n                (\"charging_timer\",     c_ubyte, 1),\n                (\"_bit_6\",             c_ubyte, 1),\n                (\"sgpc3_pre_heating\",  c_ubyte, 1),\n                (\"_bit_8\",             c_ubyte, 1),\n                (\"battery_level\",      c_ubyte, 8),\n    ]\n\n    def __new__(cls, ts, data):\n        return cls.from_buffer_copy(data)\n\n    def __init__(self, ts, data):\n        self.timestamp = ts\n\n\nclass SPS30Data(LittleEndianStructure):\n    _fields_ = [\n        ('_pm1',   c_byte*3),\n        ('_pm2_5', c_byte*3),\n        ('_pm10',  c_byte*3),\n        ('_pm4',   c_byte*3), \n    ]\n    _pack_ = 1\n\n    def __new__(cls, ts, data):\n        return cls.from_buffer_copy(data)\n\n    def __init__(self, ts, data):\n        self.timestamp = ts\n\n    @property\n    def pm1(self):\n        res = int.from_bytes(self._pm1, 'little', signed=True)\n        return res/100 if res &gt; 0 else None\n\n    @property\n    def pm2_5(self):\n        res = int.from_bytes(self._pm2_5, 'little', signed=True)\n        return res/100 if res &gt; 0 else None\n\n    @property\n    def pm10(self):\n        res = int.from_bytes(self._pm10, 'little', signed=True)\n        return res/100 if res &gt; 0 else None\n\n\nclass BME280Data(LittleEndianStructure):\n    _fields_ = [\n        ('_rh',    c_byte),\n        ('_T',     c_byte),\n        ('_P',     c_int),\n        ('_T_dec', c_short),\n        ]\n    _pack_ = 1\n\n    def __new__(cls, ts, data):\n        return cls.from_buffer_copy(data)\n\n    def __init__(self, ts, data):\n        self.timestamp = ts\n   \n    @property\n    def RH(self):\n        return self._rh\n\n    @property\n    def T(self):\n        return self._T_dec/100\n\n    @property\n    def P(self):\n        return self._P/1000\n\n\nclass SGPC3Data(LittleEndianStructure):\n    _fields_ = [\n        ('_TVOC',    c_short),\n        ]\n    _pack_ = 1\n\n    def __new__(cls, ts, data):\n        return cls.from_buffer_copy(data)\n\n    def __init__(self, ts, data):\n        self.timestamp = ts\n\n    @property\n    def TVOC(self):\n        return self._TVOC/1000\n\nWith that out of the way, there are two other components I need for this to work: a collector which will collect all of the data sent back from the atmotube and a worker which will log it to a csv. Unlike above, where I logged each advertising packet as it came in, I am going to make these run asynchronously using asyncio. I think this is what really should be done, instead of blocking for file i/o every time a callback function is triggered.\nTo make this happen I largely copied what was done in this example which uses an async queue to pass data between the two workers. The basic idea is:\n\nThe collector starts up and scans for the atmotube, by MAC address.\nWhen it finds the device it requests notifications for one of the GATT characteristics, in this case I am requesting the status data and the SPS30 data, which contains the pm concentrations.\nThe collector then waits around for the collection_time and every time it gets a new set of data uses the callbacks status_cb and sps30_cb to process the bytestring and put the result on the queue\n\n\nasync def collect_data(mac, queue, collection_time):\n    async def status_cb(char, data):\n        ts = time.time()\n        res = StatusData(ts, data)\n        await queue.put(res)\n\n    async def sps30_cb(char, data):\n        ts = time.time()\n        res = SPS30Data(ts, data)\n        await queue.put(res)\n    \n    device = await BleakScanner.find_device_by_address(mac)\n    if not device:\n        raise Exception(\"Device not found\")\n    \n    async with BleakClient(device) as client:\n        # start notifications\n        await client.start_notify(STATUS_UUID, status_cb)\n        await client.start_notify(SPS30_UUID, sps30_cb)\n\n        # wait for collection period to end\n        await asyncio.sleep(collection_time)\n\n        # signals end of queue\n        await queue.put(None)\n\nConcurrently with that, a logger needs to write things to a csv. The basic idea is this:\n\nWhen the logger starts it creates a new csv file with the given filename, and writes the column headers.\nThe worker waits for data to appear on the queue and, once it does, takes it out (first in first out).\nThe result from the queue is lined up to the right columns in the csv, I check for the attribute battery_level as a lazy check of which type of result it is.\nFinally the worker writes the result as new row on the csv.\nIf the result is None, that is a signal that the collector has finished and the loop exits.\nRegardless, once the logger has processed the data from the queue, it calls task_done() to notify the queue of this and the loop begins again.\n\n\nimport csv\n\n\nHEADERS = [\"Timestamp\", \"PM Sensor\", \"PM1\", \"PM2.5\", \"PM10\"]\n\n\nasync def write_row(filename,row):\n    async with aiofiles.open(filename, 'a', newline='') as csvfile:\n        writer = aiocsv.AsyncDictWriter(csvfile, fieldnames=HEADERS)\n        await writer.writerow(row)\n\n\nasync def log_to_csv(filename, queue):\n    # prepare csv file\n    async with aiofiles.open(filename, 'w', newline='') as csvfile:\n        writer = aiocsv.AsyncDictWriter(csvfile, fieldnames=HEADERS)\n        await writer.writeheader()\n\n    # log data from queue\n    flag = True\n    while flag:\n        result = await queue.get()\n        if result is not None:\n            # we have some data to write\n            row = dict.fromkeys(HEADERS)\n            row[\"Timestamp\"] = result.timestamp\n            if hasattr(result, \"battery_level\"):\n                # we have a status type\n                row[\"PM Sensor\"] = result.pm_sensor\n            else:\n                # we have pm data\n                row[\"PM1\"] = result.pm1\n                row[\"PM2.5\"] = result.pm2_5\n                row[\"PM10\"] = result.pm10\n\n            await write_row(filename,row)\n        else:\n            # the end of the queue\n            flag = False\n        queue.task_done()\n\nMy first attempt at this I put the while loop inside the with block, so the whole thing ran inside the file context manager. This had the effect of nothing actually being written to the csv until the with block exited and the file closed. It took me a long time to realize that is what was happening, since it looked exactly the same as the two processes running sequentially: collect all the data and then write it all to csv.\nIn this version, every time a row is added to the csv the file is opened, a line is written, and then it is closed. There is probably a way of holding it open while logging, but that might make things more complicated since a whole bunch of new logic would be needed to catch any exceptions and ensure that the file is closed properly – something that happens behind the scenes with a simple with block.\nFinally, I put it all together with a simple sequence of tasks:\n\nCreate an empty asyncio Queue\nStart the logger, the worker that logs results to the csv\nStart the collector, the worker that collects packets from the atmotube\nWait for the collector to finish, then close.\n\n\nasync def save_data(mac, csv, collection_time):\n    q = asyncio.Queue()\n    \n    logger = asyncio.create_task(log_to_csv(csv, q))\n    collector = asyncio.ensure_future(collect_data(mac, q, collection_time))\n    \n    await collector\n\nI ran this for an hour in the background as a test and it seems to work fine.\nawait save_data(ATMOTUBE, f\"atmotube-{math.floor(time.time())}.csv\", 3600)\n\ndf = pd.read_csv(\"atmotube-1748482080.csv\")\n\n\ndf['Time'] = df['Timestamp'] - df.iloc[0]['Timestamp']\n\n\ndf.head()\n\n\n\n\n\n\n\n\nTimestamp\nPM Sensor\nPM1\nPM2.5\nPM10\nTime\n\n\n\n\n0\n1.748482e+09\nNaN\n10.92\n13.43\n14.97\n0.000000\n\n\n1\n1.748482e+09\nNaN\n10.93\n13.03\n14.66\n2.610108\n\n\n2\n1.748482e+09\nNaN\n11.05\n13.42\n15.19\n5.220881\n\n\n3\n1.748482e+09\nNaN\n11.35\n13.75\n15.34\n7.784888\n\n\n4\n1.748482e+09\nNaN\n11.59\n14.13\n15.50\n10.395001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Time series data of indoor PM2.5 concentrations, a 1-hr sample using GATT notifications\n\n\n\n\n\nWith no context it looks like something is horribly wrong, what are all those gaps in the data? My atmotube is set to only sample every 15 minutes, this is usually how I leave it to save on battery. This also explains some of the weirdness with the data, why does each sample start with a rapidly increasing concentration before levelling out? The atmotube is returning data right when the sampling fan has just turned on; this is not yet an accurate sample of the ambient air, it is the stagnant air inside the atmotube. This is a much more obvious problem with VOC data, it is clearly visible on the app as a funky saw-tooth wave where the VOC concentration plunges whenever the fan starts and, once it stops, slowly creeps up. It is an artifact of how the atmotube is sampling the air, not of how the data is being collected.\nIf the atmotube is set to always on mode, these artifacts go away, but if you want to monitor it in other configurations it is worth considering how the data should be cleaned up. For example watching for the pm_sensor flag to turn on then throwing out the first ~30s of pm data before looking at the rest. The GATT notifications make it very clear when the atmotube is sampling and when it isn’t. There will be a notification that pm_sensor has turned from 0 to 1, then data will start arriving with pm data, then a notification that the pm_sensor has turned from 1 to 0, followed by an empty row of pm data. See a snippet of the csv below. Note that pm_sensor values and actual pm values are always on seperate rows.\n\ndf[36:42]\n\n\n\n\n\n\n\n\nTimestamp\nPM Sensor\nPM1\nPM2.5\nPM10\nTime\n\n\n\n\n36\n1.748482e+09\nNaN\n12.42\n14.34\n15.23\n127.802146\n\n\n37\n1.748482e+09\n0.0\nNaN\nNaN\nNaN\n130.322095\n\n\n38\n1.748482e+09\nNaN\nNaN\nNaN\nNaN\n130.322191\n\n\n39\n1.748483e+09\n1.0\nNaN\nNaN\nNaN\n1035.107224\n\n\n40\n1.748483e+09\nNaN\n7.74\n9.55\n10.21\n1040.146906\n\n\n41\n1.748483e+09\nNaN\n7.81\n9.82\n11.40\n1042.621936\n\n\n\n\n\n\n\nIn addition to some data-wrangling, there are some other obvious upgrades to my code before it would be ready to deploy in an app. For one, there is minimal error handling. Any malformed bytestrings returned by the atmotube will throw an exception and kill everything. Additionally there are no checks to maintain a connection to the atmotube. It would simply timeout, having collected nothing. If you were planning on running this passively for a long period of time, unattended, that could be a big deal."
  },
  {
    "objectID": "posts/building_infiltration_2/index.html",
    "href": "posts/building_infiltration_2/index.html",
    "title": "Building Infiltration Example – Chlorine Release",
    "section": "",
    "text": "In a previous notebook, I explored building infiltration due to forest fire smoke and noted that ambient conditions would impact the scenario, in this scenario I continue exploring building infiltration and examine the sensitivity of infiltration to changes in ambient conditions. In this case the scenario is a release of chlorine from a storage cylinder creating a cloud that moves downwind to a building. We would like to know what impact this has on the interior conditions of the building while also taking the opportunity to evaluate the impact of changes in ambient weather conditions."
  },
  {
    "objectID": "posts/building_infiltration_2/index.html#the-scenario",
    "href": "posts/building_infiltration_2/index.html#the-scenario",
    "title": "Building Infiltration Example – Chlorine Release",
    "section": "The Scenario",
    "text": "The Scenario\nThe scenario is the catastrophic failure of a liquid chlorine cylinder, perhaps one being used as part of a water treatment facility. The release is outdoors and a small building is downwind of the release, which can be occupied and so the infiltration of chlorine is important.\n\nRelease Parameters\nFor simplicity suppose the entire contents of the cylinder are released essentially immediately and form a neutrally buoyant cloud. The mass of the release is the mass of chlorine in the cylinder which we can assume to be 68kg. CAMEO Chemicals lists the three emergency response planning guideline (ERPG) levels for chlorine as 1ppm, 3ppm, and 20ppm respectively.\nSuppose the release is 1m off the ground and is otherwise the center of the coordinate system.\n\n\nBuilding Parameters\nThe building is a small one story structure 100m downwind of the release point. For the scenario it will be taken as a given that the building volume is 255 m³ and the equivalent leak area is 690 cm². For simplicity any obstructions around the building are ignored.\n\n\nWeather Parameters\nFor neutrally buoyant gaussian dispersion a class F Pasquill stability is the worst case scenario, this leads to the least dispersion and thus greatest concentrations downwind. The windspeed is initially assumed to be 1.5 m/s.\n\n\n\nm\n68 kg\n\n\nerpg-1\n1 ppm\n\n\nerpg-2\n3 ppm\n\n\nerpg-3\n20 ppm\n\n\nh\n1 m\n\n\nd\n100 m\n\n\n\\(A_l\\)\n6.90×10⁻² m²\n\n\nV\n255 m³\n\n\nstability\nF\n\n\nu\n1.5 m/s\n\n\n\n\nusing Contour, Plots, OrdinaryDiffEq, Statistics, SpecialFunctions, ForwardDiff, Roots\n\n\nm     = 68.0   # mass of chlorine released, kg\nMW    = 70.9   # molar mass chlorine, kg/kmol\nMVC   = 24.465 # molar volume at 25C and 1atm, m3/kmol\n\nppm_to_kg(ppm) = (ppm*1e-6)*(MW/MVC)\nerpg1 = ppm_to_kg(1) # erpgs for chlorine, kg/m3\nerpg2 = ppm_to_kg(3)\nerpg3 = ppm_to_kg(20)\n\nh   = 1.0      # height of release point, m\n\nd   = 100.0    # downwind distance to building, m\nAₗ  = 6.90e-2  # equivalent leak area of building, m2\nV   = 255.0    # volume of building, m3\n\nu   = 1.5      # windspeed m/s"
  },
  {
    "objectID": "posts/building_infiltration_2/index.html#air-dispersion",
    "href": "posts/building_infiltration_2/index.html#air-dispersion",
    "title": "Building Infiltration Example – Chlorine Release",
    "section": "Air Dispersion",
    "text": "Air Dispersion\n\nGaussian Puff Model\nThe simplest model for a neutrally buoyant cloud is a gaussian disperison model, in this case because the cylinder is assumed to fail catastrophically the release can be treated as instantaneous and so we use a gaussian puff model.\nIt is often worth-while to estimate the initial dimensions of the cloud and then calculate a virtual emission point from which the release is assumed to take place. This is especially useful if the area immediately around the release point is of interest as the gaussian model assumes all of the mass is initially concentrated in a single point. However for a simple screening just using the default dispersion model is likely fine, and more conservative. The model gives the concentration as a gaussian distribution in the x, y, and z directions, while also adding in a term to account for ground reflection (mass cannot disperse below groundlevel)1\n1 AIChE/CCPS, Guidelines for Use of Vapour Cloud Dispersion Models, 75.\\[ c_{puff}(x,y,z,t) = { m \\over { (2 \\pi)^{3/2} \\sigma_x \\sigma_y \\sigma_z } }\n\\exp \\left( -\\frac{1}{2} \\left( {x - ut} \\over \\sigma_x \\right)^2 \\right)\n\\exp \\left( -\\frac{1}{2} \\left( {y} \\over \\sigma_y \\right)^2 \\right) \\]\n\\[ \\times \\left[ \\exp \\left( -\\frac{1}{2} \\left( {z - h} \\over \\sigma_z \\right)^2 \\right) + \\exp \\left( -\\frac{1}{2} \\left( {z + h} \\over \\sigma_z \\right)^2 \\right)\\right]\\]\nThe parameters σx, σy, and σz are generally found using empirically derived correlations that are functions of the downwind distance to the center of the puff xc and atmospheric stability.\n\nfunction c_puff(x,y,z,t; # point in space\n                m, u, h, # parameters of the problem\n                σx::Function, σy::Function, σz::Function)\n    xc = u*t # center of the cloud\n    sx = σx(xc)\n    sy = σy(xc)\n    sz = σz(xc)\n    \n    C1 = m / ( (2*π)^(1.5) * sx * sy * sz )\n    C2 = exp(-0.5*((x-xc)/sx)^2)\n    C3 = exp(-0.5*(y/sy)^2)\n    C4 = ( exp(-0.5*((z-h)/sz)^2) + exp(-0.5*((z+h)/sz)^2) )\n    \n    c = C1*C2*C3*C4\n    \n    return isnan(c) ? 0.0 : c\nend\n\n\n\nDispersion Parameters\nThe dispersion parameters for puff models are not, in general, as well developed as for plume models, the following values were interpolated from a sparser set of correlations and it is worth keeping in mind. It is also worth noting that the dispersion parameters are where the impact of different windspeeds will be made most apparent as stability is a function of windspeed.\n\nDispersion parameters for a Gaussian puff model2\n\n\n\n\n\n\n\n\n\nStability\n\\(\\sigma_x = \\sigma_y\\)\n\\(\\sigma_z\\)\nMax Windspeed\n\n\n\n\nA\n$ 0.18 x^{0.92} $\n$ 0.60 x^{0.75} $\n3 m/s\n\n\nB\n$ 0.14 x^{0.92} $\n$ 0.53 x^{0.73} $\n5 m/s\n\n\nC\n$ 0.10 x^{0.92} $\n$ 0.34 x^{0.71} $\n6 m/s\n\n\nD\n$ 0.06 x^{0.92} $\n$ 0.15 x^{0.70} $\n\n\n\nE\n$ 0.04 x^{0.92} $\n$ 0.10 x^{0.65} $\n5 m/s\n\n\nF\n$ 0.02 x^{0.89} $\n$ 0.05 x^{0.61} $\n3 m/s\n\n\n\n\n# Class F \n\nσx(x) = 0.02*x^0.89\nσy(x) = 0.02*x^0.89\nσz(x) = 0.05*x^0.61\n\n\n2 AIChE/CCPS, Guidelines for Use of Vapour Cloud Dispersion Models.\n\nOutdoor Concentration\nThe outdoor concentration can be calculated at any point using the above equations and below is an animation showing the cloud moving from the release point down to the location of the center of the building. The horizontal slice is at 1m elevation, and the vertical slice is at 0m crosswind distance, the center planes of the cloud.\nAs is clear, for a very stable cloud the dispersion is small relative to the distance traveled (the cross wind distance is exaggerated for visibility) and the bulk of the mass of chlorine is contained in a 10m diameter ball by the time it reaches the building.\n\n\n\n[ Info: Saved animation to /home/allan/Code/notes/notebooks/tmp.gif\n\n\n\n\n\n\n\n\n\nFigure 1: Dispersion of an instantaneous release of chlorine.\n\n\n\n\nThe concentration as a function of time also shows how brief the exposure is at this point. A rapid pulse of chlorine passes over the area and is gone within a few seconds. However the concentration at that time is millions of times larger than the ERPG-1 limit (note the units, the scale is in kg/m³ whereas the ERPG limits are on the order of mg/m³)\n\n\n\n\n\n\n\n\nFigure 2: Outdoor concentration at the receptor as a function of time\n\n\n\n\n\nThe concentration along the centerline of the cloud is also worth looking at, as this provides some context for the extent of the downwind impacts of the release.\n\n\n\n\n\n\n\n\nFigure 3: Centerline concentration of the puff as a function of travel distance.\n\n\n\n\n\nDownwind distance to ERPG-3 (outdoors) 13.5km\nDownwind distance to ERPG-2 (outdoors) 29.9km\nDownwind distance to ERPG-1 (outdoors) 47.3km\n\n\nClearly this release presents a serious hazard, one would have to travel downwind over 10km to be below the ERPG-3 line and nearly 50km to be below the ERPG-1 line. Though, keep in mind, this is for instantaneous exposure and not overall dose."
  },
  {
    "objectID": "posts/building_infiltration_2/index.html#building-infiltration",
    "href": "posts/building_infiltration_2/index.html#building-infiltration",
    "title": "Building Infiltration Example – Chlorine Release",
    "section": "Building Infiltration",
    "text": "Building Infiltration\n\nSingle Zone Model\nThe single zone model assumes the air within the building is generally well mixed and well described by a single concentration. This is approximately true over long timescales, however in the situation of the brief pulse of chlorine passing over the building this assumption may breakdown and is a critical weakness of the discussion that follows.\nVery likely in the ~10s it takes for the cloud to pass very little of it will have had time to diffuse into the interior space of the building and the interior mixing (or lack thereof) will be a significant slow step in the overall mass transfer.\nThe single zone model, however, will work as a first pass at least, and in this model the interior concentration is related to the outside concentration by the following ODE3\n3 Lees, Loss Prevention in the Process Industries, sec. 15.51.\\[\\frac{d}{dt} c_i(t) = f \\left( c_i, \\lambda, t \\right) = \\lambda \\cdot \\left( c_o(t) - c_i(t) \\right) \\]\nWhere cᵢ is the inside concentration, cₒ the outside concentration, and λ the natural ventilation rate of the building.\nThe natural ventilation rate itself is a function of windspeed, the temperature difference between inside and outside, and how leaky the building is.\nThe model is defined in a more generic way for now as this will be more useful later.\n\nf(cᵢ, λ, t; cₒ=zero) = λ*(cₒ(t) - cᵢ)\n\nf(g) = (cᵢ, λ, t) -&gt; f(cᵢ, λ, t; cₒ=g)\n\n\n\nSimplified ASHRAE Model\nThe last parameter we need to estimate before solving the problem is the ventilation rate, λ, which can be estimated using the simplified ASHRAE model4\n4 2017 ASHRAE Handbook - Fundamentals (SI Edition), chap. 16.\\[\\lambda = \\frac{Q}{V} \\\\\nQ = A_L \\sqrt{ C_s \\vert \\Delta T \\vert + C_w u^2 } \\\\\n\\lambda = \\frac{A_L}{V} \\sqrt{ C_s \\vert \\Delta T \\vert + C_w u^2 } \\]\nWhere \\(A_L\\) and \\(V\\) were given earlier, \\(C_s\\) and \\(C_w\\) are tabulated constants, \\(\\Delta T\\) is the difference between indoor and outdoor temperatures, in K, and u the windspeed, in m/s, and the ventilation rate is in s⁻¹.\nIn this case the indoor and outdoor temperature are assumed to be the same for simplicity5\n5 Note that the constants have been adjusted such that the leak area is in m², in the ASHRAE handbook the leak area is in cm²\n\n\n\nShelter Class\n1 Story\n2 Story\n3 Story\n\n\n\n\n\\(C_s\\)\nall\n14.5×10⁻³\n29.0×10⁻³\n43.5×10⁻³\n\n\n\\(C_w\\)\n1\n31.9×10⁻³\n42.0×10⁻³\n49.4×10⁻³\n\n\n\n2\n24.6×10⁻³\n32.5×10⁻³\n38.2×10⁻³\n\n\n\n3\n17.4×10⁻³\n23.1×10⁻³\n27.1×10⁻³\n\n\n\n4\n10.4×10⁻³\n13.7×10⁻³\n16.1×10⁻³\n\n\n\n5\n3.20×10⁻³\n4.20×10⁻³\n4.90×10⁻³\n\n\n\nWith the shelter class defined as 1. No obstructions or local shielding 2. Isolated rural home 3. Another building across the street 4. Urban buildings on larger lots, with the nearest building &gt;1 building height away 5. Immediately adjacent buildings (closer than 1 building height)\nFor this scenario we are assuming the (one-story) building is isolated and there are no obstructions or local shielding, so the class is 1.\n\n# 1 Story building, shelter class 1\n\nλ(ΔT, u; Aₗ, V, Cs, Cw) = (Aₗ/V)*√(Cs*abs(ΔT)+Cw*u^2)\n\nλ₀ = λ(0.0, u, Aₗ=Aₗ, V=V, Cs=14.5e-3, Cw=31.9e-3)\n\n7.249290622734888e-5"
  },
  {
    "objectID": "posts/building_infiltration_2/index.html#indoor-concentration",
    "href": "posts/building_infiltration_2/index.html#indoor-concentration",
    "title": "Building Infiltration Example – Chlorine Release",
    "section": "Indoor Concentration",
    "text": "Indoor Concentration\nThe indoor concentration is calculated simply by solving the ODE with the initial condition that the indoor concentration is zero. Since the pulse of chlorine is so brief it is important to be careful with the integrator, typical variable step integrators can step right over brief pulses and miss them entirely. To counteract that the first phase of the response, up until twice the time it takes for the wind to travel the distance, is solved using a maximum timestep of 1s, the remainder is left free to the solver to adjust as necessary to meet the tolerances.\n\n# puff model, note the units have changed to mg/m³\n\ncₒ(t) = c_puff(d, 0, h, t; m=m, u=u, h=h, σx=σx, σy=σy, σz=σz)*1e6\n\nc0 = 0.0       # initial condition\nsys = f(cₒ)\n\ntsp1 = (0.0, 2*d/u)\nprb1 = ODEProblem(sys, c0, tsp1, λ₀)\nsln1 = solve(prb1, Tsit5(), dtmax=1);\n\n\nc0_2 = sln1[end]\ntsp2 = (tsp1[end], 2.5*(1/λ₀))\nprb2 = ODEProblem(sys, c0_2, tsp2, λ₀)\nsln2 = solve(prb2, Tsit5());\n\nThis is a very conservative model. It assumes all of the mass transfer occurs at a point, which happens to be where the maximum outdoor concentration will be, and that interior mixing is essentially instantaneous. It also has a significant weakness in that it assumes the building does not impede the movement of the cloud. If the building is very small relative to the cloud this might be reasonable, but in this case the cloud is quite concentrated and the deflection around the building is important.\nThe first assumption can be moderated by taking an average outdoor concentration over the building footprint. In this case I assume the building is a 10m×10m square just for demonstration. This tries to capture the reality that not all of the building is being exposed to an equally high concentration and that an effective outdoor concentration might be better estimated as a spatial average. These points are hanging in space at the centerline of the cloud and as such are exposed to the highest concentrations. Instead of a slice like this, a more fully featured box could be used, but at that point it would probably be more useful to look into the ways the building itself is deflecting and shaping the cloud. Recall that the cloud is essentially passing through the building in this model.\n\nbox_x = (d-5):1:(d+5)\nbox_y = -5:1:5\n\nfunction box_average(t)\n    cs = [ c_puff(x, y, h, t; m=m, u=u, h=h, σx=σx, σy=σy, σz=σz)*1e6\n           for x in box_x, y in box_y ]\n    return mean(cs)\nend\n\n\nsys_avg = f(box_average)\n\ntsp_box = (0.0, 2*d/u)\nprb_box = ODEProblem(sys_avg, c0, tsp_box, λ₀)\nsln_box = solve(prb_box, Tsit5(), dtmax=1);\n\n\nc0_box2 = sln_box[end]\ntsp_box2 = (tsp_box[end], 2.5*(1/λ₀))\nprb_box2 = ODEProblem(sys_avg, c0_box2, tsp_box2, λ₀)\nsln_box2 = solve(prb_box2, Tsit5());\n\n\n\n\n\n\n\n\n\nFigure 4: Indoor concentrations assuming a linear ventillation model, showing both the building as a point source and as averaged box.\n\n\n\n\n\nIn either the single point or averaged outdoor concentration models the indoor concentration rapidly rises above the ERPG-3 limit, which is very bad, and then slowly decays over time6. In this case almost immediately after the cloud has passed the indoor space is more concentrated in Chlorine than the outside air. At the very least this suggests that the building is not a good shelter in place location, or at least a much more detailed analysis of building infiltration would be needed to show that it was a good shelter in place location.\n6 Note that the indoor concentrations are in mg/m³ whereas the outdoor concentration peaks in the kg/m³, so the building is doing something, it is reducing the indoor concentration by several orders of magnitude, it just isn’t enough"
  },
  {
    "objectID": "posts/building_infiltration_2/index.html#sensitivity",
    "href": "posts/building_infiltration_2/index.html#sensitivity",
    "title": "Building Infiltration Example – Chlorine Release",
    "section": "Sensitivity",
    "text": "Sensitivity\nFor the scenario modeling I approached the problem in a very general way such that the methods for solving the indoor concentration didn’t depend explicitly upon the model of the outdoor concentration. Which is why it was solved numerically. This is a very useful way of approaching things, especially from a code re-use point of view.\nHowever, in this particular case, if we take the outdoor concentration to be simply the gaussian puff model at a single point then this ODE can be solved analytically and that is useful for exploring the system’s sensitivity to windspeed, atmospheric stability, etc.\nStarting with the original puff model for the outside concentration\n\\[ c_{o}(x,y,z,t) = { m \\over { (2 \\pi)^{3/2} \\sigma_x \\sigma_y \\sigma_z } }\n\\exp \\left( -\\frac{1}{2} \\left( {x - ut} \\over \\sigma_x \\right)^2 \\right)\n\\exp \\left( -\\frac{1}{2} \\left( {y} \\over \\sigma_y \\right)^2 \\right) \\]\n\\[ \\times \\left[ \\exp \\left( -\\frac{1}{2} \\left( {z - h} \\over \\sigma_z \\right)^2 \\right) + \\exp \\left( -\\frac{1}{2} \\left( {z + h} \\over \\sigma_z \\right)^2 \\right)\\right] \\]\nWe can split this into the product of three gaussians:\n\\[ c_{o}(x,y,z,t) = m \\left[{ \\exp \\left( -\\frac{1}{2} \\left( {x - ut} \\over \\sigma_x \\right)^2 \\right) \\over { \\sqrt{2 \\pi} \\sigma_x } } \\right]\n\\left[{ \\exp \\left( -\\frac{1}{2} \\left( {y} \\over \\sigma_y \\right)^2 \\right) \\over { \\sqrt{2 \\pi} \\sigma_y } } \\right]  \\]\n\\[ \\times { 1 \\over { \\sqrt{2 \\pi} \\sigma_z } } \\left[ \\exp \\left( -\\frac{1}{2} \\left( {z - h} \\over \\sigma_z \\right)^2 \\right) + \\exp \\left( -\\frac{1}{2} \\left( {z + h} \\over \\sigma_z \\right)^2 \\right)\\right]\\]\n\\[ c_{o}(x,y,z,t) = m C_x(x, t) C_y(x, y) C_z(x, z)\\]\nand noting that only \\(C_x\\) depends on time we can collect the other stuff into a big constant called \\(C_1\\), giving usI am assuming the dispersion parameters are all constant, this is not strictly true as they all depend upon the downwind location of the center of the puff, which is a function of time\n\\[ c_{o}(t) = C_1 { 1 \\over { \\sqrt{2 \\pi} \\sigma_x } }\\exp \\left( -\\frac{1}{2} \\left( {x - ut} \\over \\sigma_x \\right)^2 \\right) \\]\nthis is a gaussian in time, let \\(\\mu = \\frac{x}{u}\\) and \\(\\sigma_t = \\frac{\\sigma_x}{u}\\)\n\\[ c_{o}(t) = { C_1 \\over u } { 1 \\over { \\sqrt{2 \\pi} \\sigma_t } }\\exp \\left( -\\frac{1}{2} \\left( {t - \\mu} \\over \\sigma_t \\right)^2 \\right) \\]\nsuppose the Laplace transform of this is \\(C_{o}(s)\\), and taking the Laplace transform of the ODE we arrive at\n\\[ C_{i}(s) = { \\lambda \\over {s + \\lambda} } C_{o}(s) \\]\nwhere \\(C_{i}(s)\\) is the Laplace transform of \\(c_{i}(t)\\), inverting the Laplace transform leads us to conclude\n\\[ c_{i}(t) = { C_1 \\over u } \\int_{0}^{\\infty} \\lambda \\exp \\left( -\\lambda \\left(t - \\tau \\right) \\right)\n{ 1 \\over { \\sqrt{2 \\pi} \\sigma_t } }\\exp \\left( -\\frac{1}{2} \\left( {\\tau - \\mu} \\over \\sigma_t \\right)^2 \\right) d\\tau\\]\nthat is, the solution is the convolution of the exponential and gaussian times some constants. Conveniently for us this is a well known integral and we can just look up the answer in a book\n\\[ c_{i}(t) = { C_1 \\over u } \\frac{\\lambda}{2} \\exp \\left( \\frac{\\lambda}{2} \\left( 2\\mu + \\lambda \\sigma_t^2 - 2t \\right) \\right) \\mathrm{erfc} \\left( { \\mu + \\lambda \\sigma_t^2 -t } \\over { \\sqrt{2} \\sigma_t } \\right) \\]\nwhere \\(\\mathrm{erfc(x)}\\) is the complementary error function \\(1 - \\mathrm{erf}(x)\\)\nYou could expand all this back out, but it is far more compact and readable in this form, especially when written out as code\n\nfunction cᵢ(x,y,z,t;    # point in space\n            m, u, h, λ, # parameters of the problem\n            σx::Function, σy::Function, σz::Function)\n    \n    sx = σx(x)\n    sy = σy(x)\n    sz = σz(x)\n    \n    # time independent part\n    C1 = m / (2*π*sy*sz)\n    C1 *= exp(-0.5*(y/sy)^2)\n    C1 *= ( exp(-0.5*((z-h)/sz)^2) + exp(-0.5*((z+h)/sz)^2) )\n    \n    # the convolution\n    μ = x/u\n    σₜ = sx/u\n    \n    c =  (C1*λ)/(2*u)\n    c *= exp(0.5*λ*(2*μ+λ*σₜ^2-2*t))\n    c *= erfc((μ+λ*σₜ^2-t)/(√(2)*σₜ))\n    \n    return isnan(c) ? 0.0 : c\nend\n\nFrom here we can use automatic differentiation to find the max concentration, for the original case\n\n# at the point x=d, y=0, z=h, note the units in mg/m³\n\nc(t) = cᵢ(d, 0, h, t, m=m, u=u, h=h, λ=λ₀, σx=σx, σy=σy, σz=σz)*1e6\n\n∂c∂t(t) = ForwardDiff.derivative(t -&gt; c(t), float(t))\n        \ntₘₐₓ = find_zero(∂c∂t, d/u)\n\ncₘₐₓ = c(tₘₐₓ)\n\ntₘₐₓ, cₘₐₓ\n\n(70.04333860265841, 551.5422273514416)\n\n\nAn alternative is to make the approximation \\(\\mathrm{erfc}(-x) \\approx 2 H(x)\\), where \\(H(x)\\) is the Heaviside step function ( \\(\\mathrm{erfc}(-x)\\) runs from 0 to 2 and \\(H(x)\\) runs from 0 to 1 hence the factor of 2)\n\\[ c_{i}(t) = { C_1 \\over u } \\frac{\\lambda}{2} \\exp \\left( \\frac{\\lambda}{2} \\left( 2\\mu + \\lambda \\sigma_t^2 - 2t \\right) \\right) \\mathrm{erfc} \\left( { \\mu + \\lambda \\sigma_t^2 -t } \\over { \\sqrt{2} \\sigma_t } \\right) \\]\n\\[ = { C_1 \\over u } \\frac{\\lambda}{2} \\exp \\left( { \\left( \\lambda \\sigma_t \\right)^2 \\over 2 } \\right) \\exp \\left( -\\lambda \\left( t - \\mu \\right) \\right) \\mathrm{erfc} \\left( { \\mu + \\lambda \\sigma_t^2 -t } \\over { \\sqrt{2} \\sigma_t } \\right) \\]\n\\[ \\approx { \\lambda C_1 \\over u } \\exp \\left( { \\left( \\lambda \\sigma_t \\right)^2 \\over 2 } \\right) \\exp \\left( -\\lambda \\left( t - \\mu \\right) \\right) H( t - \\mu - \\lambda \\sigma_t^2 ) \\]\n\\[ \\approx { \\lambda C_1 \\over u } \\exp \\left( { \\left( \\lambda \\sigma_t \\right)^2 \\over 2 } \\right) \\exp \\left( -\\lambda \\left( t - \\mu \\right) \\right) H( t - \\mu )\n\\]\nthe maximum of this is clearly\n\\[ c_{max} = { \\lambda C_1 \\over u } \\exp \\left( { \\left( \\lambda \\sigma_t \\right)^2 \\over 2 } \\right) \\]\nand occurs when \\(t = \\mu\\)\n\nlet x=d, y=0, z=h, λ=λ₀\n    sx = σx(x)\n    sy = σy(x)\n    sz = σz(x)\n    μ  = x/u\n    σₜ = sx/u\n    \n    C1 = m / (2*π*sy*sz)\n    C1 *= exp(-0.5*(y/sy)^2)\n    C1 *= ( exp(-0.5*((z-h)/sz)^2) + exp(-0.5*((z+h)/sz)^2) )\n    \n    c =  (λ*C1)/u\n    c *= exp(0.5*(λ*σₜ)^2)\n    \n    return (μ, c*1e6)\nend\n\n(66.66666666666667, 551.6845234598728)\n\n\nWe can compare the solution from the ODE solver with the two approximations – the convolution and the step-function approximation – to convince ourselves that we are capturing the dynamics well.\n\n\n\n\n\n\n\n\nFigure 5: Approximations to the full integration of the linear ventillation model.\n\n\n\n\n\n\nAtmospheric stability\nThe sensitivity to atmospheric stability, at the max windspeed given in the table of dispersion parameters above, is shown below. Clearly the indoor concentration depends strongly on the atmospheric stability, and that makes sense as more unstable conditions lead to far greater mixing and thus lower outdoor concentrations. This effect is far greater than any increase in ventilation rate due to the greater windspeeds.\n\n\n\n\n\n\n\n\nFigure 6: Sensitivity of max indoor concentration to atmospheric stability\n\n\n\n\n\n\n\nWindspeed\nReturning to the original scenario parameters we explore the impact of changing the windspeed, while assuming the atmospheric stability remains constant, in the figure below. In this case windspeed impacts both the parameters of the gaussian puff model and the natural ventilation rate in the single zone building infiltration model.\n\n\n\n\n\n\n\n\nFigure 7: Sensitivity of indoor concentration to outdoor windspeed.\n\n\n\n\n\nThis was, to me, a surprising result. I expected the max indoor concentration to depend strongly on the windspeed whereas it appears to have far more to do with how quickly the indoor build up of Chlorine dissipates. This is due to the difference in timescales between the puff passing over the building and the single zone building infiltration model.\nThe time-scale of interest for the puff model is the width of the gaussian \\(\\sigma_t\\), whereas the time-scale of interest for the building infiltration model is the time constant of the exponential decay \\(\\tau = \\frac{1}{\\lambda}\\), for this situation with a class F atmospheric stability they are\n\nσₜ = σx(d)/u\n\n0.8034127814324771\n\n\n\nτ = 1/λ₀\n\n13794.453168477568\n\n\n\nσₜ/τ\n\n5.82417274262381e-5\n\n\nThe time it takes the cloud to pass is simply orders of magnitude faster than the response of the infiltration model. So, in effect, the puff is acting like a impulse – causing a step change – and changing the windspeed merely moves the time at which that step change takes place. The faster the windspeed the better the approximation $ (-x) H(x) $ gets and if we take another look at the approximation for max concentration, we see it is independent of windspeed.\nNote that for the simplified ASHRAE model with \\(\\Delta T = 0\\) the natural ventilation rate is directly proportional to windspeed, i.e. $ = k u $ where k is all the vaious constants of that model collected for convenience, plugging this into the approximation we find the windspeed cancels out entirely.\n\\[ { \\lambda C_1 \\over u } \\exp \\left( { \\left( \\lambda \\sigma_t \\right)^2 \\over 2 } \\right) \\]\n\\[ = { { k u C_1 } \\over u } \\exp \\left( { \\left( k u \\frac{\\sigma_x}{u} \\right)^2 \\over 2 } \\right) \\]\n\\[ = k C_1 \\exp \\left( { \\left( k \\sigma_x \\right)^2 \\over 2 } \\right) \\]\nWhich is independent of windspeed.\nWe can perhaps see how small the effect is more clearly by directly varying the ratio \\({ \\sigma_t \\over \\tau}\\) while keeping \\(\\mu\\) constant and looking at the response. The maximum indoor concentration does change, but only slightly.\n\n\n\n\n\n\n\n\nFigure 8: Sensitivity of the indoor concentration to the ratio σt/τ\n\n\n\n\n\nWhich is not to say the model is insensitive to the natural ventilation rate, merely that for a given building the impact of changing windspeed on the ventilation rate is canceled out by the change in the outdoor concentration profile.\n\n\nEquivalent Leak Area\nBelow the equivalent leak area \\(A_L\\) is varied while keeping all other parameters constant. Unsurprisingly building tightness matters, and the impact is approximately linear. This is intuitive, of course, because the outside concentration does not depend in any way on the leakage area and so there is no canceling of effects like was seen with windspeed, and of course having more leaks leads to more of the outside air getting inside.\n\n\n\n\n\n\n\n\nFigure 9: Sensitivity of indoor concentration to the leakage area.\n\n\n\n\n\n\n\nTemperature\nA similar effect as was seen with equivalent leak area is present with changes in the temperature difference between indoors and outdoors. Though in this case the change goes with the square-root of the temperature difference.\n\n\n\n\n\n\n\n\nFigure 10: Sensitivity of indoor concentration to the indoor-outdoor temperature difference\n\n\n\n\n\nOn a summer day like today large temperature differences like this might seem extreme, but in the depths of winter when days around here routinely get to -30°C a standard heated building with a normal indoor temperature around 20°C would have a 50°C temperature difference with the outdoors.\n\n\nDistance\nThe most obvious case of interest, however, is how far downwind would the building have to be such that it did not exceed the ERPG limits. With all other parameters equal to the scenario, the max indoor concentration as a function of building distance is shown in the figure below.\n\n\n\n\n\n\n\n\nFigure 11: Max indoor concentration as a function of downwind distance to the receptor.\n\n\n\n\n\nDownwind distance to ERPG-3 (indoors) 625.0m\nDownwind distance to ERPG-2 (indoors) 2383.0m\nDownwind distance to ERPG-1 (indoors) 5006.0m\n\n\nAs was seen above when examining the outdoor concentrations, this is a significant release and the downwind distance is large. This also shows the value of sheltering in place as an unprotected individual would have to travel downwind for many tens of kilometers to reach a safe distance, whereas indoors that is greatly reduced.\nAgain, this is all considering the instantaneous concentration and not considering dose."
  },
  {
    "objectID": "posts/building_infiltration_2/index.html#final-remarks",
    "href": "posts/building_infiltration_2/index.html#final-remarks",
    "title": "Building Infiltration Example – Chlorine Release",
    "section": "Final Remarks",
    "text": "Final Remarks\nThe results of the scenario speak to something the previous discussion noted, namely that after the cloud has passed the indoor concentration exceeds the outdoor concentration and so an important response, post release, is not just when to shelter in place but when to stop.\nThe worst case indoor concentration far exceeded the ERPG-3 limit, however it was still significantly lower exposure than had one been standing outside. But once the chlorine had leaked into the building it would take hours to fully clear purely by natural ventilation. At that point it would be far more prudent to leave the building and to turn the ventilation system back on. Identifying when this is the case is an interesting problem, because there are no guarantees that the emergency outside the building is fully resolved merely because the toxic release has passed. So one must balance the risks of sheltering in a place that is no longer safe versus evacuating into a potentially dangerous environment. In such a scenario it may be prudent to have indoor and outdoor air monitoring in the shelter in place location and a store of emergency escape respirators, though a better solution would be to move the shelter in place to a safer location.\nThese two examples cover two extremes of building infiltration, the forest fire smoke looked at enormous clouds that take hours to pass and this chlorine example covers very concentrated clouds which pass in under a minute. Most real scenarios at a chemical plant or other facility are likely to be between these extremes, but the same tools would apply."
  },
  {
    "objectID": "posts/building_infiltration_2/index.html#references",
    "href": "posts/building_infiltration_2/index.html#references",
    "title": "Building Infiltration Example – Chlorine Release",
    "section": "References",
    "text": "References\n\n\n2017 ASHRAE Handbook - Fundamentals (SI Edition). Atlanta, GA: American Society of Heating, Refrigerating; Air-Conditioning Engineers, 2017.\n\n\nAIChE/CCPS. Guidelines for Use of Vapour Cloud Dispersion Models. 2nd ed. New York: American Institute of Chemical Engineers, 1996.\n\n\nLees, Frank P. Loss Prevention in the Process Industries. 2nd ed. Oxford: Butterworth-Heinemann, 1996."
  },
  {
    "objectID": "posts/turbulent_jet_notes_part_2/index.html",
    "href": "posts/turbulent_jet_notes_part_2/index.html",
    "title": "More on Turbulent Jets",
    "section": "",
    "text": "Previously I worked through the velocity profiles for turbulent jets and left off claiming that everything else of interest followed simply from those profiles. This time I am going to follow through by sketching out how to derive the concentration profile and volumetric flow rate."
  },
  {
    "objectID": "posts/turbulent_jet_notes_part_2/index.html#concentration",
    "href": "posts/turbulent_jet_notes_part_2/index.html#concentration",
    "title": "More on Turbulent Jets",
    "section": "Concentration",
    "text": "Concentration\nFor hazard identification, among other purposes, what one often wants is not the velocity distribution of the jet, but the concentration profile. For example, suppose a vessel develops a small hole and a jet of process fluid is exiting out into the air, to determine how bad that is and what sort of hazard is presented (explosive, toxic, etc.) we first need to determine the concentration profile.\nSuppose the concentration of a species A is cA, for the sake of simplicity let this be a time-averaged concentration done in a way that is consistent with Reynolds averaging.1\n1 Note the concentration is given in units of \\([[ quantity ]] \\times [[length]]^{-3}\\), e.g. kmol/m³2 Bird, Stewart, and Lightfoot, Transport Phenomena, 850.The continuity equation for species A is given by2\n\\[ {\\mathrm{D} \\over \\mathrm{D}t} c_A = - \\nabla \\cdot \\mathbf{J}_A + r_A \\]\nwhere J is the molar flux and r is the rate of reaction3\n3 The molar flux J is the time averaged molar flux and is the sum of both viscous and turbulent terms. \\(\\mathbf{J}_A = \\mathbf{J}_A^{(v)} + \\mathbf{J}_A^{(t)}\\)In cylindrical coordinates this is:\n\\[ {\\partial c_A \\over \\partial t}  + \\bar{v}_r {\\partial c_A \\over \\partial r} + {\\bar{v}_{\\theta} \\over r } {\\partial c_A \\over \\partial \\theta} + \\bar{v}_z {\\partial c_A \\over \\partial x} \\]\n\\[ = -\\left[ {1 \\over r} {\\partial \\over \\partial r} \\left(r J_{A,r}\\right) + {1 \\over r} {\\partial \\over \\partial \\theta} J_{A,\\theta} + {\\partial \\over \\partial z} J_{A,z} \\right] + r_A\\]\nMaking the following assumptions:\n\nSteady state ( \\({\\partial \\over \\partial t} \\left( \\dots \\right) = 0\\) )\nAxisymmetric ( \\({\\partial \\over \\partial \\theta} \\left( \\dots \\right) = 0\\) )\nBoundary layer approximation ( \\(\\vert {\\partial c_A \\over \\partial z} \\vert \\ll \\vert {\\partial c_A \\over \\partial r} \\vert\\) )\nNon-reacting ( \\(r_A = 0\\) )\n\nThis simplifies down to\n\\[ \\bar{v}_r {\\partial c_A \\over \\partial r} +  \\bar{v}_z {\\partial c_A \\over \\partial z} = {-1 \\over r} {\\partial \\over \\partial r} \\left( r J_{A,r} \\right) \\]\nWe suppose that, much like the velocity profile, the concentration profile is self-similar. That is, at any given downstream distance z the profile has the same shape, just scaled and stretched.\n\\[ c_A = {k_c \\over z} g\\left(\\xi\\right) \\]\nwhere ξ is r/z and kc is some constant to be determined. From this we can work out some useful partial derivatives\n\\[ {\\partial c_A \\over \\partial r} = {k_c \\over z^2} g^{\\prime}\\left(\\xi\\right) \\]\n\\[ {\\partial c_A \\over \\partial z} = {-k_c \\over z^2} \\left[ g\\left(\\xi\\right) + \\xi g^{\\prime}\\left(\\xi\\right) \\right]\\]\nrecalling the velocity profiles in terms of F(ξ) and substituting into the equation of continuity we arrive at\n\\[{ k k_c \\over z^2 } {d \\over d\\xi} \\left(F g\\right) = -{ \\partial \\over \\partial r} \\left( r J_{A,r} \\right)\\]\nWhich gives us our path forward: find a model for \\(J_{A,r}\\) and substitute into the right hand side of the equation, integrate both sides and solve for g in terms of F and ξ.\n\nPrandtl mixing length models\nWe are going to assume that the overall molar flux is proportional to the concentration gradient and some mixing length4 l, that is\n4 Bird, Stewart, and Lightfoot, Transport Phenomena, 659.\\[ J_{A,r} = -l_c^2 \\left\\vert \\partial \\bar{v}_z \\over \\partial r \\right\\vert \\left(\\partial c_A \\over \\partial r \\right)\\]\nwe assume the mixing length is proportional to the downstream distance for the same reasons as when we derived the velocity profile and, anticipating the form of the constants from how it worked out for the velocity distribution, \\(l_c = a_c^{3/2} z\\)\nPutting this into the equation of continuity for A and doing some rearranging gives\n\\[{ k k_c \\over z^2 } {d \\over d\\xi} \\left(F g\\right) = { k k_c \\over z^2 } a_c^3 {d \\over d\\xi}\\left(g^{\\prime}F^{\\prime\\prime} - {g^{\\prime}F^{\\prime} \\over \\xi} \\right)\\]\nCancelling some terms and integrating both sides gives us\n\\[ F g = a_c^3 \\left(g^{\\prime}F^{\\prime\\prime} - {g^{\\prime}F^{\\prime} \\over \\xi} \\right) + \\mathrm{const} \\]\nWhere we can see from the boundary conditions that the constant of integration is zero. We can separate F and g\n\\[ {g^{\\prime} \\over g} = { a_c^{-3} F \\over F^{\\prime \\prime} - {F^{\\prime} \\over \\xi} }\\]\nand integrating both sides again, we arrive at\n\\[ \\log{c_A \\over c_{A,max} } = \\log{g\\left(\\xi\\right) \\over g\\left(0\\right)} = a_c^{-3} \\int_0^{\\xi} {F \\over F^{\\prime \\prime} - {F^{\\prime} \\over \\xi} } d\\xi\\]\nNote that, when setting up the original ode, we had\n\\[ F F^{\\prime} = a^3 \\left( F^{\\prime\\prime} - {F^{\\prime} \\over \\xi} \\right)^2 \\]\nor\n\\[ F^{\\prime\\prime} - {F^{\\prime} \\over \\xi} = \\sqrt{ F F^{\\prime} \\over a^3 } \\]\nMaking the substitution gives us an integral entirely in terms of F, F′ and ξ\n\\[ \\log{g\\left(\\xi\\right) \\over g\\left(0\\right)} = a_c^{-3} a^{3/2} \\int_0^{\\xi} {F \\over \\sqrt{ F F^{\\prime} } } d\\xi\\]\nTaking the exponential of both sides gives us:\n\\[ {g\\left(\\xi\\right) \\over g\\left(0\\right)} = \\left( \\exp \\left(\\int_0^{\\xi} {F \\over \\sqrt{ F F^{\\prime} } } d\\xi \\right)\\right)^{ a_c^{-3} a^{3/2} } \\]\nWhich is something we can compute using the ode solution from last time, by importing the code for the ode from the previous notebook and running it to get the velocity distribution.\n\n# importing just the ODE solution\n\nusing NBInclude\n\n@nbinclude(\"2022-04-08-turbulent_jet_notes.ipynb\"; counters=[1 3])\n\nWe can then perform the integration numerically, in this case the cumulative integral\n\nusing NumericalIntegration: cumul_integrate\n\nϕ, F, F′ = sol.t, sol[1,:], sol[2,:]\n\n# trim any unphysical values\nF[ F.&gt;0 ] .= 0.0\nF′[F′.&gt;0] .= 0.0\n\n\nfunction intgrnd(F, F′) \n    if F == 0.0\n        return 0.0\n    elseif F′ == 0.0\n        return -Inf\n    else\n        return F ./ .√(F.*F′)\n    end\nend\n    \nlog_g = cumul_integrate(ϕ, intgrnd.(F, F′));\n\nFor some context we can plot the concentration along with the velocity, with the constants \\(a_c^{-3} a^{3/2} = 1\\)\n\n\n\n\n\n\n\n\nFigure 1: Comparison of the concentration profile to the velocity profile, Prandtl mixing length theory.\n\n\n\n\n\nWell, that’s interesting, it looks like we have arrived at\n\\[ {c_A \\over c_{A,max} } = \\left( \\bar{v}_z \\over \\bar{v}_{z,max} \\right)^{\\mathrm{const} } \\]\nWhich is, in fact, the case and is generally the case – the Prandtl mixing length, eddy diffusion, and Gaussian models work out to the same conclusion.\nConsider the following derivative\n\\[ {d \\over d\\xi} \\log{f\\left(\\xi\\right)} = {d \\over d\\xi} \\log \\left( -F^{\\prime} \\over \\xi \\right) = {1 \\over F^{\\prime} } \\left( F^{\\prime\\prime} - {F^{\\prime} \\over \\xi} \\right)\\]\nRecalling back to our original ode for the Prandtl mixing length velocity distribution, we had the relationship\n\\[ F F^{\\prime} = a^3 \\left( F^{\\prime\\prime} - {F^{\\prime} \\over \\xi} \\right)^2 \\]\nor\n\\[ {F \\over F^{\\prime\\prime} - {F^{\\prime} \\over \\xi} } = a^3 { {F^{\\prime\\prime} - {F^{\\prime} \\over \\xi} } \\over F^{\\prime} } \\]\nand so\n\\[ {d \\over d\\xi} \\log{f\\left(\\xi\\right)} =  a^{-3} {F \\over F^{\\prime\\prime} - {F^{\\prime} \\over \\xi} } \\]\nand recalling that the integral we originally wished to solve was\n\\[ \\log{c_A \\over c_{A,max} } = a_c^{-3} \\int_0^{\\xi} {F \\over F^{\\prime \\prime} - {F^{\\prime} \\over \\xi} } d\\xi\\]\nwe get\n\\[ \\log{c_A \\over c_{A,max} } = \\left(a \\over a_c\\right)^{3} \\left[ \\log \\left( f\\left(\\xi\\right) \\over f\\left(0\\right) \\right) \\right] = \\log \\left( \\bar{v}_z \\over \\bar{v}_{z,max} \\right)^{\\left(a \\over a_c\\right)^{3} } \\]\nand finally\n\\[ { c_A \\over c_{A,max} } = \\left( \\bar{v}_z \\over \\bar{v}_{z,max} \\right)^{\\left(a \\over a_c\\right)^{3} }\\]\nWhere, rather pleasingly, the constant \\({\\left(a \\over a_c\\right)^{3} }\\) works out to be the ratio of the mixing lengths, all squared5\n5 Suppose an “equivalent” eddy viscosity for the Prandtl mixing length model of\n\\[\\varepsilon = -l^2 \\left\\vert \\partial \\bar{v}_z \\over \\partial r \\right\\vert\\]\nand eddy diffusivity of\n\\[\\mathscr{D}_{AB} = -l_c^2 \\left\\vert \\partial \\bar{v}_z \\over \\partial r \\right\\vert\\]\nthe turbulent Schmidt number is then\n\\[\\mathrm{Sc} = {\\varepsilon \\over \\mathscr{D}_{AB} } = \\left( l \\over l_c \\right)^2\\]\nmaking the final result\n\\[{c_A \\over c_{A,max} } = \\left( \\bar{v}_z \\over \\bar{v}_{z,max} \\right)^{Sc}\\]\\[ {\\left(a \\over a_c\\right)^{3} } = \\left( l \\over l_c \\right)^2 \\]\nWe can now plot the concentration profile along with the velocity profile and see that the two profiles have a similar shape, with the concentration profile stretched to be wider. Concentration spreads out more than velocity.\n\n\n\n\n\n\n\n\nFigure 2: The dimensionless concentration profile and velocity profile, Prandtl mixing length theory.\n\n\n\n\n\n\n\nEddy diffusivity models\nIn the eddy diffusivity model we assume the molar flux is proportional to the concentration gradient with the constant of proportionality being an effective diffusivity, the eddy diffusivity. In some treatments the viscous and turbulent diffusivities are treated separately, in this model we lump it all together into one constant\n\\[ J_{A,r} = -\\mathscr{D}_{AB} {\\partial c_A \\over \\partial r} \\]\nwhere \\(\\mathscr{D}_{AB}\\) is the eddy diffusivity for species A. Using the definition of cA we can work out the right hand side of the equation of continuity for A\n\\[ -{ \\partial \\over \\partial r} \\left( r J_{A,r} \\right) = {k_c \\over z^2} \\mathscr{D}_{AB} {d \\over d\\xi} \\left( \\xi g^{\\prime} \\right) \\]\nputting that into the equation of continuity for A, we get\n\\[{ k k_c \\over z^2 } {d \\over d\\xi} \\left(F g\\right) =  {k_c \\over z^2} \\mathscr{D}_{AB} {d \\over d\\xi} \\left( \\xi g^{\\prime} \\right) \\]\ncancelling some terms and integrating once gives us\n\\[k F g =  \\mathscr{D}_{AB} \\xi g^{\\prime} + \\mathrm{const}\\]\nwhere, by use of boundary conditions, the constant of integration is zero. This can be rearranged to isolate F and g\n\\[ {g^{\\prime} \\over g} = {k \\over \\mathscr{D}_{AB} } {F \\over \\xi} \\]\nintegrating once more\n\\[ \\log \\left( g\\left(\\xi\\right) \\over g\\left(0\\right) \\right) = {k \\over \\mathscr{D}_{AB} } \\int_0^{\\xi} {F \\over \\xi} d\\xi \\]\nrecalling that, for the eddy diffusivity model\n\\[ F\\left( \\xi \\right) = { - c \\left( C_2 \\xi \\right)^2 \\over {1 + \\frac{1}{4} \\left( C_2 \\xi \\right)^2 } } \\]\n\\[ \\int_0^{\\xi} {F \\over \\xi} d\\xi = {c} \\int_0^{\\xi} { -C_2^2 \\xi \\over {1 + \\frac{1}{4} \\left( C_2 \\xi \\right)^2 } } d\\xi = {c} \\log \\left(1 + \\frac{1}{4} \\left( C_2 \\xi \\right)^2 \\right)^{-2} \\]\nand so we have\n\\[ \\log \\left( g\\left(\\xi\\right) \\over g\\left(0\\right) \\right) = {k c \\over \\mathscr{D}_{AB} } \\log \\left(1 + \\frac{1}{4} \\left( C_2 \\xi \\right)^2 \\right)^{-2} \\]\nrecalling that the constant k is related to the eddy diffusivity \\(\\varepsilon\\) by \\(\\varepsilon=ck\\), we have\n\\[ {c_A \\over c_{A,max} } = \\left( g\\left(\\xi\\right) \\over g\\left(0\\right) \\right) = \\left(1 + \\frac{1}{4} \\left( C_2 \\xi \\right)^2 \\right)^{-2{\\varepsilon \\over \\mathscr{D}_{AB} } } \\]\nand, looking back on the definition of f(ξ) from the eddy viscosity model, we get6\n6 The turbulent Schmidt number is defined as the ratio of the eddy viscosity to the eddy diffusivity\n\\[\\mathrm{Sc} = {\\varepsilon \\over \\mathscr{D}_{AB} }\\]\nmaking the final result\n\\[{c_A \\over c_{A,max} } = \\left( \\bar{v}_z \\over \\bar{v}_{z,max} \\right)^{Sc}\\]\\[ {c_A \\over c_{A,max} } = \\left( \\bar{v}_z \\over \\bar{v}_{z,max} \\right)^{\\varepsilon \\over \\mathscr{D}_{AB} } \\]\nWe can plot the concentration and velocity profiles for the eddy diffusivity model as well, and it is a similar story. The shapes of the profiles are the same but the concentration profile is stretched, such that concentration “spreads out” more than velocity does.\n\n\n\n\n\n\n\n\nFigure 3: The dimensionless concentration profile and velocity profile, Eddy diffusivity model.\n\n\n\n\n\n\n\nGaussian models\nThe standard Gaussian model was defined as\n\\[ f\\left(\\xi\\right) = \\exp \\left( -c \\xi^2 \\right) \\]\nwhere the constant c (note: not the concentration) was found to be\n\\[ c = \\log{2} \\left(z \\over b_{1/2}\\right)^2 = \\log{2} \\left(1 \\over \\beta\\right)^2 \\]\nwhere I am introducing the constant β to represent the spreading constant (i.e. the ratio of b to z) mostly to cut down on all the constants I call c given that I am also using c to represent concentrations.\nWe can then write the velocity distribution as\n\\[ f\\left(\\xi\\right) = \\exp \\left( -c \\xi^2 \\right) = \\exp \\left( -\\log{2} \\left(\\xi \\over \\beta\\right)^2 \\right) = \\exp \\left(- \\log 2 \\left(\\xi \\over \\beta\\right)^2 \\right)\\]\nThe concentration distribution is similarly defined empirically in terms of a half-width, \\(b_{1/2,c}\\) and entirely analogously we end up with a distribution\n\\[ g\\left(\\xi\\right) = \\exp \\left( -\\log 2 \\left(\\xi \\over \\beta_c\\right)^2 \\right)\\]\nwhere βc is the spreading constant for the concentration profile. But it’s fairly easy to see that this is equivalent to\n\\[ g\\left(\\xi\\right) = \\exp \\left( -\\log 2 \\left(\\xi \\over \\beta\\right)^2 \\left(\\beta \\over \\beta_c\\right)^2 \\right) = f\\left(\\xi\\right)^{\\left(\\beta \\over \\beta_c\\right)^2} \\]\nor7\n7 We can argue, in a manner analogous to the Prandtl mixing length theory, that the eddy viscosity is proportional to the characteristic length squared, and similarly for the eddy diffusivity and thus the turbulent Schmidt number is then\n\\[\\mathrm{Sc} = {\\varepsilon \\over \\mathscr{D}_{AB} } = \\left( b_{1/2} \\over b_{1/2,c} \\right)^2 = \\left( \\beta \\over \\beta_c \\right)^2\\]\nthus making the final result\n\\[{c_A \\over c_{A,max} } = \\left( \\bar{v}_z \\over \\bar{v}_{z,max} \\right)^{Sc}\\]\\[ {c_A \\over c_{A,max} } = \\left({\\bar{v}_z \\over \\bar{v}_{z,max} }\\right)^{\\left(\\beta \\over \\beta_c\\right)^2} \\]\n\n\n\n\n\nThe dimensionless concentration profile and velocity profile, Gaussian empirical model.\n\n\n\n\n\n\nSchmidt number\nAll three of the turbulent jet models looked at so far ended up with a concentration profile of\n\\[{c_A \\over c_{A,max} } = \\left( \\bar{v} \\over \\bar{v}_{z,max} \\right)^{Sc} \\]\nwhere I declared the constant Sc to be the turbulent Schmidt number. There are, of course, several ways of arriving at this value but literature generally gives \\(Sc \\approx 0.7\\). This also tends to be the same value given for the turbulent Prandtl number, which is convenient.\n\n\n\nSc\nReference\n\n\n\n\n0.7\nBird, Stewart, and Lightfoot8\n\n\n0.73\nKaye, Khan, and Testik9\n\n\n\n8 Transport Phenomena.9 “Environmental Fluid Mechanics”.\n\nMass balance\nThroughout all of this there has been a constant kc floating around, unaddressed. In practice this is usually a free parameter determined by fitting with experimental data. However it can also be determined by a mass balance.\nThe total molar flux through any plane z=m is the same for all m (in the region of fully developed flow), which is to say\n\\[ n_A = \\int_0^{2\\pi} \\int_0^{\\infty}  c_A \\bar{v}_z r dr d\\theta = \\mathrm{const} \\]\nWe also know what total molar flux is from the initial conditions of the jet\n\\[ n_A = c_0 v_0 {\\pi \\over 4} d_0^2 \\]\nand we can write the integral for any downstream distance z\n\\[ n_A = 2\\pi \\int_0^{\\infty}  c_A \\bar{v}_z r dr = 2 \\pi k k_c \\int_0^{\\infty} \\left(\\bar{v}_z \\over \\bar{v}_{z,max} \\right)^{Sc+1} \\xi d\\xi \\]\nand so we can write kc in terms of the other parameters\n\\[ k_c = { c_0 v_0 d_0^2 \\over 8 k I_c } \\]\nwhere\n\\[I_c = \\int_0^{\\infty} \\left(\\bar{v}_z \\over \\bar{v}_{z,max} \\right)^{Sc+1} \\xi d\\xi\\]\nrecalling that \\(k = {v_0 d_0 \\over \\sqrt{8I} }\\) where \\[I = \\int_0^{\\infty} \\left(\\bar{v}_z \\over \\bar{v}_{z,max} \\right)^{2} \\xi d\\xi\\]\nwe can simplify this to\n\\[ k_c = \\sqrt{I \\over 8 I_c^2} c_0 d_0 \\]\nor\n\\[ {c_A \\over c_0} = \\sqrt{I \\over 8 I_c^2} {d_0 \\over z} \\left(\\bar{v}_z \\over \\bar{v}_{z,max} \\right)^{Sc} \\]\nThese integrals can get difficult to solve analytically – except for the Gaussian case which is fairly simple – but are very easy to estimate numerically.10\n10 It is worth keeping in mind, when using the empirical constants provided in the literature, that they are often fitted parameters and as such mass and momentum conservation is not necessarily guaranteed.As an example, suppose we wish to calculate the constant for the Prandtl mixing length model, we have already imported the solution to the ode for the velocity profile and it is a simple matter to numerically integrate (using the trapezoidal rule) the two integrals in question.\nWe still need two parameters to complete the integration, the parameter a and ac or, equivalently, β and the Schmidt number Sc.\n\nusing NumericalIntegration: integrate\n\n# solutions of the ode\nϕ, F′ = sol.t, sol[2,:]\n\n# trim any unphysical values\nF′[F′.&gt;0] .= 0\n\n# parameters of the model\nβ = 0.0848\na = β/1.2277667062444657\nSc = 0.7\nf  = -F′./ϕ\nξ  = a.*ϕ\n\n# momentum balance integrand\nint = (f.^2).*ξ\nint[ξ.≤0] .= 0\n\n# mass balance integrand\nint_c = (f.^(Sc+1)).*ξ\nint_c[ξ.≤0] .= 0\n\nI   = integrate(ξ, int)\nI_c = integrate(ξ, int_c)\n\nk_c = √(I/(8*I_c^2))\n\n5.793131625346911\n\n\nThe other two models could also be integrated numerically, for example the eddy diffusivity model\n\nusing QuadGK: quadgk\n\nC₂ = 2*√(√(2)-1)/β\nf_ev(ξ) = ( 1 + (C₂*ξ/2)^2 )^-2\n\nI, err   = quadgk((ξ) -&gt; ξ*f_ev(ξ)^2, 0, Inf)\nI_c, err = quadgk((ξ) -&gt; ξ*f_ev(ξ)^(Sc+1), 0, Inf)\n\nk_c = √(I/(8*I_c^2))\n\n5.258197856093409\n\n\nand the Gaussian model\n\nc = log(2)/β^2\nf_emp(ξ) = exp(-c*ξ^2)\n\nI, err   = quadgk((ξ) -&gt; ξ*f_emp(ξ)^2, 0, Inf)\nI_c, err = quadgk((ξ) -&gt; ξ*f_emp(ξ)^(Sc+1), 0, Inf)\n\nk_c = √(I/(8*I_c^2))\n\n5.900934664729693\n\n\nIn these cases I followed my convention from the previous notebook of setting each of the velocity distributions to have the same width at half height, and using the same Schmidt number for the concentration profiles. This makes it more of an “apples to apples” comparison.\nLooking at the results we see that the constant for the concentration is smaller than the velocity profiles, indicating that the centerline concentration drops off faster than the velocity. Which makes some sense as we saw above that the concentration also spreads out radially more than the velocity.\nWe can combine the velocity and concentration profiles and get a sense of how the jet expands. Note that the plot is looking at the fully-developed jet, in this case ~4 diameters downstream.\n\n\n\n\n\n\n\n\nFigure 4: The flowfield and concentration contours for an example turbulent jet, Prandtl mixing length model\n\n\n\n\n\n\n\nPractical considerations\nIn most practical cases I’ve encountered, by far the easiest approach is to use a standard Gaussian model with parameters from literature. That said, there is a lot of variability of recommended parameters and some thought needs to go into what the model is being used for. Consider the following table giving model parameters for various gaseous jets entering into air.11\n11 Long, “Estimation of the Extent of Hazard Areas Around a Vent,” 7.\\[ {c_A \\over c_0} = k_2 {d_0 \\over z} \\sqrt{ \\rho_a \\over \\rho_j} \\exp \\left( - \\left(k_3 {r \\over z} \\right)^2 \\right) \\]\n\n\n\nJet\nRe\n\\(k_2\\)\n\\(k_3\\)\n\n\n\n\nCO₂\n50 000\n5.4\n9.2\n\n\nN₂\n27 000\n5.4\n7.9\n\n\nHe\n3 400\n4.1\n5.3\n\n\nair + 1.1% towngas\n67 000\n5.3\n8.8\n\n\nhot air\n67 000\n5.3\n8.8\n\n\nair + N₂O tracer\n27-57 000\n4.5\n7.1\n\n\nhot air\n18-25 000\n4.5\n7.1\n\n\nhot air\n30-60 000\n5.3\n7.8\n\n\nhot air\n10-20 000\n5.0\n6.4\n\n\nhot air\n—\n4.8\n7.1\n\n\nhot air\n—\n5.9\n7.7\n\n\n\nBelow is a plot showing the range of values this generates for the Gaussian jet model, along with the recommended parameters for use when estimating the extent of a hazardous area around a vent.12 The range of values is quite wide.\n12 Clearly the recommendation is a conservative approach, which is what you would want for a hazard analysis.\n\n\n\n\n\n\n\nFigure 5: Gaussian concentration profile, range of values and recommended constants from Long13\n\n13 “Estimation of the Extent of Hazard Areas Around a Vent”.\n\n\n\nIt is also worth noting that the concentration model breaks down when \\(z&lt;k_2\\), it will register concentrations greater than is possible. The normal way of dealing with this is a so-called top-hat model: chop off any concentrations \\(c_A &gt; c_0\\), though if the region of interest is primarily very close to the hole a different model should be used."
  },
  {
    "objectID": "posts/turbulent_jet_notes_part_2/index.html#temperature",
    "href": "posts/turbulent_jet_notes_part_2/index.html#temperature",
    "title": "More on Turbulent Jets",
    "section": "Temperature",
    "text": "Temperature\nThe temperature profiles follow directly from the velocity profiles in an entirely analogous way to concentration. The temperature profile is given by:\n\\[ {T - T_a \\over T_0 - T_a} = k_T {d_0 \\over z} \\left( \\bar{v}_z \\over \\bar{v}_{z,max} \\right)^{Pr} \\]\nWhere Ta is the ambient temperature, T0 is the jet temperature, and Pr is a turbulent Prandtl number. The constant kT is then determined by an energy balance, again entirely analogously to the case for concentration."
  },
  {
    "objectID": "posts/turbulent_jet_notes_part_2/index.html#volumetric-flow",
    "href": "posts/turbulent_jet_notes_part_2/index.html#volumetric-flow",
    "title": "More on Turbulent Jets",
    "section": "Volumetric Flow",
    "text": "Volumetric Flow\nThe volumetric flow-rate of the jet grows with the downstream distance, as the jet entrains the surrounding fluid. This can be calculated from an integral of the velocity distribution\n\\[ Q = \\int_0^{2\\pi} \\int_0^{\\infty} \\bar{v}_z r dr d\\theta \\]\nrecalling the definition of \\(\\bar{v}_z\\) in terms of the function F and making the change of variables to ξ\n\\[ Q = \\int_0^{2\\pi} \\int_0^{\\infty} {-k \\over z} {F^{\\prime} \\over \\xi} (z \\xi) z d\\xi d\\theta \\]\n\\[   = 2 \\pi k z \\int_0^{\\infty} -F^{\\prime} d\\xi \\]\n\\[   = 2 \\pi k z \\left[ -F\\left(\\xi\\right) \\right]_{0}^{\\infty} \\]\nlet’s define the limit\n\\[ \\lim_{\\xi \\to \\infty} F\\left(\\xi\\right) = F_{\\infty}\\]\nand recall that from boundary conditions F(0)=0, so\n\\[ \\left[ -F\\left(\\xi\\right) \\right]_{0}^{\\infty} = -F_{\\infty} \\]\nrecalling the definition of k\n\\[ k = {v_0 d_0 \\over \\sqrt{8 I} }\\]\nand putting it all together we get\n\\[ Q = -{ 2 \\pi \\over \\sqrt{8 I} } F_{\\infty} v_0 d_0 z  \\]\nIt is often more convenient to put things in in dimensionless terms, so let\n\\[ Q_0 = {\\pi \\over 4} v_0 d_0^2 \\]\nwhich gives us\n\\[ {Q \\over Q_0} = - \\sqrt{8 \\over I} F_{\\infty} {z \\over d_0} \\]\nwe can define a new constant kQ\n\\[ k_Q = - \\sqrt{8 \\over I} F_{\\infty} \\]\nand finally\n\\[ {Q \\over Q_0} = k_Q {z \\over d_0} \\]\nWhere \\(k_Q\\) is some constant defined by the particular model and the corresponding model parameter.\nFor the Prandtl mixing length model we can compute this constant numerically\n\na = β/1.2277667062444657\n\nI   = integrate(ξ, int)\nF∞  = a^2*sol[1,end]\n\nk_Q = -√(8/I)*F∞\n\n0.3026710383017731\n\n\nFor the eddy viscosity model this can be worked out analytically to be \\[k_Q = {4\\sqrt{3} \\over C_2}\\]\n\nC₂ = 2*√(√(2)-1)/β\n\nk_Q = 4*√3/C₂\n\n0.4564301431180997\n\n\nThe Gaussian model can also be worked out analytically, giving \\[k_Q = {4 \\over \\sqrt{2 c}}\\]\n\nc = log(2)/β^2\n\nk_Q = 4/√(2*c)\n\n0.28808995465769605\n\n\nThe literature14 gives \\(k_Q = 0.32\\) which compares well with the calculations above. Though it’s worth noting that the eddy viscosity model over predicts the volumetric flow rate quite noticeably, this is not surprising considering that it has fatter tails than either the Prandtl mixing length model or the Gaussian model. In the tails is where the eddy viscosity model no longer matches well with the observed data, so this is just a weakness of the model itself.\n14 Rajaratnam, Turbulent Jets."
  },
  {
    "objectID": "posts/turbulent_jet_notes_part_2/index.html#conclusions",
    "href": "posts/turbulent_jet_notes_part_2/index.html#conclusions",
    "title": "More on Turbulent Jets",
    "section": "Conclusions",
    "text": "Conclusions\nThis was just a brief tour of the different parameters that can be calculated from the velocity distribution or stream function, once it is known. In practice, very often the constants encountered along the way are treated like fitting parameters and so it is always worth keeping in mind what the model is being used for and what conditions must be strictly followed or not. For example, if one wants a very accurate fit to concentration, that may result in mass no longer being conserved because a model may not have enough degrees of freedom to fit the one and guarantee the other.\nIf you are not fitting data, there is a wide range of parameters given in the literature and I think it is more important to find a set of parameters that work for the situation of interest – for whichever model they were fit, but generally it is Gaussian – than it is to fiddle around in the margins of whether or not one should use a Prandtl mixing length model versus an eddy viscosity model. Very often the answer is going to be Gaussian because that’s what the best model in the literature was fit to."
  },
  {
    "objectID": "posts/turbulent_jet_notes_part_2/index.html#references",
    "href": "posts/turbulent_jet_notes_part_2/index.html#references",
    "title": "More on Turbulent Jets",
    "section": "References",
    "text": "References\n\n\nBird, R. Byron, Warren E. Stewart, and Edwin N. Lightfoot. Transport Phenomena. 2nd ed. Hoboken, NJ: John Wiley & Sons, 2007.\n\n\nKaye, Nigel B., Abdul A. Khan, and Firat Y. Testik. “Environmental Fluid Mechanics.” In Handbook of Environmental Engineering, edited by Myer Kutz. New York: John Wiley & Sons, 2018.\n\n\nLong, V. D. “Estimation of the Extent of Hazard Areas Around a Vent.” Second Symposium On Chemical Process Hazards, 1963, 6–14.\n\n\nRajaratnam, N. Turbulent Jets. Amsterdam: Elsevier, 1974."
  },
  {
    "objectID": "posts/turbulent_jet_example/index.html",
    "href": "posts/turbulent_jet_example/index.html",
    "title": "Turbulent Jet Example - Acetylene Leak",
    "section": "",
    "text": "In previous examples I discussed release scenarios involving vapour clouds spreading over a large area, carried by the wind. In those examples the momentum of the jet of fluid was not very important relative to the ambient wind conditions and could be ignored. In this example I am looking at the opposite extreme, a release from a pressure vessel inside a building where the momentum of the jet dominates."
  },
  {
    "objectID": "posts/turbulent_jet_example/index.html#the-scenario",
    "href": "posts/turbulent_jet_example/index.html#the-scenario",
    "title": "Turbulent Jet Example - Acetylene Leak",
    "section": "The Scenario",
    "text": "The Scenario\nConsider, for an example, a leak from an acetylene cylinder inside a large building, such as in a warehouse or shop. We imagine, for convenience, that the air within the building is quiescent. For the sake of an example suppose the leak is a 1/4 in. hole, similar in diameter to a typical acetylene hose, and that the operating pressure at that point is 15psig1 We are interested in exploring the concentration distribution as the acetylene jets into the air and mixes, with our reference concentration of interest being half the LEL of 2.5%(vol).\n1 From CGA G-1 2009 the safe operating pressure of an acetylene system\nusing Unitful: @u_str, ustrip\n\ninch = ustrip(u\"m\", 1u\"inch\") # unit conversion inch-&gt;m\npsi = ustrip(u\"Pa\", 1u\"psi\")  # unit conversion psi-&gt;Pa\n\np₂ = 14.7psi   # atmospheric pressure, Pa absolute\nT₂ = 25+273.15 # ambient temperature, K\n\nd  = 0.25inch  # diameter of the hole, m\np₁ = 15psi+p₂ # pressure of the acetylene, Pa absolute\nT₁ = T₂        # the release temperature, K\n\n298.15\n\n\nWe can look up some properties of acetylene in Perry’s2\n2 Poling et al., “Physical and Chemical Data”.\n# universal gas constant, J/mol/K\nR = 8.31446261815324 \n\n# ideal gas density, kg/m³\nρ(p,T;MW) = (p*MW)/(R*T)/1000\n\n# gas viscosity correlation, Pa*s\nμ(T;C) = (C[1]*T^(C[2]))/(1+(C[3]/T)+(C[4]/T^2)) \n\n# Properties of Acetylene\nMWⱼ = 26.037 # molar mass, kg/kmol\nLEL = 0.025  # Lower explosive limit, vol/vol\nk   = 1.26   # ratio cp/cv at 15C\nμⱼ  = μ(T₁;C=[1.2025e-6,0.4952,291.4,0])\nρ₁  = ρ(p₁,T₁;MW=MWⱼ)\n\n# Properties of Air\nMWₐ = 28.960  # molar mass, kg/kmol\nρ₂  = ρ(p₂,T₂;MW=MWₐ)\n\n1.1840386427594014"
  },
  {
    "objectID": "posts/turbulent_jet_example/index.html#the-release-rate",
    "href": "posts/turbulent_jet_example/index.html#the-release-rate",
    "title": "Turbulent Jet Example - Acetylene Leak",
    "section": "The Release Rate",
    "text": "The Release Rate\nWe can model the release as a gas jet3 where the gas is ideal and the expansion through the jet is an isentropic process4\n3 AIChE/CCPS, Guidelines for Consequence Analysis of Chemical Releases., 29.4 AIChE/CCPS, Guidelines for Consequence Analysis of Chemical Releases. has a mistake in equation 2.16, the version given here is correct.\\[ G = \\rho u = c_d \\sqrt{ \\rho_1 p_1 \\left( 2 k \\over k-1 \\right) \\left[ \\left(p_2 \\over p_1\\right)^{2 \\over k} - \\left(p_2 \\over p_1\\right)^{k+1 \\over k} \\right]} \\]\nfor non-choked flow and\n\\[ G = c_d \\sqrt{ \\rho_1 p_1 k \\left( 2 \\over k+1 \\right)^{k+1 \\over k-1} } \\]\nfor choked flow, which occurs when\n\\[ \\left(p_2 \\over p_1 \\right) \\lt \\left( 2 \\over k+1 \\right)^{k \\over k-1} \\]\nWhere G is the mass velocity of acetylene discharged through the hole (in kg/m²/s), cd is the discharge coefficient which can be assumed to be 0.61,5 and the rest are as defined earlier. I am assuming, here, that the hole is circular for simplicity.\n5 AIChE/CCPS, 30.\n(p₂/p₁) &lt; (2/(k+1))^(k/(k-1)) \n\ntrue\n\n\nTherefore the flow is choked and\n\nc_d = 0.61\n\nG = c_d * √(ρ₁*p₁*k*(2/(k+1))^((k+1)/(k-1)) )\n\n267.1556913840265\n\n\nThe density at the orifice is reduced, through the expansion and, for an isentropic process, is related to the pressure by\n\\[ {\\rho_o \\over \\rho_1} = \\left( p_o \\over p_1 \\right)^{1 \\over k} \\]\nWhere subscript o indicates at the orifice. At this point, after the expansion \\(p_o = p_2\\) and\n\\[ \\rho_o = \\rho_1 \\left( p_o \\over p_1 \\right)^{1 \\over k} \\]\n\nρₒ = ρ₁*(p₂/p₁)^(1/k)\n\n1.2307940295609565\n\n\nThe velocity at the orifice, i.e. after the gas has expanded, is then\n\\[ u_o = {G \\over \\rho_o} \\]\n\nuₒ = G/ρₒ\n\n217.05962571115586"
  },
  {
    "objectID": "posts/turbulent_jet_example/index.html#jet-behavior",
    "href": "posts/turbulent_jet_example/index.html#jet-behavior",
    "title": "Turbulent Jet Example - Acetylene Leak",
    "section": "Jet Behavior",
    "text": "Jet Behavior\nTo model the concentration profile I am going to assume a turbulent jet, from a circular hole, mixing with air. In this case the density of air and acetylene are similar and so a simple turbulent jet model is appropriate. If there was a significant difference in densities then a density correction would be needed, however for many applications “close” means a ratio of ambient to jet densities between6\n6 Poleshaw and Golub., “Jets”.\\[ \\frac{1}{4} \\le { \\rho_{a} \\over \\rho_{j} } \\le 4 \\]\nWhere subscript a indicates the ambient fluid and j the jet.\nCircular turbulent jets expand by entraining ambient fluid, tracing out a cone defined by a jet angle \\(\\alpha \\approx 15-25^\\circ\\). The mixing layer penetrates into the jet forming the potential cone, inside is pure jet material and outside is mixed. After approximately 6 hole diameters the region is fully developed.7\n7 Revill, “Jet Mixing”.\n\n\n\n\n\nFigure 1: A turbulent jet expanding into a quiescent atmosphere.\n\n\n\nEmpirical approximations of the velocity, and concentration, profiles are often given with respect to this jet angle or, equivalently, the slope of line (i.e. \\(\\tan \\frac{\\alpha}{2}\\))\nAnother important factor is the Reynolds number, the jet is fully turbulent when \\(Re \\gt 2000\\), where the Reynolds number is calculated with respect to the initial jet velocity and jet diameter (i.e. the hole diameter)\n\\[ Re = { \\rho u d \\over \\mu } = { G d \\over \\mu }\\]\n\n0.25 &lt; (ρ₂/ρₒ) &lt; 4\n\ntrue\n\n\n\nRe = G*d/μⱼ\n\nRe &gt; 2000\n\ntrue\n\n\nThe densities are within the appropriate range and the flow is fully turbulent, so the turbulent jet model requirements are satisfied.\n\nVelocity and Concentration distributions\nThere are many different empirical velocity distributions as well as velocity distributions derived from theories of turbulent mixing available in various references. Mostly of the same general type (gaussian), but parametrized slightly differently. However, in my experience, there are far fewer concentration distributions available, this is not too critical due to an interesting result in turbulent mass transfer for jets8\n8 Bird, Stewart, and Lightfoot, Transport Phenomena, 416.\\[ { C \\over C_{max} } = \\left( v_z \\over v_{z,max} \\right)^{Sc_t} \\]\nThat is, at a given distance z away from the hole, the concentration profile is the velocity profile raised to the power \\(Sc_t\\) – the turbulent Schmidt number. Experimentally this is approximately 0.7. Note also that \\(C_{max}\\) and \\(v_{z,max}\\) are taken at the centerline. Physically this means that the concentration profile, at a given downstream distance, is wider than the velocity distribution; concentration expands more.\nA similar way of capturing the same phenomenon that is often seen with empirical velocity distributions is to define a width parameter \\(b\\) and note that the equivalent width for the concentration profile is \\(1.17b\\)9 and substitute in accordingly.\n9 Kaye, Khan, and Testik, “Environmental Fluid Mechanics”.10 Lees, Loss Prevention in the Process Industries, 15/140.In this example I am using the empirical concentration given in Lees10 for simplicity\n\\[ {C \\over C_0 } = k_2 \\left( d_h \\over z \\right) \\left( \\rho_z \\over \\rho_o \\right)^{0.5} \\exp \\left( - \\left( k_3 r \\over z \\right)^2 \\right) \\]\nNote also the ratio of densities, the density \\(\\rho_z\\) is the density of the jet at some distance z and it is common to conservatively take this as \\(\\rho_a\\).\nThe parameters \\(k_2\\) and \\(k_3\\) are empirically derived for the particular jet and \\(k_2\\) is a function of Reynolds number below \\(Re \\lt 20000\\).11 The conservative values suggested are 6 and 5 respectively.\n11 Long, “Estimation of the Extent of Hazard Areas Around a Vent”.\nfunction C(r, z; C₀=1.0, k₂=6, k₃=5, d=d, ρz=ρ₂, ρₒ=ρₒ)\n    C = C₀ * k₂ * (d/z) * √(ρz/ρₒ) * exp(-(k₃*r/z)^2)\nend\n\nC (generic function with 1 method)\n\n\n\n\n\n\n\n\n\n\nFigure 2: The centerline concentration of acetylene as a concentration of downstream distance.\n\n\n\n\n\nAt this point it is worth pointing out that the model of the jet is independent of the discharge rate. The concentration profile is only a function of the hole diameter and the fluid density. The velocity in the jet, and the amount of air entrained in the jet, do depend strongly on the initial discharge rate but in such a way that the concentration does not. As the jet velocity increases proportionally more air is entrained and the concentration profile remains constant.\n\n\n\n\n\n\n\n\nFigure 3: Concentration contours at the release elevation."
  },
  {
    "objectID": "posts/turbulent_jet_example/index.html#explosive-mass",
    "href": "posts/turbulent_jet_example/index.html#explosive-mass",
    "title": "Turbulent Jet Example - Acetylene Leak",
    "section": "Explosive Mass",
    "text": "Explosive Mass\nNow that we have a model of the jet, showing the concentration of acetylene, the most relevant parameter we would want to know is the explosive mass such that some blast modeling could be done.\nThe most obvious way to do this is to integrate over the jet, using cylindrical coordinates for convenience\n\\[ m_e = \\int \\rho C(r,z) dV = 2\\pi \\rho_o \\int_{0}^{\\infty} \\int_{0}^{\\infty} C(r,z) r dr dz \\]\nExcept that we define the explosive mass to be the volume where \\(C &gt; \\frac{1}{2} LEL\\). A lazy way to do this is to define a function that equals \\(C\\) if it is \\(\\gt \\frac{1}{2} LEL\\) and zero otherwise.\nThe potential core region is poorly described by this model, and the closer to the origin of the jet the more un-physical the results: giving concentrations greater than 100% and being undefined completely at the origin. One way of hand waving this away is to chop off any concentrations above 100%.\n\nfunction igrd(v; lim=0.5*LEL)\n    r, z = v\n    \n    if z&gt;0\n        c = C(r,z)\n        c = c&lt;lim ? 0 : min(1,c)\n    else\n        c = 0\n    end\n\n    return r*c\nend\n\nigrd (generic function with 1 method)\n\n\nIntegrating over some plausible bounds, taken by looking at the plots above, gives the volume of acetylene.\n\nusing HCubature: hcubature\n\nI, err = hcubature(igrd, [0, 0], [0.25, 2.0], atol = 1e-8)\n\n(0.0008207940258726464, 9.999922827914883e-9)\n\n\nWhich can be plugged into the equation to calculate the final explosive mass.\n\nmₑ = 2*π*ρₒ*I\n\n0.006347452155224944\n\n\nTo give a sense of how much this is, the explosive mass is equivalent to ~1s of discharge at the steady state discharge rate.\n\nm = G*(π/4)*d^2\n\nmₑ/m\n\n0.7502356087241902"
  },
  {
    "objectID": "posts/turbulent_jet_example/index.html#conclusions",
    "href": "posts/turbulent_jet_example/index.html#conclusions",
    "title": "Turbulent Jet Example - Acetylene Leak",
    "section": "Conclusions",
    "text": "Conclusions\nTurbulent jet mixing is a much simpler model for estimating releases, especially when using empirical models, compared to models for plumes influences by buoyancy and wind. There are much fewer parameters that need to be estimated.\nOne big weakness to the model as presented here is that it does not take into account the enclosed space. If the assumption is that the warehouse is large and ignition sources are numerous then that likely doesn’t matter, the acetylene leak will ignite before it has a chance to accumulate. However it will grossly underestimate the potential explosive mass that could develop as the acetylene disperses through the air of warehouse, since the model presumes the ambient air has no acetylene in it and is effectively infinite in extent.\nThis limitation would, for me, motivate exploring more detailed models of gas build up in enclosed spaces"
  },
  {
    "objectID": "posts/turbulent_jet_example/index.html#references",
    "href": "posts/turbulent_jet_example/index.html#references",
    "title": "Turbulent Jet Example - Acetylene Leak",
    "section": "References",
    "text": "References\n\n\nAIChE/CCPS. Guidelines for Consequence Analysis of Chemical Releases. New York: American Institute of Chemical Engineers, 1999.\n\n\nBird, R. Byron, Warren E. Stewart, and Edwin N. Lightfoot. Transport Phenomena. 2nd ed. Hoboken, NJ: John Wiley & Sons, 2007.\n\n\nKaye, Nigel B., Abdul A. Khan, and Firat Y. Testik. “Environmental Fluid Mechanics.” In Handbook of Environmental Engineering, edited by Myer Kutz. New York: John Wiley & Sons, 2018.\n\n\nLees, Frank P. Loss Prevention in the Process Industries. 2nd ed. Oxford: Butterworth-Heinemann, 1996.\n\n\nLong, V. D. “Estimation of the Extent of Hazard Areas Around a Vent.” Second Symposium On Chemical Process Hazards, 1963, 6–14.\n\n\nPoleshaw, Yury V., and V. V. Golub. “Jets.” In Thermopedia, 2013. https://doi.org/10.1615/AtoZ.j.jets.\n\n\nPoling, Bruce E., George H. Thomson, Daniel G. Friend, Richard L. Rowley, and W. Vincent Wilding. “Physical and Chemical Data.” In Perry’s Chemical Engineers’ Handbook, edited by Don W. Green, 8th ed. New York: McGraw Hill, 2008.\n\n\nRevill, B. K. “Jet Mixing.” In Mixing in the Process Industries, edited by N. Harnby, M. F. Edwards, and A. W. Nienow, 2nd ed. Oxford: Butterworth-Heinemann, 1992."
  },
  {
    "objectID": "posts/impossible_bowling/index.html",
    "href": "posts/impossible_bowling/index.html",
    "title": "Impossible bowling",
    "section": "",
    "text": "While bowling, this week, an interesting question came up: is it possible to get every score from 1 to 450 in a game of five pin bowling? Or, to flip it around, is there a score that you can never get no matter how fancy your bowling? The answer is not immediately obvious!"
  },
  {
    "objectID": "posts/impossible_bowling/index.html#the-rules-of-five-pin-bowling",
    "href": "posts/impossible_bowling/index.html#the-rules-of-five-pin-bowling",
    "title": "Impossible bowling",
    "section": "The rules of five pin bowling",
    "text": "The rules of five pin bowling\nFive pin bowling uses five pins but, unlike ten pin bowling, the pins are worth different amounts. Notably no pin is worth 1, and so a score of 1 is the first impossible score.\n\n\n\n\n\n\nFigure 1: The points value of each pin in five-pin bowling.\n\n\n\nLike ten pin bowling, if a strike or a spare is recorded in a given frame then the scores of subsequent ball(s) are counted in that frame, as well as the frame in which they were thrown. So, for example, if I throw a strike in the first frame I don’t actually know what to write on the score sheet for the first frame until the second, and possibly third, frames have been thrown. I know it is at least 15, but until I throw the next ball it could be anything up to 45. This was what initially gave me pause. It adds a layer of complexity since the possible scores for a given frame depend on what happens next.\nBy symmetry, though, this way of scoring is equivalent to every strike and spare adding a multiplier to the next frame, and each frame is just scored counting whatever the pinfall is and applying the multiplier (no looking backwards). So, if I throw a strike in the first frame, then I record a 15 for the first frame and double count the next two balls. If I have thrown two strikes in a row then I triple count the first subsequent ball and double count the next one. This is a weird way of managing a score sheet, for bowlers, but makes it a lot easier to reason about the possible scores, since you don’t have to constantly be looking back two or three frames. This passing forward score sheet looks different to a regular one: The maximum score for the first frame is now 15, and for the second frame 30, and in the tenth frame it is possible to score 90 points. On a conventional score sheet the max score in any frame is 45."
  },
  {
    "objectID": "posts/impossible_bowling/index.html#trying-everything",
    "href": "posts/impossible_bowling/index.html#trying-everything",
    "title": "Impossible bowling",
    "section": "Trying everything",
    "text": "Trying everything\nWhile hanging out at the lanes a few obvious impossible scores got thrown out: a 1, obviously, but also a 449 – there’s no way to throw a 14 with the last ball in the tenth frame. But the question still lingered: are there any other gaps? It was not immediately obvious, to my bowling team, how you would figure that out without checking.\nMaybe we can brute-force this and try every conceivable bowling game? However there are a lot of possible bowling games. As a first pass, there are thirty balls thrown in a game and each ball has up to fourteen possible pinfall scores (0 through 15 excluding 1 and 14). This would give 1430 possible bowling games. Even if it took a single nanosecond to evaluate each game that would take longer than the current age of the universe to work through.\nBut that’s not a great upper bound, it doesn’t take into account the rules of bowling: you can only knock down up to five pins in any given frame, for example if the first ball scores a 13 then the second ball doesn’t get to choose from fourteen possibilities, it gets to chose from two: 0 and 2. Still, it is going to be a large number. The vast majority of those games are going to be completely redundant, since we are only looking for scores from 2 to 450."
  },
  {
    "objectID": "posts/impossible_bowling/index.html#nothing-fancy",
    "href": "posts/impossible_bowling/index.html#nothing-fancy",
    "title": "Impossible bowling",
    "section": "Nothing fancy",
    "text": "Nothing fancy\nThe easiest case to look at is when one never throws a strike or spare. In this case the possible scores for each frame are the same: just what you can get from knocking down any subset of the pins. This happens to be anything from 0 to 15 except 1 and 14. That’s easy enough to see just by inspection.\nThis also leads to a (kinda loose) argument for why you should be able to get anything from 0 to 150 except 1 and 149:\nSuppose you are playing game with n frames and your goal is a score \\(x \\le 15 n\\).\nIf \\(x \\not \\equiv 1 (\\textrm{mod} 15)\\) and \\(x \\not \\equiv 14 (\\textrm{mod} 15)\\) then you can always get from a multiple of 15 to the final score in one frame.\nIf \\(x \\equiv 1 (\\textrm{mod} 15)\\) or \\(x \\equiv 14 (\\textrm{mod} 15)\\) then you can’t get from a multiple of 15 to the final score in one frame, this is because you cannot score 1 or 14 in one frame. You can score 1 more than a multiple of 15 if you have two frames remaining: score a 13 in the first and a 2 in the next. Similarly you can score 1 less than a multiple of 15 if you have two frames remaining: score a 7 in the first frame and a 7 in the second.\nSince for any x such that \\(1 \\lt x \\le 15 (n-1)\\) there are \\(\\ge 2\\) frames remaining, all of those scores can thus be achieved.\nWhat remains is the x such that \\(15(n-1) \\lt x \\lt 15 n\\) and \\(x \\equiv 14 (\\textrm{mod} 15)\\), this single score is not achievable in a game with no strikes and spares.\nWhich is all to say there are only two impossible scores: 1 and 15n -1 or 149 in a standard ten frame game.\nI suspect that, if you wanted to put the work in, you could extend this argument to include spares and strikes, with all of the complications around how the 10th frame is scored. But an alternative is to just look through all possible scores and try and find a game that achieves it, using this general approach as a guide.\nThis is pretty easy to do when only looking at the case where there are no strikes or spares: I generate a list of possible scores for a single frame (a possible move I can take towards my goal), sorted largest to smallest.\n\nbasic_moves = [ n for n in range(16) if n not in [1,14] ]\nbasic_moves.reverse()\n\nThen I define a function that recursively walks through the tree of possible games, always picking the largest viable move at each frame. If it finds an answer it returns it (in reverse order), if it exhausts the possible moves then it returns an empty list.1\n1 This code stops once it has found a single valid solution, it could be extended very easily to find every valid solution, however the space of possible games is huge.\ndef make_moves(cur_frame, cur_score, max_frame, target, moves=basic_moves):\n    if cur_frame == max_frame:\n        return [0] if cur_score == target else []\n    else:\n        next_frame = cur_frame + 1\n        mn, mx = min(moves), max(moves)\n        n = max_frame - next_frame\n        r = target - cur_score\n        for move in filter(lambda x: n*mn &lt;= (r-x) &lt;= n*mx, moves):\n            new_score = cur_score + move\n            advance = make_moves(next_frame, new_score, max_frame, target, moves)\n            if len(advance) &gt; 0:\n                advance.append(move)\n                return advance\n        else:\n            return []\n\nLooping through all the scores: \\(0 \\le score \\le 150\\) yields the impossible to bowl scores.\n\nfor score in range(151):\n    game = make_moves(0,0,10,score)\n    if len(game) == 0:\n        print(\"Score {0} is not possible\".format(score))\n\nScore 1 is not possible\nScore 149 is not possible\n\n\nWhich is what I expected, good news as I will be building off this general strategy for the cases where strikes and spares are included.\n\nDetour: what about with no gutters?\nAnother question that comes to mind is: what if you were restricted to always hitting a pin, no gutter balls? Now you can’t get a score less than 7 (equivalent to hitting 2-2-3, the lowest pins). Does this change anything?2\n2 In five pin it is actually possible to bowl between the pins and hit nothing without it technically being a gutter ball, and you can bowl into the blank spots left by pins that were already knocked down to score zeros without putting it in the gutter. I am using the term gutter ball loosely.Probably I could go back and look at the math again, but the nice thing about having written code is that I can just change the space of possible moves and run it again.\n\nno_gutters = [ n for n in range(7,16) if n != 14 ]\nno_gutters.reverse()\n\n\nfor score in range(70,151):\n    game = make_moves(0,0,10,score,no_gutters)\n    if len(game)==0:\n        print(\"Score {0} is not possible, with no gutters\".format(score))\n\nScore 149 is not possible, with no gutters\n\n\nSo nothing really changes. I mean you can’t get a score &lt;70, obviously, but this doesn’t open up any gaps in possible scores either.\nIt does mean the strategy changes, now the code takes the biggest strides it can until the remainder is a multiple of 7 then runs out the game with a string of 7s.3\n3 You may have noticed an extra “frame” at the end with a score of 0. This is because, in five pin bowling, the last frame has special rules. You always get 3 balls in the last frame, even if your first two are a strike or spare. In this case, with no strikes or spares allowed by design, that extra scoring doesn’t enter into it.\n[ frame for frame in reversed(make_moves(0,0,10,100,no_gutters)) ]\n\n[15, 15, 15, 13, 7, 7, 7, 7, 7, 7, 0]"
  },
  {
    "objectID": "posts/impossible_bowling/index.html#sparing-no-effort",
    "href": "posts/impossible_bowling/index.html#sparing-no-effort",
    "title": "Impossible bowling",
    "section": "Sparing no effort",
    "text": "Sparing no effort\nAdding in spares means I can’t easily track the state of each frame with an integer, like I did for the case with deadwood every frame. Now I need to track three different properties for a given frame:\n\nwhat was scored in the frame\nwhether a strike or a spare was recorded\nwhether this is the end of the last frame (i.e am I done bowling yet?)\n\nInstead of diving into the full set of scoring rules for everything, I’m going to take one baby step forward and add in a data structure to track the state of the frame and select from two sets of possibilities for the subsequent frame: was there a spare or not?\nThe data structure I’m using is just a struct, tracking the score, whether it is a “special” frame and whether or not it is the end of the game.\n\nclass SingleFrame:\n    def __init__(self, score, special=None, end=False):\n        self.score = score\n        self.special = special\n        self.end = end\n\nInstead of a list of integers for possible moves, I now need a list of possible SingleFrame objects that represent a possible frame, now including the possibility of a spare. Note the pass forward approach to scoring: a spare in a regular frame is only worth 15.\n\nsingle_frame_moves = [ SingleFrame(score) for score in basic_moves ]\nsingle_frame_moves.insert(0, SingleFrame(15,\"spare\"))\n\nNow I iterate through the possible first, second, and third balls for a frame following a spare. There are more possible scores here since the first ball will be double counted. First I exhaustively generate every combination then use set() to extract only the unique elements. For this purpose it doesn’t matter how many ways you can get a given score.\n\nspares = [ 2*f+s for f in basic_moves \n                 for s in filter(lambda x: f+x==15,basic_moves)]\n\nspares = list(set(spares))\nspares.sort(reverse=True)\n\nnon_spares = [ 2*f+s+t for f in basic_moves\n                       for s in filter(lambda x: f+x&lt;15, basic_moves)\n                       for t in filter(lambda x: f+s+x&lt;=15, basic_moves) ]\n\nnon_spares = list(set(non_spares))\nnon_spares.sort(reverse=True)\n\nNow I generate a list of moves by combining the spares and non-spares. They are arranged such that the code tries the spares first before the non-spares, going from largest to smallest. There are now 42 possible ways of scoring a frame when spares are included (versus only 14 when they aren’t).\n\nspare_frame_moves = [ SingleFrame(score,\"spare\") for score in spares ]\nspare_frame_moves += [ SingleFrame(score) for score in non_spares ]\n\nlen(spare_frame_moves)\n\n42\n\n\nThere are only two scores that are not achievable in the frame following a spare: 1 and 29\n\n[ s for s in range(31) if s not in [x.score for x in spare_frame_moves] ]\n\n[1, 29]\n\n\nAdding spares has also complicated determining if a move is valid or not. Since scoring now depends on the state of a given move – is it a spare or not – this impacts the bounds of possible scores that can follow any given move. Instead of putting this all into the same function, as I did before, I have broken it out into its own function that decides, given a move, a remaining number of frames, and a remaining number of points to pick up, is the move valid.4\n4 There is an extra move at the end because of the last frame rule: A spare at the start of the last frame leads to an extra ball, but one that can only count for up to 15.\ndef valid_spare_moves(move, n, r, mn=0, mx=15):\n    if move.special==\"spare\":\n        up = (2*n + 1)*mx\n    else:\n        up = (1 + 2*max(n-1,0) + 1)*mx\n    \n    return n*mn &lt;= (r - move.score) &lt;= up\n\nThe bulk of the main function is the same. The one exception is that it now tracks whether the previous frame was a spare (with was_spare) and uses this to determine how to finish the last frame: if the last frame was a spare, then an extra ball is thrown but with no multiplier.\n\ndef make_spare_moves(cur_frame, cur_score, max_frame, target, \n                     moves=single_frame_moves, was_spare=False):\n    if cur_frame == max_frame:\n        if cur_score == target:\n            return [SingleFrame(0,False,True)]\n        elif was_spare and (target - cur_score) in basic_moves:\n            # extra ball in the last frame\n            return [SingleFrame(target-cur_score,False,True)]\n        else:\n            return []\n    else:\n        next_frame = cur_frame + 1\n        n = max_frame - next_frame\n        r = target - cur_score\n        for move in filter(lambda x: valid_spare_moves(x,n,r), moves):\n            new_score = cur_score + move.score\n            if move.special == \"spare\":\n                next_moves = spare_frame_moves\n            else:\n                next_moves = single_frame_moves\n            \n            advance = make_spare_moves(next_frame, new_score, max_frame, target, next_moves, move.special==\"spare\")\n            if len(advance) &gt; 0:\n                advance.append(move)\n                return advance\n        else:\n            return []\n\nLooping through all the scores: \\(0 \\le score \\le 300\\) yields the impossible to bowl scores, when spares are allowed.\n\nfor score in range(301):\n    game = make_spare_moves(0,0,10,score)\n    if len(game)==0:\n        print(\"Score {0} is not possible, with only spares allowed\".format(score))\n\nScore 1 is not possible, with only spares allowed\nScore 299 is not possible, with only spares allowed\n\n\nWhich is perhaps not surprising, we still can’t get 1 less than the largest multiple of 15 because we cannot bowl a 14 with the extra ball at the end of the 10th frame."
  },
  {
    "objectID": "posts/impossible_bowling/index.html#in-striking-distance-of-the-final-answer",
    "href": "posts/impossible_bowling/index.html#in-striking-distance-of-the-final-answer",
    "title": "Impossible bowling",
    "section": "In striking distance of the final answer",
    "text": "In striking distance of the final answer\nThis puts me in a good position to try a full game, I need to add the possibility of a strike. For single frame scores with nothing special before them, this just means adding a third way to score a 15. There are now 16 possible moves.\n\n# all the different scores for a frame following a regular frame\n\nsingle_frame_moves = [ SingleFrame(score) for score in basic_moves ]\nsingle_frame_moves.insert(0, SingleFrame(15,\"spare\"))\nsingle_frame_moves.insert(0, SingleFrame(15,\"strike\"))\n\nlen(single_frame_moves)\n\n16\n\n\nSimilarly the possible ways to follow a spare are the same as before, except that there is no way to “spare” with 30. Scoring a 30 after a spare requires that one throw a strike.\n\n# all the different scores for a frame following a spare\n\nspare_frame_moves = [ move for move in spare_frame_moves if move.score !=30 ]\nspare_frame_moves.insert(0, SingleFrame(30,\"strike\"))\n\nlen(spare_frame_moves)\n\n42\n\n\nFollowing a single strike the rules are different: the first two balls count twice and the third counts once. Here I exhaustively generate all strikes, spares, and remaining that could follow a single strike and then trim only to the unique scores. This is a much smaller set as all spares that follow a strike must, by definition, have a score of 30.\n\n# all the different scores for a frame following a single strike\n\nnon_spares = [ 2*f+2*s+t for f in filter(lambda x: x!=15,basic_moves)\n                         for s in filter(lambda x: f+x&lt;15, basic_moves)\n                         for t in filter(lambda x: f+s+x&lt;=15, basic_moves) ]\n\nnon_spares = list(set(non_spares))\nnon_spares.sort(reverse=True)\n\nsingle_strike_moves = [ SingleFrame(30,\"strike\") ]\nsingle_strike_moves += [ SingleFrame(30,\"spare\") ]\nsingle_strike_moves += [ SingleFrame(score) for score in non_spares ]\n\nlen(single_strike_moves)\n\n30\n\n\nFollowing a double strike the rules are different again: the first ball is triple counted, the second double counted, and the third single counted.\n\n# all the different scores for a frame following 2 strikes\n\nspares = [ 3*f+2*s for f in filter(lambda x: x!=15,basic_moves)\n                   for s in filter(lambda x: f+x==15,basic_moves)]\n\nspares = list(set(spares))\nspares.sort(reverse=True)\n\nnon_spares = [ 3*f+2*s+t for f in filter(lambda x: x!=15,basic_moves)\n                         for s in filter(lambda x: f+x&lt;15, basic_moves)\n                         for t in filter(lambda x: f+s+x&lt;=15, basic_moves) ]\n\nnon_spares = list(set(non_spares))\nnon_spares.sort(reverse=True)\n\ndouble_strike_moves = [ SingleFrame(45,\"strike\") ]\ndouble_strike_moves += [ SingleFrame(score,\"spare\") for score in spares ]\ndouble_strike_moves += [ SingleFrame(score) for score in non_spares ]\n\nlen(double_strike_moves)\n\n55\n\n\nThis change in scoring, allowing for triple scoring, changes the bounds of possible scores following a given move. Now, after a strike, the next frame could count triple. But there are also potentially two more balls in the 10th frame that don’t count equally towards the upper bound on the score. The function to check for valid moves needs to be updated to reflect this.\n\ndef valid_full_moves(move, n, r, mn=0, mx=15):\n    if move.special==\"strike\":\n        # max score is 3 times the max score for the remaining frames\n        # plus the multiplier for the remaining balls in the last frame\n        up = (3*n + 2 + 1)*mx\n    elif move.special==\"spare\":\n        # max score is 2 times the max score for the next frame\n        # 3 times the max score for the remaining frames\n        # plus the multiplier for the remaining balls in the last frame\n        up = (2 + 3*max(n-1,0) + 2 + 1)*mx\n    else:\n        up = (1 + 2*max(n-1,0) + 2 + 1)*mx\n    return n*mn &lt;= r - move.score &lt;= up\n\nAt this point all of the sets of moves for a regular frame contain every score except 1 and 1 less than the max score (i.e. a frame following a spare or single strike cannot score a 29 and a frame following a double-strike cannot score a 44). At this point you may expect that the only impossible scores will be 1 and 449 – this was true with the cases above. However there are two more sets of scoring possibilities just for the last frame.\nIf the last frame starts with a spare, then it is the same as before: single ball, no multiplier.\nIf the last frame starts with a strike, and is not preceded by one, then there are potentially two more balls left with no multipliers attached. And these do leave gaps.\n\n# all the different scores for the last 2 balls of the last frame\n# assuming the first ball in the last frame was a strike\nlast_frame_moves = [ f+s for f in basic_moves \n                         for s in filter(lambda x: f+x&lt;=15,basic_moves) ]\nlast_frame_moves += [ 15 + s for s in basic_moves ]\nlast_frame_moves = set(last_frame_moves)\n\n[ s for s in range(31) if s not in last_frame_moves ]\n\n[1, 16, 29]\n\n\nIf the last frame starts with a strike and is preceded by one, then there are potentially two more balls but the first one is double counted. Again, this leaves gaps.\n\n# all the different scores for the last 2 balls of the last frame\n# assuming the second to last frame was a strike and the first ball\n# in the last frame was a strike\nlast_frame_double_moves = [ 2*f+s for f in basic_moves\n                           for s in filter(lambda x: f+x&lt;=15,basic_moves) ] \nlast_frame_double_moves += [ 30 + s for s in basic_moves ]\nlast_frame_double_moves = set(last_frame_double_moves)\n\n[ s for s in range(46) if s not in last_frame_double_moves ]\n\n[1, 29, 31, 44]\n\n\nIt certainly looks now like there will be at least 4 scores that can’t be achieved because there no way to make the last step with the extra balls in the 10th frame, because the 10th frame is scored differently. This additional scoring complexity now makes it unwieldy to put all of that into the main function. I have broken it out into a separate function that just checks the last frame and either returns the last move with the remaining balls or returns an empty list if there is no possible move.\n\ndef last_frame_rule(remaining, last_frame, max_move, mx=45):\n    if remaining == 0:\n        # hit the target, don't need any additional balls\n        return [SingleFrame(0,None,True)]\n    elif last_frame == \"strike\" and max_move == mx and remaining in last_frame_double_moves:\n        # two extra balls following a double strike\n        return [SingleFrame(remaining,None,True)]\n    elif last_frame == \"strike\" and remaining in last_frame_moves:\n        # two extra balls following a single strike\n        return [SingleFrame(remaining,None,True)]\n    elif last_frame == \"spare\" and remaining in basic_moves:\n        # only one extra ball\n        return [SingleFrame(remaining,None,True)]\n    else:\n        # not possible\n        return []\n\nThe main function now has to track whether the last frame was a strike, to trigger the double strike rules, versus single strikes, spares, and regular frames. The logic is the same, though.\n\ndef make_full_moves(cur_frame, cur_score, max_frame, target, \n                     moves=single_frame_moves, last_frame=None):\n    if cur_frame == max_frame:\n        # max_move is used to check if the second-to-last frame was a strike\n        max_move = max( s.score for s in moves )\n        return last_frame_rule(target-cur_score, last_frame, max_move)\n    else:\n        next_frame = cur_frame + 1\n        n = max_frame - next_frame\n        r = target - cur_score\n        for move in filter(lambda x: valid_full_moves(x,n,r), moves):\n            new_score = cur_score + move.score\n            if last_frame == \"strike\" and move.special == \"strike\":\n                next_moves = double_strike_moves\n                next_last_frame = \"strike\"\n            elif move.special == \"strike\":\n                next_moves = single_strike_moves\n                next_last_frame = \"strike\"\n            elif move.special == \"spare\":\n                next_moves = spare_frame_moves\n                next_last_frame = \"spare\"\n            else:\n                next_moves = single_frame_moves\n                next_last_frame = None\n\n            advance = make_full_moves(next_frame, new_score, max_frame, target, next_moves, next_last_frame)\n            if len(advance) &gt; 0:\n                advance.append(move)\n                return advance\n        else:\n            return []\n\nLooping through all the scores: \\(0 \\le score \\le 450\\) yields the impossible to bowl scores, when spares and strikes are allowed.\n\nfor score in range(451):\n    game = make_full_moves(0,0,10,score)\n    if len(game)==0:\n        print(\"Score {} is not possible, full game\".format(score))\n\nScore 1 is not possible, full game\nScore 434 is not possible, full game\nScore 436 is not possible, full game\nScore 449 is not possible, full game\n\n\nThis conforms with our intuition, after looking at the possible last-frame moves. Of course this is entirely academic as, the way I bowl, I am in no danger of coming close to these impossible scores."
  },
  {
    "objectID": "posts/vessel_blowdown_ideal_gases/index.html",
    "href": "posts/vessel_blowdown_ideal_gases/index.html",
    "title": "Vessel Blowdown - Ideal Gases",
    "section": "",
    "text": "A recurring task of mine is to look at some old calculations, done by some previous engineer whose identity is lost to time and organizational flux, and update them to match current reality. Depending on the state of the spreadsheet, and its lack of documentation, this can also mean going down a rabbit hole of research to find where, exactly, a given equation came from and what all the constants in it represent. This post is the result of one of those journeys, trying to track down the source of a model for depressuring a vessel.\nConsider the blowdown of a pressure vessel to a vent stack, where the vessel contains a gas. What we want is the time to fully depressure and the pressure curve (the blowdown curve). As a first approximation we can consider the ideal gas case and examine two limiting behaviours for the vessel: when the walls are perfect insulators (the adiabatic case) and when the walls are perfect conductors of heat (the isothermal case). Furthermore we assume the blowdown is through an isentropic nozzle."
  },
  {
    "objectID": "posts/vessel_blowdown_ideal_gases/index.html#the-adiabatic-case",
    "href": "posts/vessel_blowdown_ideal_gases/index.html#the-adiabatic-case",
    "title": "Vessel Blowdown - Ideal Gases",
    "section": "The Adiabatic Case",
    "text": "The Adiabatic Case\nThe adiabatic case is often a good approximation for small vessels and early in the blowdown, when the rate of energy lost from the vessel through the bulk transport of the gas is much higher than any heat gained from the environment.\nStarting with a mass balance on the vessel:\n\\[\n\\frac{dm}{dt} = - w\n\\]\nwhere m is the mass inside the vessel and w is the mass flow through the valve. Since the volume of the vessel is a constant, V, we can write the mass balance as\n\\[\nV \\frac{d \\rho}{dt} = - w\n\\]\nWe can perform a change of variables from ρ to P\n\\[\nV \\left( \\frac{\\partial \\rho}{\\partial P} \\right)_S \\frac{dP}{dt} = - w\n\\]\nThe partial derivative is taken along an isentropic path as the adiabatic expansion within the vessel is isentropic (not because the valve is isentropic).\nWe can write the mass flow through the nozzle in terms of the theoretical, friction less, mass velocity G, the discharge coefficient \\(c_D\\), and the flow area A.\n\\[\nw = c_D A G\n\\]\ngiving1\n1 Botros, Jungowski, and Weiss, “Models and Methods of Simulating Gas Pipeline Blowdown”.\\[\n\\frac{dP}{dt} = - \\frac{c_D A}{V} \\left( \\frac{\\partial P}{\\partial \\rho} \\right)_S G\n\\]\n\nFully Choked Flow\nAssuming the flow through the valve is choked, the velocity in the throat is the sonic velocity \\(a_t\\) which, for an ideal gas, is given by\n\\[\na = \\sqrt{ {k R T} \\over M} = \\sqrt{ {k P} \\over \\rho}\n\\]\nAn ideal gas undergoing an adiabatic expansion from vessel pressure to the pressure in the throat of the valve has the following relationship between density and pressure2\n2 Any physical chemistry textbook, such as Laidler, Meiser, and Sanctuary, Physical Chemistry, 79–81.\\[\n\\frac{\\rho_t}{\\rho} = \\left( \\frac{P_t}{P} \\right)^{\\frac{1}{k} }\n\\]\nand, for choked flow, the pressure ratio is at maximum at3\n3 Tilton, “Fluid and Particle Dynamics,” 6-22-6-23.\\[\n\\frac{P_t}{P} = \\left( { 2 \\over {k+1} } \\right)^{\\frac{k}{k-1} }\n\\]\nputting this all together we can write G in terms of vessel conditions \\(\\rho\\) and \\(P\\)\n\\[\nG = \\rho_t u_t = \\rho_t a_t = \\rho_t \\sqrt{ {k P_t} \\over \\rho_t} = \\sqrt{k \\rho_t P_t}\n\\]\n\\[\nG = \\sqrt{k \\rho P} \\left( 2 \\over {k+1} \\right)^{\\frac{k+1}{2 \\left( k - 1 \\right)} }\n\\]\nFrom thermodynamics we know\n\\[\n\\left( \\frac{\\partial P}{\\partial \\rho} \\right)_S = a^2 = \\frac{k P}{\\rho}\n\\]\nand we can put this all together to get\n\\[\n\\frac{dP}{dt} = - \\frac{c_D A}{V} \\left( {k P} \\over \\rho \\right) \\sqrt{k \\rho P} \\left( 2 \\over {k+1} \\right)^{\\frac{k+1}{2 \\left( k - 1 \\right)} }\n\\]\nAt this point, it is standard to introduce a time constant \\(\\tau\\)\n\\[\n\\tau = \\frac{m_0}{w_0} = \\frac{\\rho_0 V}{c_D A \\sqrt{k \\rho_0 P_0} } \\left( 2 \\over {k+1} \\right)^{-\\frac{k+1}{2 \\left( k - 1 \\right)} }\n\\]\nor, more clearly,\n\\[\n\\frac{1}{\\tau} = \\frac{c_D A}{V} \\sqrt{ {k P_0} \\over \\rho_0 } \\left( 2 \\over {k+1} \\right)^{\\frac{k+1}{2 \\left( k - 1 \\right)} }\n\\]\nWhere the subscript 0 indicates the initial conditions in the vessel. This simplifies the expression to\n\\[\n\\frac{dP}{dt} = -\\frac{k}{\\tau} P \\left( \\frac{P}{P_0} \\right)^{\\frac{k-1}{2k} }\n\\]\nWhich is separable and can be integrated to give (after some rearrangement)\n\\[\n\\frac{P}{P_0} = \\left( 1 + \\left( {k-1} \\over 2 \\right) \\frac{t}{\\tau} \\right)^{\\frac{2k}{1-k} }\n\\]\nand the depressure time is\n\\[\nt = \\frac{2\\tau}{1-k} \\left( 1 - \\left( \\frac{P_a}{P_0} \\right)^{\\frac{1-k}{2k} } \\right)\n\\]\nAnother useful thing to determine is the mass flow rate over time, which can be recovered rather easily recalling\n\\[\nw = -\\frac{V}{a^2} \\frac{dP}{dt} = -\\frac{\\rho V}{k P} \\frac{dP}{dt}\n\\]\nand\n\\[\n\\frac{dP}{dt} = -\\frac{k}{\\tau} P \\left( \\frac{P}{P_0} \\right)^{\\frac{k-1}{2k} }\n\\]\nwe get\n\\[\nw = \\frac{\\rho V}{\\tau} \\left( \\frac{P}{P_0} \\right)^{\\frac{k-1}{2k} } = \\frac{\\rho_0 V}{\\tau} \\left( \\frac{\\rho}{\\rho_0} \\right) \\left( \\frac{P}{P_0} \\right)^{\\frac{k-1}{2k} }\n\\]\nBy recalling the definition of \\(\\tau\\) this simplifies to\n\\[\n\\frac{w}{w_0} = \\left( \\frac{P}{P_0} \\right)^{ {k+1} \\over {2k} } = \\left( 1 + \\left({k-1} \\over 2 \\right) \\frac{t}{\\tau} \\right)^{\\frac{1+k}{1-k} }\n\\]\nThis final model, for mass flow, is the model most often given in process safety references for blowdown rates. This makes some sense as early in a blowdown the observed pressure curve tend to approximate the adiabatic curve. However (foreshadowing) the isothermal curve leads to higher predicted vessel pressures, and generally higher mass flow rates, which might be more conservative depending on the context.\n\n\nIn the Literature\nThe adiabatic model is the only simple model given in Lees,4 with the recommendation to use software such as BLOWDOWN to handle more complex, multi phase, mixtures and heat transfer problems. This is also what my older copy of Perry’s gives,5 albeit with a typo.\n4 Lees, Loss Prevention in the Process Industries, 15/44.5 Crowl et al., “Process Safety.” 23–57.\n\n\n\n\n\nNote\n\n\n\nPerry’s gives the following\n\\[\n\\frac{w}{w_0} = \\left( 1 + \\left(\\mathbf{k + 1} \\over 2 \\right) \\frac{t}{\\tau} \\right)^{\\frac{1+k}{1-k} }\n\\]\nNote the sign change, it should be k-1 not k+1, given typical values of k~1.4 this actually a huge difference.\n\n\nPerry’s only gives the mass flow, so if you wanted the pressure (and the gas density and temperature) you would need to find some other reference. Or do it yourself, it does sketch out how the equation is derived, if you have the spare time to sit down and integrate.\n\n\nThe Complete ODE\nThere are two obvious limitations to this model: it relies on the gas being well approximated by an ideal gas and that the flow out of the vessel is always choked. The first issue I am not going to deal with right now, the second one I think can be easily dealt with by slightly modifying the governing equations.\n\\[\n\\frac{dP}{dt} = -\\frac{c_D A}{V} a^2 G\n\\]\nWe can solve this numerically given\n\\[\n\\rho = \\rho_0 \\left(\\frac{P}{P_0}\\right)^{\\frac{1}{k} }\n\\]\n\\[\nG = \\sqrt{ \\rho P \\left( {2k} \\over {k - 1} \\right) \\left( \\left( \\frac{P_t}{P}\\right)^{ \\frac{2}{k} } - \\left( \\frac{P_t}{P} \\right)^{ \\frac{k+1}{k} } \\right) }\n\\]\nfunction isentropic_mass_flow(P, ρ; k=1.4, Pₐ=101325)\n    η = max( Pₐ/P, (2/(k+1))^(k/(k-1)))\n    G² = ρ*P*(2k/(k-1))*( η^(2/k) - η^((k+1)/k) )\n    G = G² &gt; 0 ? √(G²) : 0\n    return G\nend\nfunction speed_of_sound(P, ρ; k=1.4)\n    a = √(k*P/ρ)\n    return a\nend\nfunction adiabatic_vessel(P, params, t)\n    c, A, V, k, ρ₀, P₀, Pₐ = params\n    ρ = ρ₀*(P/P₀)^(1/k)\n    a² = speed_of_sound(P, ρ; k=k)^2\n    G = isentropic_mass_flow(P, ρ; k=k, Pₐ=Pₐ)\n    return-c*A*a²*G/V\nend\nwith a callback function to terminate the integration once the vessel is fully depressured\nfunction depressured_callback(P, t, integrator; tol=0.001)\n    c, A, V, k, ρ₀, P₀, Pₐ = integrator.p\n    return P - (1+tol)*Pₐ\nend\n\n\nA Motivating Example\nJust to have a real system to think about, I used to SCUBA dive when I was a teenager and had a few mishaps early on, when I was still figuring things out, accidentally opening the tank valve when the regulator yoke was not fully attached. Blasting air all over the place while I scrambled to shut it off. Typical tanks have capacities ranging from 80 cu. ft. to 100 cu. ft., with working pressures of &gt;3000 psi. That’s a pretty high pressure for a relatively small tank. How fast could the tank blowdown if I opened the valve fully and just sat back and watched?\n# Ambient conditions\nbegin\n    Pₐ = 101.325e3 # Pa\n    Tₐ = 288.15    # K\n    ρₐ = 1.21      # kg/m³\nend;\nI looked around online and a typical tank with a 80 cu. ft. capacity might have a “water volume” (actual internal volume) of 678 cu. in. (11.11L) and a working pressure of 3000 psi (20.68 MPa). I don’t actually know the flow area of a tank valve, I couldn’t find it easily, so I’m going to guess it’s basically a 1 mm diameter tube when fully open, with a discharge coefficient of 0.85 – all of this could be firmed up better with some real details of the valve. But this is a start.\n#Vessel conditions\nbegin\n    c = 0.85\n    D = 0.001      # m\n    A = 0.25*π*D^2 # m²\n    V = 0.01111    # m³\n    P₀ = 20.68e6   # Pa\n    T₀ = Tₐ\n    ρ₀ = ρₐ*(P₀/Pₐ) # ideal gas law\n    k = 1.4\nend;\nI then set up the differential equation and integrate to get the blowdown curve.\nusing OrdinaryDiffEq, Plots\nbegin\n    params = (c, A, V, k, ρ₀, P₀, Pₐ)\n    t_span = (0.0, 12.0)\n    prob = ODEProblem(adiabatic_vessel, P₀, t_span, params)\n    sol = solve(prob, Tsit5(),\n                callback=ContinuousCallback(depressured_callback, terminate!))\nend;\n\n\n\n\n\n\nFigure 2: The adiabatic blowdown curve for a fully charged SCUBA tank, showing both the fully choked model and the ODE solution.\n\n\n\nThis model has the tank blowing down pretty fast, in less than 30s. Probably my guess for the valve area is too large. I did just make it up.\nRegarding the models themselves, the adiabatic choked model is a very good approximation to the full ODE until the last few fractions of a second, at which point the models diverge. This likely to be true for any high pressure blowdowns, where the vessel pressure starts well above ~2atm, as in that case the majority of the blowdown will be entirely in the choked flow regime.\nTo play around with this more, I am first going to detour into creating some helper functions and I think this is a natural point to create a struct to contain the vessel parameters.\nbegin\n\nstruct PressureVessel{F &lt;: Number}\n    c::F\n    A::F\n    V::F\n    k::F\n    ρ₀::F\n    P₀::F\n    Pₐ::F\n    τ::F\nend\n\nPressureVessel(c, A, V, k, ρ₀, P₀, Pₐ, τ) = \n    PressureVessel(promote(c, A, V, k, ρ₀, P₀, Pₐ, τ)...)\n\nfunction PressureVessel(c, A, V, k, ρ₀, P₀, Pₐ)\n    τ = 1/( (c*A/V)*√(k*P₀/ρ₀)*(2/(k+1))^((k+1)/(2*(k-1))) )\n    return PressureVessel(c, A, V, k, ρ₀, P₀, Pₐ, τ)\nend\n\nend;\nWhere I have added a helper function to ensure all numbers are of the same type, and calculate the value of τ when the PressureVessel type is constructed.\nRecreating the results from above, I start with a definition of the vessel\nvessel = PressureVessel(c, A, V, k, ρ₀, P₀, Pₐ);\nI would like to create some generic functions for the blowdown properties I am interested in: pressure and mass flow rate as functions of time and total blowdown time. To accommodate this I define another type to contain the VesselBlowdown solution.\nabstract type Blowdown end\nstruct AdiabaticBlowdown{S} &lt;: Blowdown\n    pv::PressureVessel\n    sol::S\nend\nHere I add some functions to make a Blowdown object act like an iterator with only a single element. This is absolutely pointless except that I just happen to like being able to generate a vector of results by using the “dot” notation, like so\nmy_function.(blowdown, time_vector)\nwhere I want it to broadcast over the time_vector.\nBase.length(::Blowdown) = 1\nBase.iterate(b::Blowdown, state=1) = state &gt; length(b) ? nothing : (b,state+1)\nFor the simple choked model this is fairly straight forward.\nadiabatic_blowdown_choked(vessel::PressureVessel) = \n    AdiabaticBlowdown(vessel,nothing)\nfunction blowdown_pressure(bd::AdiabaticBlowdown, t)\n    P₀, k, τ = bd.pv.P₀, bd.pv.k, bd.pv.τ\n    return P₀*( 1 + 0.5*(k-1)*(t/τ))^((2*k)/(1-k))\nend\nfunction blowdown_mass_rate(bd::AdiabaticBlowdown, t)\n    ρ₀, V, P₀, k, τ = bd.pv.ρ₀, bd.pv.V, bd.pv.P₀, bd.pv.k, \n                      bd.pv.τ\n    m₀ = ρ₀*V\n    w₀ = m₀/τ\n    P = blowdown_pressure(bd, t)\n    return w₀*(P/P₀)^((k+1)/(2k))\nend\nfunction blowdown_time(bd::AdiabaticBlowdown)\n    P₀, Pₐ, k, τ = bd.pv.P₀, bd.pv.Pₐ, bd.pv.k, bd.pv.τ\n    return (2τ/(1-k))*(1 - (Pₐ/P₀)^((1-k)/2k))\nend\nFor the full model the initial step is to integrate the differential equation. As a first guess, I calculate the blowdown time for a fully choked blowdown and set the outer-bound for the integration to 10× this. The integrator will terminate when the pressure reaches ambient and thus the last time stored will be the actual blowdown time.\nfunction adiabatic_blowdown_full(vessel::PressureVessel; solver=Tsit5())\n    # unpack the parameters\n    c, A, V, k, ρ₀, P₀, Pₐ = vessel.c, vessel.A, vessel.V, \n                             vessel.k, vessel.ρ₀, vessel.P₀,\n                             vessel.Pₐ\n    params = (c, A, V, k, ρ₀, P₀, Pₐ)\n\n    # estimate the time span needed to fully blowdown\n    τ = vessel.τ\n    t_bd = (2τ/(1-k))*(1 - (Pₐ/P₀)^((1-k)/2k))\n    t_span = (0.0, 10t_bd)\n\n    # set up the ODEProblem and solve\n    prob = ODEProblem(adiabatic_vessel, P₀, t_span, params)\n    sol = solve(prob, solver,\n                callback=ContinuousCallback(depressured_callback, terminate!))\n\n    return AdiabaticBlowdown(vessel,sol)\nend\nfunction blowdown_pressure(bd::AdiabaticBlowdown{&lt;:ODESolution}, t)\n    if t &lt; blowdown_time(bd)\n        return bd.sol(t)\n    else\n        return bd.sol.u[end]\n    end\nend\nfunction blowdown_mass_rate(bd::AdiabaticBlowdown{&lt;:ODESolution}, t)\n    if t &lt; blowdown_time(bd)\n        # unpack the parameters\n        c, A, k, ρ₀, P₀, Pₐ = bd.pv.c, bd.pv.A, bd.pv.k,  \n                              bd.pv.ρ₀, bd.pv.P₀, bd.pv.Pₐ\n        \n        # calculate w = c*A*G\n        P = blowdown_pressure(bd, t)\n        ρ = ρ₀*(P/P₀)^(1/k)\n        G = isentropic_mass_flow(P, ρ; k=k, Pₐ=Pₐ)\n    \n        return c*A*G\n    else\n        return 0.0\n    end\nend\nblowdown_time(bd::AdiabaticBlowdown{&lt;:ODESolution}) = \n    bd.sol.t[end]\n\n\n\n\n\n\nFigure 3: The adiabatic blowdown curves for a fully charged SCUBA tank, showing both the fully choked model and the ODE solution for pressure (top) and mass flow rate (bottom).\n\n\n\nAt this point I’ve built up enough machinery that playing around with all sorts of variations to the original case become quite simple. As an example, I look at the same air tank but pressured to 1.5 atm instead.\ntest_vessel = PressureVessel(c, A, V, k, ρ₀, 1.5Pₐ, Pₐ);\n\n\n\n\n\n\nFigure 4: The adiabatic blowdown curve for a partially charged SCUBA tank, showing both the fully choked model and the ODE solution.\n\n\n\nNow it is clear that the fully choked model model isn’t working well, it predicts a blowdown time of 11.68s whereas numerically solving the ODE gives an answer of 20.49s, a 75.0% greater predicted blowdown.\nThat said…I’m being a little coy about something: the full ODE predicts that the vessel will never blowdown. The pressure will get closer and closer to ambient but never get there. This is because G, for non-choked flow, asymptotically approaches zero as the vessel pressure approaches ambient pressure. How you define blowdown time is really a function of how close to ambient is close enough. Even if I set the tolerance in the depressured_callback function, which terminates the integration once the integrator is within tolerance of the ambient pressure, to zero it would, in reality, simply terminate at the default numerical precision of DifferentialEquations.jl. In this case I’ve said “within 0.1% of ambient is close enough,” but that’s totally arbitrary."
  },
  {
    "objectID": "posts/vessel_blowdown_ideal_gases/index.html#the-isothermal-case",
    "href": "posts/vessel_blowdown_ideal_gases/index.html#the-isothermal-case",
    "title": "Vessel Blowdown - Ideal Gases",
    "section": "The Isothermal Case",
    "text": "The Isothermal Case\nThe other limiting case worth exploring is the isothermal case, which is equivalent to the vessel having perfectly conductive walls and remaining always at thermal equilibrium with the environment. This is often a good approximation for large vessels where the blowdown rate is small relative to the thermal mass of the gas in the vessel.\nRecalling, for the adiabatic case, we had the following\n\\[\n\\frac{dP}{dt} = - \\frac{c_D A}{V} \\left( \\frac{\\partial P}{\\partial \\rho} \\right)_S G\n\\]\nFor the isothermal case the vessel is being depressured along an isothermal path (not an isentropic path) and so we substitute the appropriate partial derivative6\n6 Botros, Jungowski, and Weiss, “Models and Methods of Simulating Gas Pipeline Blowdown”.\\[\n\\frac{dP}{dt} = - \\frac{c_D A}{V} \\left( \\frac{\\partial P}{\\partial \\rho} \\right)_T G\n\\]\n\nFully Choked Flow\nAs before, the blowdown is through an isentropic nozzle and we assume that flow is choked\n\\[\nG = \\sqrt{k \\rho P} \\left( \\frac{2}{k+1} \\right)^{\\frac{k+1}{2 \\left( k-1 \\right)} } = \\rho \\sqrt{ {k P} \\over \\rho} \\left( \\frac{2}{k+1} \\right)^{\\frac{k+1}{2 \\left( k-1 \\right)} }\n\\]\nFrom thermodynamics we can write the partial derivative as\n\\[\n\\left( \\frac{\\partial P}{\\partial \\rho} \\right)_T = \\frac{a^2}{k} = \\frac{P}{\\rho}\n\\]\nThus\n\\[\n\\frac{dP}{dt} = - \\frac{c_D A}{V} \\frac{P}{\\rho} \\rho \\sqrt{ {k P} \\over \\rho} \\left( \\frac{2}{k+1} \\right)^{\\frac{k+1}{2 \\left( k-1 \\right)} }\n\\]\nwhere the densities, \\(\\rho\\), can be cancelled and, since the vessel is isothermal (i.e. \\(\\frac{P}{\\rho}\\) is a constant), the various constants can be collected to give\n\\[\n\\frac{dP}{dt} = -\\frac{P}{\\tau}\n\\]\nWhere \\(\\tau\\) is as defined for the adiabatic case. This can easily be integrated to give\n\\[\n\\frac{P}{P_0} = \\exp \\left( \\frac{-t}{\\tau} \\right)\n\\]\nit also follows, from the ideal gas law, that\n\\[\n\\frac{\\rho}{\\rho_0} = \\exp \\left( \\frac{-t}{\\tau} \\right)\n\\]\nand\n\\[\n\\frac{w}{w_0} = \\exp \\left( \\frac{-t}{\\tau} \\right)\n\\]\nThis can also be rearranged to give the blowdown time[^ N.B. the \\(\\log \\left( \\dots \\right)\\) is the natural log, this matches the convention used in julia]\n\\[\nt = \\tau \\log \\left( \\frac{P_0}{P_a} \\right)\n\\]\n\n\nIn the Literature\nThis is the equation seen most often in references for estimating blowdown time for pipelines and compressor systems. It is also what is going on under the hood with many online calculators for vessel blowdown times. Though, in my experience, this is not always well documented and a modified form is often presented.\nThe time constant, \\(\\tau\\), can be broken up to look like this\n\\[\n\\tau = \\frac{V}{c_D A} \\sqrt{\\frac{M}{M_{air} Z_0 T_0} } \\sqrt{\\frac{M_{air} }{kR} } \\left( 2 \\over {k+1} \\right)^{\\frac{-1}{2} \\frac{k+1}{k-1} }\n\\]\nWhere we have made the substitution \\(Z_0 R T_0\\) for \\(R T_0\\) to account for non-ideal behaviour. If the gas has a value of k ~ 1.4, we can write\n\\[\n\\tau = \\mathrm{const} \\frac{V}{c_D A} \\sqrt{ \\frac{SG}{Z_0 T_0} }\n\\]\nWhere the constant is calculated entirely from the properties of air. Generally, I have found, few references describe where this constant comes from and in particular that it depends implicitly on a particular value for k. It also often has unit conversions absorbed into it, for example7\n7 Campbell, Gas Conditioning and Processing, 2:29; VANEC, “Pressure Volume-Blowdown Time Calculation”.\\[\nt = 5.5 \\frac{V}{c_D A} \\sqrt{ \\frac{SG}{Z_0 T_0} } \\log \\left( \\frac{P_0}{P_a} \\right)\n\\]\nwith the units\n\nBlowdown time, t, in seconds\nVessel volume, V, in cubic feet\nValve flow area, A, in square inches\nInitial temperature, \\(T_0\\), in Rankine\nInitial pressure, \\(P_0\\), in psia\nAmbient pressure, \\(P_a\\), in psia\n\nI have also found a few sources that leave the value of the constant as a mystery for the user to puzzle out.8 I was honestly surprised at the quality of the results when I first googled this and looked it up in Knovel. The highest ranked results, at the time, were cryptic to the point of uselessness or included obvious mistakes (several referred to t as the “interstitial velocity” with units of cm/s, an obvious misprint being blindly recopied in several places, including some e-books on Knovel where one would hope the quality control would be better). There are a few places with useful derivations9 but I think a good starting point is the Tank Blowdown Math set of notes. It is pretty straight forward and does not require a lot of prior knowledge of the partial derivatives of various thermodynamic state variables.\n8 Temizel et al., Formulas and Calculations for Petroleum Engineering, 262; Engineers Edge, “Blowdown Time in Unsteady Gas Flow Calculator and Equation”.9 Wheeler, “Tank Blowdown Math”; Botros, Jungowski, and Weiss, “Models and Methods of Simulating Gas Pipeline Blowdown”; Botros and Hardeveld, Pipeline Pumping and Compression Systems - a Practical Approach, 447; Saad, Compressible Fluid Flow, 98–103, to list but a few.I personally would not bother with the models that pre-calculate the constant for you. We no longer live in the age of slide-rules. The blowdown time equation for fully choked flow is well within the capabilities of excel or any competent person with a scientific calculator. I think it is easier to justify and explain, will be a better model for gases where k is not 1.4, and allows one to incorporate small levels of non-ideality through the isentropic expansion factor n.\nThe isothermal fully-choked model can be implemented building on the types already created, by first creating an IsothermalBlowdown type and associated blowdown functions\nstruct IsothermalBlowdown{S} &lt;: Blowdown\n    pv::PressureVessel\n    sol::S\nend\nisothermal_blowdown_choked(vessel::PressureVessel) = \n    IsothermalBlowdown(vessel,nothing)\nfunction blowdown_pressure(bd::IsothermalBlowdown, t)\n    P₀, τ = bd.pv.P₀, bd.pv.τ\n    return P₀*exp(-t/τ)\nend\nfunction blowdown_mass_rate(bd::IsothermalBlowdown, t)\n    ρ₀, V, τ = bd.pv.ρ₀, bd.pv.V, bd.pv.τ\n    m₀ = ρ₀*V\n    w₀ = m₀/τ\n    return w₀*exp(-t/τ)\nend\nblowdown_time(bd::IsothermalBlowdown) = \n    bd.pv.τ*log(bd.pv.P₀/bd.pv.Pₐ)\nIn a similar vein as the adiabatic case, the requirement for fully choked flow can be relaxed and the ODE integrated numerically instead, starting with the system\n\\[\n\\frac{dP}{dt} = -\\frac{c_D A}{V} \\frac{a^2}{k} G\n\\]\nWe can solve this numerically given that, for an isothermal system, the density is given by\n\\[\n\\rho = \\rho_0 \\left(\\frac{P}{P_0}\\right)\n\\]\nand using the definition of G given in the adiabatic case.\nfunction isothermal_vessel(P, params, t)\n    c, A, V, k, ρ₀, P₀, Pₐ = params\n    ρ = ρ₀*(P/P₀)\n    a² = speed_of_sound(P, ρ; k=k)^2\n    G = isentropic_mass_flow(P, ρ; k=k, Pₐ=Pₐ)\n    return-c*A*a²*G/(k*V)\nend\nfunction isothermal_blowdown_full(vessel::PressureVessel; solver=Tsit5())\n    # unpack the parameters\n    c, A, V, k, ρ₀, P₀, Pₐ = vessel.c, vessel.A, vessel.V, \n                             vessel.k, vessel.ρ₀, vessel.P₀,\n                             vessel.Pₐ\n    params = (c, A, V, k, ρ₀, P₀, Pₐ)\n\n    # estimate the time span needed to fully blowdown\n    τ = vessel.τ\n    t_bd = τ*log(P₀/Pₐ)\n    t_span = (0.0, 10t_bd)\n\n    # set up the ODEProblem and solve\n    prob = ODEProblem(isothermal_vessel, P₀, t_span, params)\n    sol = solve(prob, solver,\n                callback=ContinuousCallback(depressured_callback, terminate!))\n\n    return IsothermalBlowdown(vessel,sol)\nend\nfunction blowdown_pressure(bd::IsothermalBlowdown{&lt;:ODESolution}, t)\n    if t &lt; blowdown_time(bd)\n        return bd.sol(t)\n    else\n        return bd.sol.u[end]\n    end\nend\nfunction blowdown_mass_rate(bd::IsothermalBlowdown{&lt;:ODESolution}, t)\n    if t &lt; blowdown_time(bd)\n        # unpack the parameters\n        c, A, k, ρ₀, P₀, Pₐ = bd.pv.c, bd.pv.A, bd.pv.k, \n                              bd.pv.ρ₀, bd.pv.P₀, bd.pv.Pₐ\n        \n        # calculate w = c*A*G\n        P = blowdown_pressure(bd, t)\n        ρ = ρ₀*(P/P₀)\n        G = isentropic_mass_flow(P, ρ; k=k, Pₐ=Pₐ)\n    \n        return c*A*G\n    else\n        return 0.0\n    end\nend\nblowdown_time(bd::IsothermalBlowdown{&lt;:ODESolution}) = \n    bd.sol.t[end]\n\n\n\n\n\n\nFigure 5: The isothermal blowdown curves for a fully charged SCUBA tank, showing both the fully choked model and the ODE solution for pressure (top) and mass flow rate (bottom).\n\n\n\nIt is a similar story to the adiabatic case: for systems with a high initial pressure, the flow out of the valve is fully choked for almost the entire blowdown. It is only in the final fraction of a second that the full ODE system deviates from the model that assumes flow is choked all the time.\nIn most practical situations, the difference would likely be swamped by two much greater problems with these models:\n\nthe gases are assumed be ideal with constant k\nthe vessel is perfectly isothermal (or adiabatic)\n\nBoth of these assumptions will have a much greater impact on how well the model fits observed blowdowns than the slight deviation at the end of the blowdown due to non-choked flow."
  },
  {
    "objectID": "posts/vessel_blowdown_ideal_gases/index.html#comparing-blowdown-models",
    "href": "posts/vessel_blowdown_ideal_gases/index.html#comparing-blowdown-models",
    "title": "Vessel Blowdown - Ideal Gases",
    "section": "Comparing Blowdown Models",
    "text": "Comparing Blowdown Models\nI think it might be simpler to visualize when the choked flow blowdown models will fall down by looking at the high pressure blowdown, the original example, versus the low pressure blowdown in dimensionless form. In this form, the choked flow blowdown curves (both adiabatic and isothermal) only depend on k. They are in fact the exact same curve. All that has changed is where along the curve the blowdown terminates.\n\n\n\n\n\n\nFigure 6: The adiabatic and isothermal blowdown curves for a fully charged SCUBA tank, in dimensionless form.\n\n\n\nIn the high pressure case the blowdown terminates much closer to \\(\\frac{P}{P_0}=0\\) and most of the curve is fully choked.\n\n\n\n\n\n\nFigure 7: The adiabatic and isothermal blowdown curves for a partially charged SCUBA tank, in dimensionless form.\n\n\n\nIn the low pressure case the blowdown terminates at a much steeper part of the blowdown curve and the departure for non-choking flow is much more apparent.\nIt is not immediately clear to me why the adiabatic case is all over the standard references for process safety, and the isothermal model is not. If what you care about is the pressure sustained within a vessel, the mass flow rate emitted through a blowdown stack or vent, and the duration of the blowdown, it is almost always more conservative to use the isothermal case. The isothermal (fully choked) model is also just easier to calculate, being just \\(\\exp \\left( \\frac{-t}{\\tau} \\right)\\).\nThe adiabatic case will give a better sense of how temperature changes within the vessel. I’ve largely left it out, but adiabatic blowdown does lead to a significant temperature drop and this cryogenic cooling can be a process hazard on its own. The gas exiting, and the vessel walls themselves, will get quite cold. Anyone who has gone camping in more marginal weather and watched a one-pound propane cylinder develop frost on the outside while cooking has seen this effect in action.[^This is also why butane cylinders are often not a good idea for early spring camping (in Canada), the cooling effect is strong enough to cause the butane inside to liquefy and the stove won’t work very well.] But actually calculating the vessel temperature is almost entirely ignored in blowdown calculations for ideal gases, in my experience.\nThe isothermal model, in my review of the literature, appeared to be more commonly used in operational contexts, such as estimating the time required to blowdown a system through a blowdown vent. In this case it is likely to be the conservative answer. The two curves do cross at high \\(\\frac{t}{\\tau}\\) and so it is not always the case that the isothermal model is more conservative. Something worth noting."
  },
  {
    "objectID": "posts/vessel_blowdown_ideal_gases/index.html#final-thoughts",
    "href": "posts/vessel_blowdown_ideal_gases/index.html#final-thoughts",
    "title": "Vessel Blowdown - Ideal Gases",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI deliberately set up the ODEs such that there is a clear path to implementing a real gas model through an equation of state. All that really needs to be done is to create functions for these three steps:\n\nthe speed of sound\nthe density as a function of pressure, either along an isentropic path (in the adiabatic case) or along an isothermal path\nthe isentropic mass velocity, G\n\nPlugging those into the relevant steps in the adiabatic_vessel and isothermal_vessel functions changes from the ideal gas case to the real gas case. The rest of the code remains the same and operates unchanged.\nIn this case I think solving the full ODE for the ideal gas case alone is probably not worth the effort for most cases. The error in assuming an ideal gas, or in assuming one of the limiting heat transfer cases, is probably far larger than the error in assuming fully choked flow for all but the few cases that are near atmospheric pressure. If you are going to be estimating the blowdown for a real gas, then that’s different. If you are going to the hassle of setting up and solving the ODE, might as well have as few unnecessary assumptions as you can get away with. It really isn’t any more person effort, at that point, just more computer effort, and when the calculations happen in less than a second, how much less than a second is of little practical importance."
  },
  {
    "objectID": "posts/vessel_blowdown_ideal_gases/index.html#references",
    "href": "posts/vessel_blowdown_ideal_gases/index.html#references",
    "title": "Vessel Blowdown - Ideal Gases",
    "section": "References",
    "text": "References\n\n\nBotros, K. K., W. M. Jungowski, and M. H. Weiss. “Models and Methods of Simulating Gas Pipeline Blowdown.” The Canadian Journal of Chemical Engineering 67 (1989): 529–39. https://doi.org/10.1002/cjce.5450670402.\n\n\nBotros, Kamal K., and Thomas Van Hardeveld. Pipeline Pumping and Compression Systems - a Practical Approach. 3rd ed. New York: ASME Press, 2018.\n\n\nCampbell, John M. Gas Conditioning and Processing. Vol. 2. Tulsa, OK: John M. Campbell & Co, 1992.\n\n\nCrowl, Daniel A., Lawrence G. Britton, Walter L. Frank, Stanley Grossel, Dennis Hendershot, W. G. High, Robert W. Johnson, et al. “Process Safety.” In Perry’s Chemical Engineers’ Handbook, edited by Don W. Green, 8th ed. New York: McGraw Hill, 2008.\n\n\nEngineers Edge. “Blowdown Time in Unsteady Gas Flow Calculator and Equation,” 2025. https://www.engineersedge.com/calculators/blowdown_time_in_unsteady_gas_16011.htm.\n\n\nLaidler, Keith J., John H. Meiser, and Bryan C. Sanctuary. Physical Chemistry. 4th ed. Boston, MA: Houghton Mifflin Co, 2003.\n\n\nLees, Frank P. Loss Prevention in the Process Industries. 2nd ed. Oxford: Butterworth-Heinemann, 1996.\n\n\nSaad, Michel A. Compressible Fluid Flow. Englewood Cliffs, NJ: Prentiss-Hall, 1985.\n\n\nTemizel, Cenk, Tayfun Tuna, Mehmet Melik Oskay, and Luigi Saputelli. Formulas and Calculations for Petroleum Engineering. Cambridge, MA: Gulf Professional Publishing, 2019.\n\n\nTilton, James N. “Fluid and Particle Dynamics.” In Perry’s Chemical Engineers’ Handbook, edited by Don W. Green, 8th ed. New York: McGraw Hill, 2008.\n\n\nVANEC. “Pressure Volume-Blowdown Time Calculation,” 2025. https://www.vanec.com/pressurized-volume-blowdown-time-calculation.html.\n\n\nWheeler, Dean R. “Tank Blowdown Math,” 2019. http://www.et.byu.edu/~wheeler/Tank_Blowdown_Math.pdf."
  },
  {
    "objectID": "posts/hydrogen_blending/index.html",
    "href": "posts/hydrogen_blending/index.html",
    "title": "Hydrogen Blending",
    "section": "",
    "text": "November this year came in with a bang where I live: the temperature outside is currently -20°C, there is a pile of snow, and suddenly staying in and staying warm is a very important activity. At the same time, with COP27 taking place in Egypt, climate change is top of mind for everyone (Edmonton is evaluating its net zero strategy) and I thought it would be worthwhile to look at one of the largest sources of household energy use in Canada: space heating. Space heating accounts for 61.6% of household energy use, and in the province of Alberta that predominantly comes from burning natural gas. If we are going to meet our net zero goals as a municipality, we will need to address the carbon emissions that come from simply living here.\nA commonly bandied about tool for reducing household carbon emissions is hydrogen blending, such as this project in the Edmonton area. The idea is to gradually increase the hydrogen content of the natural gas and, since hydrogen burns without producing any carbon dioxide, the carbon emissions will decline. This has the obvious advantage of using the existing gas distribution infrastructure and existing appliances (e.g. people’s furnaces), thus avoiding costly retrofits.\nBut is this actually a useful thing to do? A common criticism of hydrogen is its low energy density: for a given flowrate you would expect to get ~1/3 the energy from pure hydrogen than from natural gas (assuming you are combusting it). But hydrogen is also incredibly light and, since flowrate is \\(\\propto \\frac{1}{\\sqrt{\\rho} }\\), for the same pressure drop across a pipe you would expect a greater flowrate. So which is it? Does the energy content decrease, increase, or stay the same? Instead of just waving my hands and guessing, I thought it might be worthwhile to work through some simple calculations to get a sense of the scale of things.\nThat said, it is rarely the case in engineering that decisions are clear cut. Blending hydrogen will have advantages and disadvantages, and whether the one out-weighs the other depends greatly on the particular location, its needs, infrastructure, and a host of other factors."
  },
  {
    "objectID": "posts/hydrogen_blending/index.html#energy-density",
    "href": "posts/hydrogen_blending/index.html#energy-density",
    "title": "Hydrogen Blending",
    "section": "Energy density",
    "text": "Energy density\nThe obvious place to start, and where I have found people often end, is with the heating value. For convenience I am going to use higher heating values, given here per standard cubic meter. Where standard in this case means at 15°C and 1atm.\n\nusing Unitful\n\n# Standard State\nR  = 8.31446261815324u\"m^3*Pa/K/mol\"\nTᵣ = 288.15u\"K\"\nPᵣ = 101325u\"Pa\"\n\n# Hydrogen, from the GPSA handbook, 13th ed.\nHHV_H2 = 12.109u\"MJ/m^3\"\n\n# Natural Gas, typical\nHHV_NG = 35.396u\"MJ/m^3\"\n\nWe can use a simple mixing rule to determine what the heating value would be with x% hydrogen by volume.\n\nHHV(x) = x*HHV_H2 + (1-x)*HHV_NG\n\n\n\n\n\n\n\n\n\nFigure 1: Higher heating value of blended natural gas/hydrogen fuel gas as a function of hydrogen content, assuming an ideal gas and simple mixing rule.\n\n\n\n\n\nWhich clearly shows that increasing the hydrogen content decreases the overall heating value of the fuel. At 100% hydrogen the fuel gas has lost ~66% of it’s heat content.\nSuppose you are a customer whose natural gas was transitioned entirely to hydrogen, now you are receiving about a third of the energy per unit volume than you were before. Well the obvious thing to do would be to increase the volume that you use (by a factor of three) to make up the difference. This does raise the obvious question of can you actually do that? Superficially, it looks like you are asking to triple the demand on the current infrastructure."
  },
  {
    "objectID": "posts/hydrogen_blending/index.html#energy-per-unit-of-pressure-drop",
    "href": "posts/hydrogen_blending/index.html#energy-per-unit-of-pressure-drop",
    "title": "Hydrogen Blending",
    "section": "Energy per unit of pressure drop",
    "text": "Energy per unit of pressure drop\nThe glaring omission with the previous analysis is that we know that flowrate, generally, is \\(\\propto \\frac{1}{\\sqrt{\\rho} }\\), and natural gas is something like 9× denser than hydrogen. So, you would expect, for the same pressure drop in the same pipes, that you would get around 3× the flow of hydrogen.\nSuppose we are only looking at the “last mile” of the distribution network, perhaps the pipe connecting your house to the main, reducing the problem to that of simple pipe flow. Assuming the flow is nearly isothermal, and an ideal gas, the mass velocity, G, of the fuel gas arriving at your house is given by:\n\\[ G = \\sqrt{ \\rho_1 P_1 } \\sqrt{ \\left(1 - \\left( P_2 \\over P_1 \\right)^2 \\right) \\over { K - 2\\log \\left( P_2 \\over P_1 \\right)} } \\]\nWhere 1 is the point just after the tee and 2 is the point just before your meter (e.g. a straight length of pipe). The volumetric flow rate, Q, at the upstream point 1, is then given by\n\\[ Q_1 = {\\pi \\over 4} D^2 {G \\over \\rho_1} \\]\nwhich, when corrected to the reference (standard) state is\n\\[ Q_s = Q_1 \\cdot {v_{r} \\over v_1} \\]\nWhere v is the ideal gas molar volume (RT/P), a constant independent of the gas.\nThe heat rate, q, is simply the higher heating value times the volumetric flowrate (at standard state)\n\\[ q = HHV \\cdot Q_s = HHV \\cdot {\\pi \\over 4} D^2 \\sqrt{P_1 \\over \\rho_1} \\sqrt{ \\left(1 - \\left( P_2 \\over P_1 \\right)^2 \\right) \\over { K - 2\\log \\left( P_2 \\over P_1 \\right)} } \\cdot {v_{r} \\over v_1} \\]\nThis looks like a lot however most of that is a constant, i.e. it is a function of the system and not the gas moving through it.\n\\[ q = { HHV \\over \\sqrt{\\rho_1} } \\times \\textrm{a constant} \\]\nSo, assuming a constant pressure drop, along an identical pipe, with fully developed turbulent flow (i.e. K is constant) the ratio of heat delivered by hydrogen to that of natural gas is given by:\n\\[ { q_{H_2} \\over q_{NG} } = { HHV_{H_2} \\over HHV_{NG} } \\sqrt{ \\rho_{NG} \\over \\rho_{H_2} } = { HHV_{H_2} \\over HHV_{NG} } \\sqrt{ MW_{NG} \\over MW_{H_2} }\\]\n\n# Hydrogen\nMW_H2 = 2.016e-3u\"kg/mol\"\n\n# Natural Gas\nMW_NG = 19.5e-3u\"kg/mol\"\n\nheat_ratio = (HHV_H2/HHV_NG)*√(MW_NG/MW_H2)\n\n1.0639620426132184\n\n\nSo in total opposition to what we expected from merely looking at energy density we now expect, for the same system operating at the same pressures, to receive slightly more energy when transitioned over to pure hydrogen.\nBut what about the in-between, when hydrogen is blended into natural gas? Is it just a straight line connecting these two?\nWe can explore this more closely by first looking at how density, and thus volumetric flowrate, changes with the hydrogen content. I am assuming an ideal gas case and so the mixing rule is quite simple\n\\[ \\rho = x_{H_2} \\rho_{H_2} + x_{NG} \\rho_{NG} \\]\nwhere, for an ideal gas\n\\[ \\rho = MW {P \\over {R T} } \\]\ngiving\n\\[ \\rho = {P \\over {R T} } \\left( x_{H_2} MW_{H_2} + x_{NG} MW_{NG} \\right)\\]\n\n# ideal gas density\nρ(x, T, P) = (P/(R*T))*( x*MW_H2 + (1-x)*MW_NG );\n\n\n\n\n\n\n\n\n\nFigure 2: Density and relative flowrate for blended natural gas/hydrogen fuel gas, assuming an ideal gas.\n\n\n\n\n\nSo we have two competing effects: as the mole fraction increases the heating value of the gas decreases but at the same time the flowrate increases. We can explore this further by plotting the ratio of the heat rate with blended fuel gas to the heat rate with straight natural gas.\n\n# heat rate for blended fuel gas relative to natural gas\nq_ratio(x) = (HHV(x)/HHV_NG)*√(ρ(0,Tᵣ,Pᵣ)/ρ(x,Tᵣ,Pᵣ));\n\n\n\n\n\n\n\n\n\nFigure 3: The amount of energy delivered, in higher heating value, for a blended natural gas/hydrogen fuel gas system at constant operating conditions, relative to natural gas\n\n\n\n\n\nInitially the loss of heating value “wins out” and increasing the hydrogen content merely decreases the energy supplied at a given pressure. But once the stream is predominantly hydrogen, the lower density takes over and the heat rate increases.\nThe minimum ratio can be found by setting the derivative to zero\n\nusing ForwardDiff: derivative\nusing Roots: find_zero\n\n∂q_ratio(x) = derivative(q_ratio,x)\nxₘᵢₙ = find_zero(∂q_ratio,(0,1))\n\nxₘᵢₙ, q_ratio(xₘᵢₙ)\n\n(0.7106211503798253, 0.8839840357969662)\n\n\nInitially, blending hydrogen decreases the overall energy delivered, bottoming out at ~12% less, when hydrogen makes up 71% of the fuel gas. While this is not the 66% decline predicted by a naive look at energy density, neither is it nothing.\nAnother important point is where the ratio becomes one: the concentration where the blended hydrogen fuel gas reattains the energy content of the original natural gas stream\n\nxₑᵥₑₙ = find_zero( (x)-&gt; q_ratio(x)-1, (xₘᵢₙ,1))\n\n0.9684672945947692\n\n\nThe system doesn’t recover the original energy supply until the hydrogen content is &gt;96.8%, at which point a whole host of other concerns may become more relevant – burning pure and nearly pure hydrogen comes with its own issues."
  },
  {
    "objectID": "posts/hydrogen_blending/index.html#greenhouse-gas-emissions",
    "href": "posts/hydrogen_blending/index.html#greenhouse-gas-emissions",
    "title": "Hydrogen Blending",
    "section": "Greenhouse gas emissions",
    "text": "Greenhouse gas emissions\nThe whole point of doing this is to decrease the carbon emissions associated with space heating (plus the other uses of household natural gas, but mostly space heating). So it is worth circling back to answer the question: does this actually do that? and by how much?\nThe dominant greenhouse gas associated with combustion is carbon dioxide, and the carbon dioxide emissions from combustion are fairly easy to calculate from stoichiometry, for a generic hydrocarbon the combustion equation is\n\\[ C_n H_m + \\left( n + {m \\over 4} \\right) O_2 \\rightarrow n CO_2 + {m \\over 2} H_2 O \\]\nIf we presume the natural gas is mostly methane and n≈1, then there is one mole of carbon dioxide produced per mole of natural gas delivered (assuming perfectly complete combustion). When combusting hydrogen there is no carbon dioxide produced, and so the moles of carbon dioxide produced from the combustion of a blended hydrogen fuel gas is\n\\[ \\dot{n}_{CO_2} = \\left( 1 - x_{H_2} \\right) \\dot{n}_{FG} \\]\nWhere \\(\\dot{n}\\) is the molar flowrate. We don’t actually know the molar flowrate of fuel gas, but we can calculate it from the ideal gas law and the volumetric flowrate at standard state Qs\n\\[ \\dot{n}_{FG} = {P_r \\over {R T_r} } Q_s \\]\n\\[ \\dot{n}_{CO_2} = \\left( 1 - x_{H_2} \\right) {P_r \\over {R T_r} } Q_s \\]\nWhat we want is the mass flowrate of carbon dioxide, so simply multiply both sides by the molar weight\n\\[ \\dot{m}_{CO_2} = \\left( 1 - x_{H_2} \\right) MW_{CO_2} {P_r \\over {R T_r} } Q_s \\\\ = \\left( 1 - x_{H_2} \\right) \\rho_{CO_2,r} Q_s \\]\nIf we assume that the users of fuel gas are using a fixed amount of energy, regardless of the actual flowrate, then what we want is the carbon intensity of the fuel: how much carbon dioxide is emitted per Megajoule of heat generated?\n\\[ E = \\frac{\\dot{m}_{CO_2} }{q} = { {\\left( 1 - x_{H_2} \\right) \\rho_{CO_2,r} Q_s} \\over {HHV(x) Q_s} } = { { \\left( 1 - x_{H_2} \\right) \\rho_{CO_2,r} } \\over {HHV(x)} }\\]\n\n# Carbon Dioxide\nMW_CO2 = 44.009e-3u\"kg/mol\"\nρ_CO2 = MW_CO2*Pᵣ/(R*Tᵣ)\n\nE(x) = (1-x)*ρ_CO2/HHV(x)\n\n\n\n\n\n\n\n\n\nFigure 4: The carbon dioxide emissions intensity for a blended natural gas/hydrogen fuel gas, over a range of hydrogen content.\n\n\n\n\n\nSo there are emissions reductions but at a cost, beyond whatever method is used to generate the hydrogen in the first place. The system must be operated at greater pressures to supply the same amount of energy, which itself takes some energy, at least until the hydrogen exceeds 96.8%. At that high level the system seems like an easy win: it takes less pressure to supply the same amount of energy and the emissions intensity is a ~8.7% that of natural gas (a ~91% reduction)\n\nE(xₑᵥₑₙ)/E(0)\n\n0.08690379085511468\n\n\nThere are a few caveats with this: for one carbon dioxide is not the only significant greenhouse gas that comes from combustion, nitrous oxide is also produced and has a global warming potential ~300× that of carbon dioxide. Unlike carbon dioxide, nitrous oxide is producded when hydrogen is combusted with air because, like many other nitrogen oxides, it is generated from the high temperature reaction of the nitrogen and oxygen from the air. So burning pure hydrogen is only net zero for very particular definitions of zero, it is not net zero greenhouse gas emissions though it is net zero carbon emissions."
  },
  {
    "objectID": "posts/hydrogen_blending/index.html#material-concerns",
    "href": "posts/hydrogen_blending/index.html#material-concerns",
    "title": "Hydrogen Blending",
    "section": "Material concerns",
    "text": "Material concerns\nSo far the analysis has completely ignored the material issues that hydrogen brings. At high temperatures (such as, say, inside a furnace that is burning hydrogen) high temperature hydrogen attack is a real concern and using hydrogen as a fuel gas would eventually destroy most burners that were designed for use with natural gas. Similarly hydrogen embrittlement would be a concern for the entire system, wherever steel is used. Neither of these are insurmountable but they would require extensive retrofitting with different materials and special alloys. This dampens a lot of the advantages of hydrogen blending, namely being able to use the existing infrastructure.\nTo skip over the details (I’m not a materials engineer), I think it is fair to say that the mechanical integrity of the system is strongly dependent on the hydrogen content and it likely will be a limiting factor in any hydrogen blending project."
  },
  {
    "objectID": "posts/hydrogen_blending/index.html#conclusions",
    "href": "posts/hydrogen_blending/index.html#conclusions",
    "title": "Hydrogen Blending",
    "section": "Conclusions",
    "text": "Conclusions\nAt the level of “back of the envelope” calculations like I have done above, it is fairly clear that blending hydrogen into the utility natural gas system is not a panacea, but then neither is it completely infeasible. I think there are several other factors that need to be considered when evaluating the possible role of hydrogen blending in the future energy mix:\n\nAvailability of hydrogen - in areas like Edmonton, there are already industrial suppliers of hydrogen (and large industrial consumers), with a roadmap to both expand that capacity and bring it to net zero emissions. Tying into that existing network significantly lowers the barrier for a blending project and can realize real emissions reductions now.\nFeasibility of alternatives - it seems to be accepted wisdom that, at least for now, air source heat pumps are not very effective below -20°C. It is entirely possible that, while retrofitting to add heat pumps to homes would be hugely effective for most of the year, households in Edmonton would still require some additional source of space heating for those extremely cold days. Not only are -20°C days fairly normal in the winter, it is not at all uncommon to exceed -30°C and periodically it gets to -40°C. Hydrogen blending could be part of that energy future, as people with heat pumps keep their furnaces around.\nExisting housing stock/pace of retrofits - it may be the case that, after performing a full life cycle analysis, heat pumps + resistive heating is the better technology. But that may fail to acknowledge the greater metropolitan area of 1.4M people that is Edmonton who, almost universally, live in buildings that do not have heat pumps and resistive heating, and instead rely on natural gas fired heaters (e.g. furnaces, boilers). With the majority of household energy use being space heating, hydrogen blending may have a role in realizing significant emissions reductions while the existing housing stock is transitioned over.\n\nPersonally I think hydrogen’s role in the future is over-hyped. A lot of people working in the fossil fuel space have pinned their industry’s future on hydrogen, which comes with a certain amount of motivated reasoning. Also hydrogen is appealing as it looks like the easy solution: swap the burning of one fuel for the burning of another, and we don’t have to make sweeping and systemic change, except that hydrogen brings its own host of issues (low energy density, material incompatibility). I think the answer will turn out not to be one silver bullet, like hydrogen, but an entire ecosystem of different technologies, often hyper specific to different locations, and what will connect them all will be the electrification of everything. That said, we have a vast, globe spanning, infrastructure and centuries of know-how in burning things and that gives hydrogen a big leg-up as a transitional solution."
  },
  {
    "objectID": "posts/hydrogen_blending/index.html#an-example-system",
    "href": "posts/hydrogen_blending/index.html#an-example-system",
    "title": "Hydrogen Blending",
    "section": "An example system",
    "text": "An example system\nI thought I would end with a basic pipe flow example, if you wanted to look at specific numbers this is how you might start that. This is also an example of the life changing magic of solving problems with code: once you have solved them once you never have to solve them again. Since I have frequently worked out pipe flow problems with julia, I can throw together a more detailed than is at all necessary model through the magic of copying and pasting.\n\nMixture viscosity\nWe’ve already worked out the mixture density and heating value, and the next most important material property is viscosity. I don’t have a curve for natural gas, so I am just going to use methane as a proxy. I am already treating natural gas like a homogeneous substance, so this is simply an extension of that.\n\nusing UnitfulCorrelations\n\n# Hydrogen - from Perry's, 8th ed.\nμ_H2(T) = (1.797e-7*T^0.685)/(1-0.59/T+140/(T^2));\n@ucorrel μ_H2 u\"K\" u\"Pa*s\"\n\n# Methane (Natural Gas) - from Perry's, 8th ed. \nμ_NG(T) = (5.2546e-7*T^0.59006)/(1+105.67/T);\n@ucorrel μ_NG u\"K\" u\"Pa*s\"\n\nWhere I have used a macro that I wrote previously to turn correlations into correlations with units.\nAt standard conditions the viscosity of hydrogen and that of natural gas (methane) are not too different, so we can get away with using a simple method for estimating the viscosity of the overall mixture.\n\nμ_H2(Tᵣ)/μ_NG(Tᵣ)\n\n0.8004989026814741\n\n\nI happen to already have Wilke’s method for a binary mixture worked out, I just need to swap in what the two components are. A more fulsome analysis would have a complete composition of natural gas (broken down into methane, ethane, propane, etc.) in which case the generalized Wilke’s method could be used as well.\n\n# mixture viscosity using Wilke method\n# from *The Properties of Gases and Liquids* 5th ed.\nfunction μ(x,T)\n    μ₁ = μ_H2(T)\n    M₁ = MW_H2\n    y₁ = x\n    \n    μ₂ = μ_NG(T)\n    M₂ = MW_NG\n    y₂ = 1-x\n    \n    ϕ₁₂ = ((1+√((μ₁/μ₂)*√(M₂/M₁)))^2)/√(8*(1+(M₁/M₂)))\n    ϕ₂₁ = ϕ₁₂*(μ₂/μ₁)*(M₁/M₂)\n    \n    μ = (y₁*μ₁/(y₁+y₂*ϕ₁₂)) + (y₂*μ₂/(y₂+y₁*ϕ₂₁))\n    return μ\nend;\n\n\n\n\n\n\n\n\n\nFigure 5: The viscosity of blended natural gas/hydrogen for a range of hydrogen content. For a wide range the viscosity is nearly constant.\n\n\n\n\n\n\n\nPipe dimensions and friction\nFor the sake of having something to calculate I am just assuming a 20m length of 2in steel pipe. But you could put in really anything here.\n\n# Pipe dimensions\nL = 20u\"m\"      # length\nD = 52.5u\"mm\"   # diameter\nϵ = 0.0457u\"mm\" # roughness\n\nA = 0.25*π*D^2  # cross-sectional area\nl = L/D         # relative length\nκ = ϵ/D         # relative roughness\n\nThe Reynold’s number is simply a function of the mass velocity, G, the pipe diameter, D, and the mixture viscosity μ\n\n# Reynold's number\nRe(x,T,G) = G*D/μ(x,T);\n\nI am using my favourite correlation for the Darcy friction factor, f,\n\n# Churchill correlation, from Perry's\nfunction churchill(Re)\n    A = (2.457 * log(1/((7/Re)^0.9 + 0.27*κ)))^16\n    B = (37530/Re)^16\n    return 8*((8/Re)^12 + 1/(A+B)^(3/2))^(1/12)\nend;\n\nSince this is just a straight length of pipe, the K factor is simply fL/D, defaulting back to the Nikuradse rough pipe law for fully developed turbulent flow (i.e. very high Reynold’s numbers)\n\nKf() = l/(2*log10(3.7/κ))^2 # Nikuradse\nKf(Re) = l*churchill(Re)    # Churchill\n\n\n\nVolumetric flowrate\nThe volumetric flowrate for an isothermal ideal gas is simply the mass velocity, G, multiplied by the cross sectional area and divided by the density GA/ρ. It is very easy to modify some code I had previously written to solve for the volumetric flowrate.\n\n# Isothermal ideal gas pipeflow\nfunction Q₁(x, T₁, P₁, P₂, K::Number)\n    ρ₁ = ρ(x, T₁, P₁)\n    v̄₁ = 1/ρ₁\n    q  = P₂/P₁\n    Q₁ = A*√((v̄₁*P₁*(1-q^2))/(K-2*log(q)))\n    return upreferred(Q₁)\nend\n\nfunction Q₁(x, T₁, P₁, P₂, K::Function)\n    # Initialize Parameters\n    ρ₁ = ρ(x, T₁, P₁)\n    q = P₂/P₁\n    \n    # Initial Guesses\n    Q₀ = Q₁(x, T₁, P₁, P₂, K())\n    G₀ = Q₀*ρ₁/A\n\n    # Numerically solve for G\n    obj(G) = (K(Re(x,T₁,G))- 2*log(q))*(G^2) - ρ₁*P₁*(1-q^2)\n    G = find_zero(obj, G₀)\n    \n    return upreferred(G*A/ρ₁)\nend\n\nThis uses julia’s multiple dispatch to handle two cases: for large Reynold’s numbers where K is a constant, and for cases where K is a function of the Reynold’s number (and thus the volumetric flowrate).\nThe volumetric flowrate at standard state is then the flowrate from above, corrected to the reference pressure and temperature1\n1 I have been using upreferred to force Unitful to cancel out and simplify units.\nQₛ(x, T₁, P₁, P₂) = upreferred((P₁/Pᵣ)*(Tᵣ/T₁)*Q₁(x, T₁, P₁, P₂, Kf))\n\nQₛ (generic function with 1 method)\n\n\n\n\nHeat rate\nThe heat rate is then the heating value, already worked out, times the volumetric flowrate at standard state\n\nq(x, T₁, P₁, P₂) = HHV(x)*Qₛ(x, T₁, P₁, P₂)\n\nq (generic function with 1 method)\n\n\n\n\n\n\n\n\n\n\nFigure 6: The energy supplied by the example fuel gas delivery system for a range of pressure drops. Pure natural gas and pure hydrogen deliver nearly the same energy for the same pressure drop.\n\n\n\n\n\nWe see the same ordering that we expect, given the previous analysis, namely that the 0% and 100% cases are pretty close to each other, followed by the in-between hydrogen contents.\nAnother way of looking at this is to pick a required heat rate and look at the pressure drop as a function of hydrogen content.\n\n\n\n\n\n\n\n\nFigure 7: The pressure drop required to deliver a fixed heat rate for blended natural gas/hydrogen fuel gas in the example system.\n\n\n\n\n\nAll of this has been done assuming the ideal gas case. The next logical step is to start incorporating non-ideal gas models, say a cubic equation of state, and so on."
  },
  {
    "objectID": "posts/hydrogen_blending/index.html#references",
    "href": "posts/hydrogen_blending/index.html#references",
    "title": "Hydrogen Blending",
    "section": "References",
    "text": "References\n\n\nGPSA. Engineering Data Book. 13th ed. Tulsa, OK: Gas Processors Suppliers Association, 2012.\n\n\nGreen, Don W., ed. Perry’s Chemical Engineers’ Handbook. 8th ed. New York: McGraw Hill, 2008.\n\n\nPoling, Bruce E., John M. Prausnitz, and John P. O’Connell. The Properties of Gases and Liquids. 5th ed. New York: McGraw Hill, 2001.\n\n\nPoling, Bruce E., George H. Thomson, Daniel G. Friend, Richard L. Rowley, and W. Vincent Wilding. “Physical and Chemical Data.” In Perry’s Chemical Engineers’ Handbook, edited by Don W. Green, 8th ed. New York: McGraw Hill, 2008."
  },
  {
    "objectID": "posts/federal_election/index.html",
    "href": "posts/federal_election/index.html",
    "title": "The 2021 Canadian Federal Election",
    "section": "",
    "text": "On Monday, September 20 2021, Canadians went to the polls and ended up electing a parliament that looked very much like the one we had in August, prior to the election. Very notably so. I’m not much of a political watcher, but I did wonder was this really so notably similar? Or do we just have short memories?\nThis is easy enough to answer."
  },
  {
    "objectID": "posts/federal_election/index.html#methodology",
    "href": "posts/federal_election/index.html#methodology",
    "title": "The 2021 Canadian Federal Election",
    "section": "Methodology",
    "text": "Methodology\nWhat I would like to do is take a table with the number of seats each party got in each election and calculate the change in seats from one election to the next, then add that up. I can’t simply add it up though, as the total number of seats (usually) remains constant and any party’s gain is another party’s loss: the total would always be zero. Instead I am going to add up the absolute value of the change, which effectively double counts each seat (it is counted when one party loses it and again when another party gains it). Also the total number of seats in the house of commons has not always been 338, to adjust for this I will take the absolute value of the change in percentage of seats. So, for example, if party A enters an election with 20% of the seats and leaves with 20% of the seats then this counts as no change, though if they entered with 20 seats and left with 20 seats but the overall number of seats had increased, then that counts as a change.\nI can calculate this for each election and see how much of an outlier 2021 was.\n\nusing CSV, DataFrames, Statistics, Pipe, Plots\n\n\n# takes a dataframe of the form\n# | YEAR | party1 | party 2 | ... | party n |\n# |------|--------|---------|-----|---------|\n# | 1    |  100   |  50     | ... |   0     |\n# |  :   |    :   |    :    |  :  |   :     |\n# |  m   |   30   |   160   | ... |   1     |\n# and returns a length m vector with the relative change for each year\nfunction seat_change(df)\n    \n    # the first election seat change is undefined\n    changes = [NaN]\n    \n    for i in 2:nrow(df)\n        # starting with the second election\n        prev = df[i-1, Not(:YEAR)]\n        prev_total = sum(prev)\n        \n        curr = df[i, Not(:YEAR)]\n        curr_total = sum(curr)\n        \n        Δseats = 0\n        \n        # for each party, calculate the absolute difference\n        for j in 1:length(curr)\n            \n            prev_pct = prev[j]/prev_total\n            curr_pct = curr[j]/curr_total\n            Δseats += abs(curr_pct - prev_pct)\n        end\n        \n        # add the change to the list\n        push!(changes, Δseats)\n    end\n    \n    return changes\nend"
  },
  {
    "objectID": "posts/federal_election/index.html#dataset",
    "href": "posts/federal_election/index.html#dataset",
    "title": "The 2021 Canadian Federal Election",
    "section": "Dataset",
    "text": "Dataset\nI pulled the seat count for each federal election since 1867 from wikipedia as a CSV, with a little bit of finessing in the data entry. We have had a lot of political parties in our short time as a country and many of them either never ended up with any seats or only one or two before disappearing from history – I have elected to lump these in with the independents as “Other”. We have also had several parties that merged or changed, for example the CCF ultimately became the NDP and the Reform party became part of the Canadian Alliance, I have chosen to treat those as the same party.\nRunning this through the function I defined earlier gives the relative absolute seat change per election.\n\ndata_file = \"data/federal-electon-results.csv\"\n\nresults = @pipe data_file |&gt;\n    CSV.File( _ ;  header=1 ) |&gt;\n    DataFrame(_) |&gt;\n    hcat(_, seat_change(_)) |&gt;\n    rename(_, \"x1\" =&gt; \"Change\")\n\nshow(first(results, 6), allcols=true)\n\n\n6×14 DataFrame\n\n Row │ YEAR   Other  Liberal  Conservatives  CCF/NDP  BQ     Progressive  Anti-Confederate  Social Credit  United Farmers  Reform/Canadian Alliance  Liberal Progressive  Unionist Coalition  Change      \n\n     │ Int64  Int64  Int64    Int64          Int64    Int64  Int64        Int64             Int64          Int64           Int64                     Int64                Int64               Float64     \n\n─────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n   1 │  1867      0       62            100        0      0            0                18              0               0                         0                    0                   0  NaN\n\n   2 │  1872      5       95            100        0      0            0                 0              0               0                         0                    0                   0    0.311111\n\n   3 │  1874     12      129             65        0      0            0                 0              0               0                         0                    0                   0    0.368932\n\n   4 │  1878      9       63            134        0      0            0                 0              0               0                         0                    0                   0    0.669903\n\n   5 │  1882      4       73            134        0      0            0                 0              0               0                         0                    0                   0    0.0802926\n\n   6 │  1887     11       80            124        0      0            0                 0              0               0                         0                    0                   0    0.116654"
  },
  {
    "objectID": "posts/federal_election/index.html#results",
    "href": "posts/federal_election/index.html#results",
    "title": "The 2021 Canadian Federal Election",
    "section": "Results",
    "text": "Results\nPlotting the results gives us some interesting years to think about, such as 1917 when the government was composed of the Unionist Coalition, a coalition of mostly Conservatives and some Liberals and others, that basically only existed for the war in what was, apparently, one of the most bitter campaigns in Canadian history. For the next election the coalition dissolved back into it’s original parties, hence an enormous change going in and going out of that parliament. There are other large changes, like the 1993 election in which the Conservatives went into the election with 156 seats and left with 2, nearly being wiped out of parliament entirely – the largest change in history according to this metric.\nThere have been periods of low change, the red-line on the plot indicates a change of less than 10%, but none as low as 2021. I do find it interesting that in the late 1800s and the early 1900s we had successive governments with very little change in overall composition but after 1908 things are a lot more variable.\n\n\n\n\n\n\n\n\nFigure 1: Change in seats per Canadian Federal Election, 1867-2021\n\n\n\n\n\nWe can filter out the low-change elections and get a sense of not just the 2021 election, but the neighbourhood of low-change elections.\n\nlowest = filter(row -&gt; row[:Change] &lt; 0.10, results)\n\nsort!(lowest, [:Change]);\n\n\n\n\n7×9 DataFrame\n\n Row │ YEAR   Liberal  Conservatives  BQ     CCF/NDP  Social Credit  Liberal Progressive  Other  Change    \n\n     │ Int64  Int64    Int64          Int64  Int64    Int64          Int64                Int64  Float64   \n\n─────┼─────────────────────────────────────────────────────────────────────────────────────────────────────\n\n   1 │  2021      158            119     34       25              0                    0      2  0.0236686\n\n   2 │  1940      179             39      0        8             10                    3      6  0.0653061\n\n   3 │  1965      131             97      0       21             14                    0      2  0.0754717\n\n   4 │  1908      133             85      0        0              0                    0      3  0.0767539\n\n   5 │  1904      137             75      0        0              0                    0      2  0.0784959\n\n   6 │  1882       73            134      0        0              0                    0      4  0.0802926\n\n   7 │  1891       90            118      0        0              0                    0      7  0.0930233\n\n\n\nThis is an exceptionally low change, the next lowest year (1940) had &gt;2× as many seats change hands. Also, the last time the overall seat change was even close to this low was decades ago, the next previous year with a relative change &lt;10% was 1965 and in that case &gt;3× as many seats changed hands.\nThis result may change, as of right now several ridings are still too-close to call without mail in ballots, but for some of those if they flip it will actually lower the overall change in seats, not increase it. For example Edmonton Center is currently undecided with the Liberal candidate ahead, but if it flips to the incumbent Conservative the overall relative change for this election would go down."
  },
  {
    "objectID": "posts/ooms_plume_model/index.html",
    "href": "posts/ooms_plume_model/index.html",
    "title": "The Ooms Plume Model",
    "section": "",
    "text": "I have been interested in the Ooms plume model1 for a long time, but I haven’t really set aside the time to really play around with it because the implementation details surprisingly sparse. A recent weekend project of mine was to sit down and work out what the actual model equations are and get it running in julia. Something which might be useful to you if you are looking to run one of the O.G. integral plume models."
  },
  {
    "objectID": "posts/ooms_plume_model/index.html#the-ooms-plume-model",
    "href": "posts/ooms_plume_model/index.html#the-ooms-plume-model",
    "title": "The Ooms Plume Model",
    "section": "The Ooms Plume Model",
    "text": "The Ooms Plume Model\nThe Ooms plume model is a model of a continuous jet of fluid exiting into a crossflow. Unlike, for example, a simple Gaussian model which assumes the source has no momentum, or a free jet model which assumes there is no crossflow, the Ooms model accounts for the buoyancy and momentum of the jet as well as the crossflow without resorting empirical correlations (such as the Briggs’ model).\nHowever, unlike those simpler models, the Ooms model is not in the form of simple closed form expressions. It is an integral plume model which results in a system of differential algebraic equations which must be solved numerically for each particular plume. Unlike earlier integral plume models, which assumed a top hat velocity and density profile, the Ooms model assumes the plume parameters follow Gaussian profiles.\n\n\n\n\n\n\nFigure 1: A sketch of the plume and the coordinate system.\n\n\n\nConsider the sketch of a vertical vent shown in Figure 1. The plume starts at some point down stream of the actual vent, after the zone of flow establishment characterized by an elevation δ. The plume rises due to the buoyancy and momentum in the vent gases and bends over as it is carried along by the wind. The coordinate system is arranged such that the wind is in the positive x-direction and the center-line of the plume is within the x-z plane.\nTaking a slice through the plume, we assume it has a circular cross-section and use a local cylindrical coordinate system with s the direction along the plume axis, r the radial direction, and φ the radial angle. The overall plume radius at any point is \\(\\sqrt{2}b\\), with b a characteristic length which is a function of distance along the center-line.\nZooming in on a differential element of the plume, Figure 2, we take it be approximately a cylinder where flow within the plume enters and exits through the circular ends and air is entrained through the outer surface with some entrainment velocity E.\n\n\n\n\n\n\nFigure 2: A differential element of the plume along the plume center-line.\n\n\n\nThe Ooms model comes from the conservation relations for this differential element.\n\nConservation of…\n\nMass\nThe mass exiting the differential element is equal to the mass entering through the plume plus the entrained air.\n\\[ m_{out} = m_{in} + m_{ent} \\]\nThe mass of entrained air is simply the product of the mass flux (ρE) and the area:\n\\[ m_{ent} = \\rho_a E \\cdot 2\\pi \\left( \\sqrt{2} b \\right) ds \\]\nGiving a mass balance equation:\n\\[ \\frac{d}{ds} m = 2\\pi \\rho_a b \\left( \\sqrt{2} E \\right) \\]\nThe mass passing through a surface is simply the mass flux G = ρ u integrated over the surface area:\n\\[ m = \\int_{A_{in}} \\rho u dA = \\int_0^{2\\pi} \\int_0^{\\sqrt{2}b} \\rho u r dr d\\phi \\] \\[ m = 2\\pi \\int_{0}^{\\sqrt{2}b} \\rho u r dr \\]\nFinally giving\n\\[ 2\\pi \\frac{d}{ds} \\int_{0}^{\\sqrt{2}b} \\rho u r dr = 2\\pi \\rho_a b \\left( \\sqrt{2} E \\right) \\] \\[ \\frac{d}{ds} \\int_{0}^{\\sqrt{2}b} \\rho u r dr = \\rho_a b E \\]\n\n\n\n\n\n\nNote\n\n\n\nAn errant \\(\\sqrt{2}\\) has disappeared from the right hand side of the equation. It has been absorbed into the constants in E. The right hand side of the balance equations in Ooms2 appear at first blush like they were done for a top hat model of a plume with radius b, which would be a mistake. However, as the overall radius of a plume in a top hat model btop-hat = \\(\\sqrt{2}b_{gauss}\\), when the constants are scaled by a factor of \\(\\sqrt{2}\\) the two look the same.\n\n\n2 “A New Method for the Calculation of the Plume Path of Gases Emitted by a Stack”.\n\nSpecies\nThe total mass of the vented substance is conserved as the plume expands. Assuming the vent is some species i with mass concentration c:\n\\[ \\frac{d}{ds} m_{i} = 0 \\] \\[ m_i = \\int_0^{2\\pi} \\int_0^{\\sqrt{2}b} c u r dr d\\phi = 2\\pi \\int_0^{\\sqrt{2}b} c u r dr \\] \\[ \\frac{d}{ds} \\int_0^{\\sqrt{2}b} c u r dr = 0 \\]\n\n\nMomentum\nThere are two equations for conservation of momentum: in the x-direction and z-direction. This is a consequence of the choice of coordinates – that the plume centerline is confined to the x-z plane and neither the jet nor the crossflow have velocity in the y-direction. In particular the coordinates were chosen such that the crossflow is entirely in the x-direction with velocity \\(u_a\\).\nIn the x-direction the total momentum into the differential element is the mass in times the velocity component in the x direction:\n\\[ p_{x,in} = \\int_{A_{in}} \\rho_{in} u_{in} u_{x,in} dA = \\int_{A_{in}} \\rho_{in} u_{in}^2 \\cos\\theta_{in} dA \\]\nAnd similarly for the total momentum leaving the element\n\\[ p_{x,out} = \\int_{A_{out}} \\rho_{out} u_{out} u_{x,out} dA = \\int_{A_{out}} \\rho_{out} u_{out}^2 \\cos\\theta_{out} dA \\]\nThe change in momentum is equal to the momentum added to the plume from entrainment and drag from the wind. In this case the drag force acts in the positive direction, pushing the plume along.\n\\[ p_{x,out} - p_{x,in} = m_{ent} u_a + F_{d,x} \\]\nOoms notes that the drag force on the plume is only due to the component of the wind velocity which is perpendicular to the plume direction, \\(u_a \\sin \\theta\\). Drag then follows the standard relationship, with the area being the outside surface area of the cylinder.\n\\[ F_{d} = \\frac{1}{2} C_d A_{\\perp} \\rho_a v^2 = \\frac{1}{2} C_d A_{\\perp} \\rho_a u_a^2 \\sin^2 \\theta \\]\nThe drag force in the x-direction is acting on the area perpendicular to the x-direction\n\\[ F_{d,x} = \\frac{1}{2} C_d \\rho_a u_a^2 \\sin^2 \\theta 2\\pi \\left(\\sqrt{2}b\\right) | \\sin \\theta | ds = \\pi b C_d \\rho_a u_a^2 | \\sin^3 \\theta | ds\\]\nWhere the absolute value comes from the drag force being always positive.\nGiving\n\\[ 2 \\frac{d}{ds} \\int_{0}^{\\sqrt{2}b} \\rho u^2 \\cos \\theta r dr = 2 b \\rho_a u_a E + \\pi b C_d \\rho_a u_a^2 | \\sin^3 \\theta | \\]\nIn the z-direction the change in momentum is due to buoyant forces and drag in the z-direction. The buoyant force can be written as:\n\\[ F_b = \\int_V g \\left(\\rho_a - \\rho \\right) dV = 2\\pi ds \\int_0^{\\sqrt{2}b} g \\left(\\rho_a - \\rho \\right) rdr \\cdot \\]\nAssuming the density within the differential element is approximately constant with s. Combining with the drag force in the z-direction gives the final momentum balance:\n\\[ 2 \\frac{d}{ds} \\int_{0}^{\\sqrt{2}b} \\rho u^2 \\sin \\theta r dr = 2 \\int_0^{\\sqrt{2}b} g \\left(\\rho_a - \\rho \\right) r dr + \\mathrm{sgn}\\theta \\cdot \\pi b C_d \\rho_a u_a^2 \\sin^2 \\theta \\cos \\theta \\]\nWhere \\(\\mathrm{sgn} \\theta\\) ensures the drag force is acting in the right direction.\n\n\nEnergy\nStarting from an energy balance, using the ambient temperature as the reference temperature, the enthalpy entering the differential element is:\n\\[ H_{in} = \\int_{A_{in}} \\rho u_{in} c_p \\left( T - T_{a,0} \\right) dA \\]\nSimilarly for the enthalpy out, giving an enthalpy change over the element of:\n\\[ d \\left( \\int_{A} \\rho u_{in} c_p \\left( T - T_{a,0} \\right) dA \\right) = d \\left( 2\\pi \\int_0^{\\sqrt{2}b} \\rho u c_p \\left( T - T_{a,0} \\right) r dr \\right) \\]\nTo be very abusive of notation. Where T is the temperature of the plume and Ta,0 is the reference temperature – the ambient temperature at the vent exit. The enthalpy change is assumed to come only from entrainment. The enthalpy added to the differential element from entrainment of air is:\n\\[ \\rho_a E c_{p,a} \\left( T_a - T_{a,0} \\right) \\cdot 2\\pi b ds  \\]\nPutting it all together we get the energy balance:\n\\[ \\frac{d}{ds} \\int_0^{\\sqrt{2}b} \\rho u c_p \\left( T - T_{a,0} \\right) r dr = b \\rho_a E c_{p,a} \\left( T_a - T_{a,0} \\right) \\]\nAssuming the ideal gas law, we can make the substitution:\n\\[ T = {{ P {MW} } \\over {R \\rho}} \\]\nFurthermore, if we assume \\(MW = MW_a\\) and \\(c_p = c_{p,a}\\) then we can cancel all those constants giving:\n\\[ \\frac{d}{ds} \\int_0^{\\sqrt{2}b} \\rho u \\left( \\frac{1}{\\rho} - \\frac{1}{\\rho_{a,0}} \\right) r dr = b \\rho_a E \\left( \\frac{1}{\\rho_a} - \\frac{1}{\\rho_{a,0}} \\right) \\]\nThese seem like radical assumptions if you are coming to the Ooms plume model as a dense gas dispersion model, but the original paper is concerned with the release of stack gases from combustion equipment. For stack gases this is not unreasonable and other models such as the Briggs’ model for plume rise make similar simplifications (any model that calculates buoyant flux from plume temperature alone is making that assumption implicitly).\n\n\n\nCoordinate Transforms\nUp until this point all of the plume parameters have been calculated along the plume axis. This needs to be translated into the original coordinate system to be useful, in particular the curve the plume axis takes through space is given by:\n\\[ \\frac{dx}{ds} = \\cos \\theta \\] \\[ \\frac{dz}{ds} = \\sin \\theta \\]\n\n\nEntrainment\nOne of the most important parts of the model is how it accounts for entrainment. Ooms considers entrainment to be the sum of three processes.\nIn the immediate vicinity of the jet exit, when the jet velocity dominates, the entrainment is taken to be the same as a free jet, namely that it is proportional to the jet center line velocity. In this case we take the excess velocity:\n\\[ E_1 = \\alpha_1 | u - u_a \\cos \\theta | \\]\nWhere \\(u_a \\cos \\theta\\) is the component of the wind velocity parallel to the jet. The parameter \\(\\alpha_1\\) is called the entrainment coefficient for a free jet and is independent of Reynolds’ number when \\(\\mathrm{Re} &gt; 10^4\\). Ooms gives this as \\(\\alpha_1 = 0.057\\).\nAt distances further down the plume axis, when \\(u \\approx u_a\\), the entrainment is taken to be the same as a cylindrical thermal in a stagnant atmosphere, given as:\n\\[ E_2 = \\alpha_2 u_a | \\sin \\theta | \\]\nWhere \\(\\alpha_2\\) is called the entrainment coefficient for a line thermal, it is similarly a constant at large Reynolds’ numbers. Ooms gives this as \\(\\alpha_2 = 0.5\\)\nTo connect these two regimes, Ooms multiplies the line thermal term by \\(\\cos \\theta\\). This doesn’t seem to have any theoretical justification, it just works to make the second term disappear when the vent is still mostly vertical. This is an important feature to note. The model is often presented such that the initial angle of the jet can be anything, but a key assumption of the entrainment model is that the jet is initially vertical.\nFinally, Ooms adds a term to entrainment due to atmospheric turbulence. Presumably if you were only interested in jets entering a crossflow where that flow was nice and laminar you would leave this out. But Ooms is specifically developing his model for vent stacks releasing plumes into the atmosphere, and the actual structure of the atmosphere and its turbulence must be accounted for. He does this by including an entrainment velocity due to turbulence \\(u^{\\prime}\\)\n\\[ E_3 = \\alpha_3 u^{\\prime} \\]\nWhere \\(\\alpha_3\\) is the entrainment coefficient due to turbulence, which is taken to be \\(\\alpha_1 = 1.0\\). The entrainment velocity due to turbulence can be accounted for in one of two ways:\n\nFollowing Briggs, \\(u^{\\prime} = \\sqrt[3]{\\epsilon b}\\) where \\(\\epsilon\\) is the eddy energy dissipation and is a function of atmospheric stability and elevation.\nEmpirically by the root-mean-square of the wind velocity fluctuation \\(u^{\\prime} = \\sqrt{u_a^2}\\)\n\nThe total entrainment is then:\n\\[ E = E_1 + E_2 \\cos \\theta + E_3 \\]\n\\[ E = \\alpha_1 | u - u_a \\cos \\theta | + \\alpha_2 u_a | \\sin \\theta | \\cos \\theta + \\alpha_3 u^{\\prime} \\]\nor\n\\[ E = \\alpha_1 | u^{*} | + \\alpha_2 u_a | \\sin \\theta | \\cos \\theta + \\alpha_3 u^{\\prime} \\]\nWhere \\(u^{*}\\) is defined in the next section.\n\n\nSimilarity Profiles\nEarlier I mentioned that the velocity, density, and concentration in the plume are assumed to have Gaussian profiles. Though it doesn’t really have a theoretical basis, Gaussian profiles are mathematically convenient and fit observed profiles quite well. This has been experimentally validated for both free jets and bent over plumes.3\n3 Keffer and Baines, “The Round Turbulent Jet in a Cross-Wind”.The velocity is taken to be the component of the wind velocity parallel to the plume axis plus an excess velocity:\n\\[ u = u_a \\cos \\theta + u^{*} \\exp \\left( - \\left(r \\over b\\right)^2 \\right) \\]\nThe plume density, similarly, is the air density plus an excess density:\n\\[ \\rho = \\rho_a + \\rho^{*} \\exp \\left( - \\left(r \\over {\\lambda b} \\right)^2 \\right) \\]\nFinally, the concentration simply follows a Gaussian profile:\n\\[ c = c^{*} \\exp \\left( - \\left(r \\over {\\lambda b} \\right)^2 \\right) \\]\nWhere \\(\\frac{1}{\\lambda^2}\\) is the turbulent Schmidt number. This is entirely analogous to a free jet. I’m not sure entirely why Ooms gives the Schmidt number as what I would call the inverse of the Schmidt number, but that is just a quibble of notation.\nOoms uses a value of \\(\\lambda^2 = 1.35\\) or \\(\\mathrm{Sc}_t = 0.741\\), which is consistent with observations of free jets."
  },
  {
    "objectID": "posts/ooms_plume_model/index.html#practical-necessities",
    "href": "posts/ooms_plume_model/index.html#practical-necessities",
    "title": "The Ooms Plume Model",
    "section": "Practical Necessities",
    "text": "Practical Necessities\nThe original paper does not provide the final differential algebraic equations, nor does it provide the worked out integrals, that is left as an exercise for the reader. I looked around and could not find a detailed description of the final model equations other than in the model documentation for DEGADIS.4 An earlier version of DEGADIS used the Ooms plume model for dense gas plumes with modifications to the model assumptions and, especially, the energy balance. This is a good start, but it is presented in its final matrix form with 17 model constants that are pre-calculated. It is not immediately clear where the model constants come from and how they are related to the constant λ.\n4 Havens and Spicer, “A Dispersion Model for Elevated Dense Gas Jet Chemical Releases,” 7–13.\n\n\n\n\n\nFigure 3: The model constants from Havens,5 note the misprint in \\(k_{14}\\) (should read 2.227186)\n\n5 Havens and Spicer, 12.\n\nThe version in DEGADIS is intended for dense gas dispersion and makes additional assumptions such as that there is no vertical change in air density. This is a reasonable assumption for dense plumes that fall back to earth and roll along the ground, but is something that would have to be corrected for large buoyant plumes rising high into the air.\nI did my own working out here because I wanted two things:\n\nThe relationship between the model constants (e.g λ) and the integration constants (the k’s in DEGADIS)\nTo re-create the model that allows for more structure to the atmosphere.\n\n\nA Series of Tedious Integrals\nThe integrals are not difficult to work out, though they can turn into a sort of alphabet soup of variables. The integrals involving Gaussians all involve something of the form \\(\\int \\exp(-ar^2) r dr\\) which has a nice closed form solution.\nI worked out five different constants that are integrals of the Gaussian profiles and the products of them:\n\\[ C_1 = 2 \\int_0^{\\sqrt{2}} \\exp \\left( - \\xi^2 \\right) \\xi d\\xi = 1 - \\exp \\left( -2 \\right)\\] \\[ C_2 = 2 \\int_0^{\\sqrt{2}} \\exp \\left( - \\left( \\frac{\\xi}{\\lambda} \\right)^2 \\right) \\xi d\\xi = \\lambda^2 \\left( 1 - \\exp \\left( -\\frac{2}{\\lambda^2} \\right) \\right)\\] \\[ C_3 = 2 \\int_0^{\\sqrt{2}} \\exp \\left( - \\xi^2 - \\left( \\frac{\\xi}{\\lambda} \\right)^2 \\right) \\xi d\\xi = \\frac{\\lambda^2}{\\lambda^2 + 1} \\left( 1 - \\exp \\left( -\\frac{2 \\left(\\lambda^2 + 1\\right)}{\\lambda^2} \\right) \\right)\\] \\[ C_4 = \\int_0^{\\sqrt{2}} \\exp \\left( - 2\\xi^2 \\right) \\xi d\\xi = \\frac{1}{4} \\left( 1 - \\exp \\left( -4 \\right) \\right)\\] \\[ C_5 = \\int_0^{\\sqrt{2}} \\exp \\left( - 2\\xi^2 - \\left( \\frac{\\xi}{\\lambda} \\right)^2 \\right) \\xi d\\xi = \\frac{\\lambda^2}{4\\lambda^2 + 2} \\left( 1 - \\exp \\left( -\\frac{4\\lambda^2 + 2}{\\lambda^2} \\right) \\right)\\]\nThese are basically in the order that I encountered them when working out the integrals and could probably be cleaned up for some consistency. Throughout I made the substitution \\(\\xi = \\frac{r}{b}\\) such that every integral of a Gaussian in the model becomes \\(b^2 C\\) where the C corresponds to one of the above. Each of the 17 constants in the DEGADIS model correspond to one of these constants times a scaling factor. For all but \\(k_1\\) and \\(k_2\\) they are integer scaling factors, for the first two they \\(\\frac{1}{\\lambda^2}\\) times \\(C_2\\) and \\(C_3\\) respectively. Below is a table showing the concordance.\n\n\n\nTable 1: Integration Constants\n\n\n\n\n\n\n\n\n\nDEGADIS6\nMe\n\n\n\n\n\\(k_{1 }\\)\n\\(\\frac{C_2}{\\lambda^2}\\)\n\n\n\\(k_{2 }\\)\n\\(\\frac{C_3}{\\lambda^2}\\)\n\n\n\\(k_{3 }\\)\n\\(C_1\\)\n\n\n\\(k_{4 }\\)\n\\(C_2\\)\n\n\n\\(k_{5 }\\)\n\\(C_3\\)\n\n\n\\(k_{6 }\\)\n\\(2C_1\\)\n\n\n\\(k_{7 }\\)\n\\(2C_4\\)\n\n\n\\(k_{8 }\\)\n\\(2C_3\\)\n\n\n\\(k_{9 }\\)\n\\(2C_5\\)\n\n\n\\(k_{10}\\)\n\\(4C_4\\)\n\n\n\\(k_{11}\\)\n\\(4C_5\\)\n\n\n\\(k_{12}\\)\n\\(4C_1\\)\n\n\n\\(k_{13}\\)\n\\(3C_2\\)\n\n\n\\(k_{14}\\)\n\\(4C_3\\)\n\n\n\\(k_{15}\\)\n\\(\\frac{C_2}{2}\\)\n\n\n\\(k_{16}\\)\n\\(\\frac{C_1}{2}\\)\n\n\n\\(k_{17}\\)\n\\(\\frac{C_3}{2}\\)\n\n\n\n6 Havens and Spicer, 12.\n\n\n\n\nDimensionless Form\nIt is decidedly easier to put everything in dimensionless form first, using the following (where a bar over the variable indicates that it is dimensionless):\n\\[ \\bar{s} = \\frac{s}{D} \\] \\[ \\bar{c} = \\frac{c^{*}}{c_0} \\] \\[ \\bar{b} = \\frac{b}{D} \\] \\[ \\bar{u} = \\frac{u^{*}}{u_a} \\] \\[ \\bar{\\rho} = \\frac{\\rho^{*}}{\\rho_a} \\] \\[ \\bar{x} = \\frac{x}{D} \\] \\[ \\bar{z} = \\frac{z}{D} \\]\nWhere D is the initial jet diameter. This is the main point where what follows diverges from DEGADIS, where the model is given in dimensional form, which makes each of the expressions much larger and makes direct comparison between the two something of a chore.\n\n\nThe Full Equations\n\nConservation of Mass\nThusly equipped, we can work out the integrals and subsequently all the derivatives. Starting with the conservation of mass:\n\\[ \\int_0^{\\sqrt{2}b} \\rho u r dr = \\int_0^{\\sqrt{2}b} \\rho_a u_a \\left( 1 + \\bar{\\rho} \\exp \\left( - \\left(r \\over {\\lambda b} \\right)^2 \\right) \\right) \\left( \\cos \\theta + \\bar{u} \\exp \\left( - \\left(r \\over {b} \\right)^2 \\right) \\right) r dr\\]\n\\[ = \\rho_a u_a b^2 \\left( \\cos \\theta + \\bar{\\rho} \\cos \\theta \\int_0^{\\sqrt{2}} \\exp \\left( - \\left(\\xi \\over {\\lambda } \\right)^2 \\right) \\xi d\\xi + \\bar{u} \\int_0^{\\sqrt{2}} \\exp \\left( - \\xi^2 \\right) \\xi d\\xi + \\bar{\\rho} \\bar{u} \\int_0^{\\sqrt{2}} \\exp \\left( - \\xi \\left(\\xi \\over {\\lambda } \\right)^2 \\right) \\xi d\\xi \\right)\\] \\[ = \\frac{1}{2} \\rho_a u_a D^2 \\bar{b}^2 \\left( \\left( C_1 + C_3 \\bar{\\rho} \\right) \\bar{u} + \\left(2 + C_2 \\bar{\\rho} \\right) \\cos \\theta \\right) \\]\nSo the balance equation is:\n\\[ \\frac{1}{2} \\rho_a u_a \\frac{d}{d\\bar{s}} \\bar{b}^2 \\left( \\left( C_1 + C_3 \\bar{\\rho} \\right) \\bar{u} + \\left(2 + C_2 \\bar{\\rho} \\right) \\cos \\theta \\right) =  b \\rho_a u_a \\bar{E} \\] \\[ \\frac{d}{d\\bar{s}} \\bar{b}^2 \\left( \\left( C_1 + C_3 \\bar{\\rho} \\right) \\bar{u} + \\left(2 + C_2 \\bar{\\rho} \\right) \\cos \\theta \\right) =  b \\bar{E} \\]\nWhere \\(\\bar{E} = \\frac{E}{u_a}\\) is the dimensionless entrainment velocity.\nExpanding out the derivatives and dividing through by b, we get:\n\\[ \\left( 2\\cos \\theta  \\left( 2 + C_2 \\bar{\\rho} \\right) + 2 \\bar{u} \\left( C_1 + C_3\\bar{\\rho} \\right) \\right) \\frac{d\\bar{b}}{d\\bar{s}} \\] \\[ + \\bar{b} \\left( C_1 + C_3\\bar{\\rho} \\right) \\frac{d\\bar{u}}{d\\bar{s}} \\] \\[ - \\bar{b} \\sin \\theta \\left( 2 + C_2\\bar{\\rho} \\right) \\frac{d\\theta}{d\\bar{s}} \\] \\[ + \\bar{b} \\left( C_2\\cos \\theta + C_3\\bar{u} \\right) \\frac{d\\bar{\\rho}}{d\\bar{s}} = 2 \\bar{E}\\]\n\n\nConservation of Species\nIn the interests of not having this go on forever, I’m going to skip the details on the integral (they should be fairly obvious) and just give the balance equation and the final form with expanded out derivatives.\nThe balance equation is:\n\\[ \\frac{d}{d\\bar{s}} \\left( \\bar{c}\\bar{b}^2 \\left( C_2 \\cos \\theta + C_3 \\bar{u} \\right) \\right) = 0 \\]\nThe final form is:\n\\[ \\bar{b} \\left(C_3\\bar{u} + C_2 \\cos \\theta \\right) \\frac{d\\bar{c}}{d\\bar{s}} \\] \\[ + 2 \\bar{c} \\left(C_3\\bar{u} + C_2 \\cos \\theta \\right) \\frac{d\\bar{b}}{d\\bar{s}} \\] \\[ + C_3 \\bar{c} \\bar{b} \\frac{d\\bar{u}}{d\\bar{s}} \\] \\[ - C_2 \\bar{c} \\bar{b} \\sin \\theta \\frac{d\\theta}{d\\bar{s}} = 0\\]\n\n\nConservation of Momentum\nThe balance equation in the x-direction is:\n\\[ \\frac{d}{d\\bar{s}} \\left[ \\bar{b}^2 \\cos \\theta \\left( 2\\bar{u}^2 \\left( C_4 + C_5\\bar{\\rho} \\right) + 2\\bar{u} \\cos\\theta \\left(C_1 + C_3 \\bar{\\rho} \\right) + \\cos^2 \\theta \\left( 2 + C_2 \\bar{\\rho} \\right) \\right) \\right] = \\bar{b} \\left( 2\\bar{E} + C_d | \\sin^3 \\theta | \\right) \\]\nThe final form is:\n\\[ 2\\cos\\theta \\left[ 2\\bar{u}^2 \\left( C_4 + C_5 \\bar{\\rho} \\right) + 2\\bar{u} \\cos\\theta \\left(C_1 + C_3 \\bar{\\rho} \\right) + \\cos^2 \\theta \\left( 2 + C_2 \\bar{\\rho} \\right) \\right] \\frac{d\\bar{b}}{d\\bar{s}} \\] \\[ + 2\\bar{b} \\cos\\theta \\left[ 2\\bar{u} \\left( C_4 + C_5\\bar{\\rho} \\right) +  \\cos\\theta \\left(C_1 + C_3 \\bar{\\rho} \\right) \\right] \\frac{d\\bar{u}}{d\\bar{s}} \\] \\[ - \\bar{b} \\sin\\theta \\left[ 2\\bar{u}^2 \\left( C_4 + C_5 \\bar{\\rho} \\right) + \\cos \\theta \\left( 4\\bar{u} \\left(C_1 + C_3 \\bar{\\rho} \\right) + 3\\cos \\theta \\left( 2 + C_2 \\bar{\\rho} \\right) \\right) \\right] \\frac{d\\theta}{d\\bar{s}} \\] \\[ + \\bar{b} \\cos\\theta \\left[ C_2 \\cos^2 \\theta + 2C_3 \\bar{u} \\cos \\theta + 2 C_5 \\bar{u}^2 \\right] \\frac{d\\bar{\\rho}}{d\\bar{s}} = 2 \\bar{E} + C_d | \\sin^3 \\theta |\\]\nThe balance equation in the z-direction is:\n\\[ \\frac{d}{d\\bar{s}} \\left[ \\bar{b}^2 \\sin \\theta \\left( 2\\bar{u}^2 \\left( C_4 + C_5\\bar{\\rho} \\right) + 2\\bar{u} \\cos\\theta \\left(C_1 + C_3 \\bar{\\rho} \\right) + \\cos^2 \\theta \\left( 2 + C_2 \\bar{\\rho} \\right) \\right) \\right] = -C_2 \\bar{b}^2 \\bar{\\rho} \\bar{g} + \\mathrm{sgn}\\theta \\cdot C_d \\bar{b} \\sin^2 \\theta \\cos \\theta\\]\nWhere \\(\\bar{g} = \\frac{Dg}{u_a^2}\\) is the dimensionless gravity.\nThe final form is:\n\\[ 2\\sin\\theta \\left[ 2\\bar{u}^2 \\left( C_4 + C_5 \\bar{\\rho} \\right) + 2\\bar{u} \\cos\\theta \\left(C_1 + C_3 \\bar{\\rho} \\right) + \\cos^2 \\theta \\left( 2 + C_2 \\bar{\\rho} \\right) \\right] \\frac{d\\bar{b}}{d\\bar{s}} \\] \\[ + 2\\bar{b} \\sin\\theta \\left[ 2\\bar{u} \\left( C_4 + C_5\\bar{\\rho} \\right) +  \\cos\\theta \\left(C_1 + C_3 \\bar{\\rho} \\right) \\right] \\frac{d\\bar{u}}{d\\bar{s}} \\] \\[ + \\bar{b} \\left[ 2\\bar{u}^2 \\cos\\theta \\left(C_4 + C_5 \\bar{\\rho} \\right) + 2\\bar{u} \\left(\\cos^2 \\theta - \\sin^2 \\theta \\right) \\left( C_1 - C_3 \\bar{\\rho} \\right) + \\left(1 - 3\\sin^2 \\theta \\right)\\cos\\theta \\left(2 + C_2 \\bar{\\rho} \\right) \\right] \\frac{d\\theta}{d\\bar{s}} \\] \\[ + \\bar{b} \\sin\\theta \\left[ C_2 \\cos^2 \\theta + 2C_3 \\bar{u} \\cos \\theta + 2 C_5 \\bar{u}^2 \\right] \\frac{d\\bar{\\rho}}{d\\bar{s}} = -C_2 \\bar{b} \\bar{\\rho} \\bar{g} + \\mathrm{sgn}\\theta \\cdot C_d \\sin^2 \\theta \\cos \\theta\\]\n\n\nConservation of Energy\nThe balance equation is:\n\\[ \\frac{d}{d\\bar{s}} \\left[ \\bar{b}^2 \\left( 2\\cos\\theta + C_1 \\bar{u} - \\bar{\\rho_a} \\left( \\bar{u} \\left(C_1 + C_3 \\bar{\\rho} \\right) + \\cos \\theta \\left( 2 + C_2\\bar{\\rho} \\right) \\right) \\right)\\right] = 2\\bar{b} \\left( 1 - \\bar{\\rho_a} \\right) \\bar{E}\\]\nWhere \\(\\bar{\\rho_a} = \\frac{\\rho_a}{\\rho_{a,0}}\\) is the dimensionless air density.\nThe final form is:\n\\[ 2\\left( 2\\cos\\theta + C_1 \\bar{u} - \\bar{\\rho_a} \\left( \\bar{u} \\left(C_1 + C_3 \\bar{\\rho} \\right) + \\cos \\theta \\left( 2 + C_2\\bar{\\rho} \\right) \\right)\\right) \\frac{d\\bar{b}}{d\\bar{s}} \\] \\[ + b\\left( C_1 - \\bar{\\rho_a} \\left(C_1 + C_3 \\bar{\\rho} \\right) \\right) \\frac{d\\bar{u}}{d\\bar{s}} \\] \\[ -b\\sin\\theta \\left( 2 - \\bar{\\rho_a} \\left( 2 + C_2\\bar{\\rho} \\right) \\right) \\frac{d\\theta}{d\\bar{s}} \\] \\[ - \\bar{b} \\bar{\\rho_a} \\left( C_3 \\bar{u} + C_2 \\cos \\theta \\right) \\frac{d\\bar{\\rho}}{d\\bar{s}} = 2 \\left( 1 - \\bar{\\rho_a} \\right) \\bar{E}\\]"
  },
  {
    "objectID": "posts/ooms_plume_model/index.html#implementing-the-ooms-plume-model",
    "href": "posts/ooms_plume_model/index.html#implementing-the-ooms-plume-model",
    "title": "The Ooms Plume Model",
    "section": "Implementing the Ooms Plume Model",
    "text": "Implementing the Ooms Plume Model\nImplementing this in julia is very straightforward, starting with the model constants\n\n# constants from Ooms 1972\nconst λ² = 1.35\nconst α₁ = 0.057\nconst α₂ = 0.5\nconst α₃ = 1.0\nconst ϵ  = 0.0\nconst Cd = 0.3\n\n\n# integration constants\nconst C₁ = 1-exp(-2)\nconst C₂ = λ²*(1-exp(-2/λ²))\nconst C₃ = (λ²/(λ²+1))*(1-exp(-2*(λ²+1)/λ²))\nconst C₄ = (1-exp(-4))/4\nconst C₅ = (λ²/(4λ²+2))*(1-exp(-(4λ²+2)/λ²))\n\n\n# physical constants\nconst g   = 9.80665   # standard gravity, m/s²\nconst MWₐ = 0.0289652 # molar weight dry air, kg/mol\nconst cpₐ = 1.006     # specific heat dry air, kJ/kg/K\nconst ρₐ₀ = 1.2250    # standard density dry air, kg/m³\n\nThe standard way of writing a differential algebraic equation is in the form of a mass matrix, M:\n\\[ M \\frac{d}{d\\bar{s}} \\mathrm{state} = f\\left(\\mathrm{state}, s\\right) \\]\nWhere state is the state vector for this system. In this case M is not a constant, it is a function of the state of the system as well. Below is a function that calculates the mass matrix for a given state of the system, this is done in place to reduce the number of allocations required. The state variables are all in dimensionless form – the overbars are implied.\n\nfunction ooms_matrix!(M,state,p,s)\n    # unpack variables for readability\n    c, b, u, θ, ρ, x, z = state\n\n    # calculate atmospheric conditons at centerline elevation   \n    ρₐ_bar = p.rhoa_bar(z)\n\n    # species balance\n    M[1,1] = b*( C₃*u + C₂*cos(θ) )\n    M[1,2] = 2*c*( C₃*u + C₂*cos(θ) )\n    M[1,3] = C₃*c*b\n    M[1,4] = -C₂*c*b*sin(θ)\n    M[1,5] = 0\n    M[1,6] = 0\n    M[1,7] = 0\n\n    # overall mass balance\n    M[2,1] = 0\n    M[2,2] = 2cos(θ)*(2 + C₂*ρ) + 2u*(C₁ + C₃*ρ)\n    M[2,3] = b*(C₁ + C₃*ρ)\n    M[2,4] = -b*sin(θ)*(2 + C₂*ρ)\n    M[2,5] = b*(C₂*cos(θ) + C₃*u)\n    M[2,6] = 0\n    M[2,7] = 0\n\n    # x momentum balance\n    M[3,1] = 0\n    M[3,2] = 2cos(θ)*( 2u^2*(C₄ + C₅*ρ) + 2u*cos(θ)*(C₁ + C₃*ρ) \n                       + cos(θ)^2*(2 + C₂*ρ))\n    M[3,3] = 2b*cos(θ)*( cos(θ)*(C₁ + C₃*ρ) + 2u*(C₄ + C₅*ρ) )\n    M[3,4] = -b*sin(θ)*( cos(θ)*( 4u*(C₁ + C₃*ρ) + 3cos(θ)*(2 + C₂*ρ) )\n                       + 2u^2*(C₄ + C₅*ρ) )\n    M[3,5] = b*cos(θ)*( C₂*cos(θ)^2 + 2C₃*u*cos(θ) + 2C₅*u^2 )\n    M[3,6] = 0\n    M[3,7] = 0\n\n    # z momentum balance\n    M[4,1] = 0\n    M[4,2] = 2sin(θ)*( 2u*cos(θ)*(C₁ + C₃*ρ) + cos(θ)^2*(2 + C₂*ρ) \n                      + 2u^2*(C₄ + C₅*ρ))\n    M[4,3] = 2b*sin(θ)*(cos(θ)*(C₁ + C₃*ρ) +2u*(C₄ + C₅*ρ))\n    M[4,4] = b*(2u*(cos(θ)^2 - sin(θ)^2)*(C₁ + C₃*ρ) \n                + (1-3sin(θ)^2)*cos(θ)*(2 + C₂*ρ) \n                + 2u^2*cos(θ)*(C₄ + C₅*ρ))\n    M[4,5] = b*sin(θ)*(C₂*cos(θ)^2 + 2C₃*u*cos(θ) + 2C₅*u^2)\n    M[4,6] = 0\n    M[4,7] = 0\n\n    # energy balance\n    M[5,1] = 0\n    M[5,2] = 2(2cos(θ) + C₁*u - ρₐ_bar*(u*(C₁ + C₃*ρ) + cos(θ)*(2 + C₂*ρ)) )\n    M[5,3] = b*( C₁ - ρₐ_bar*(C₁ + C₃*ρ) )\n    M[5,4] = -b*sin(θ)*( 2 - ρₐ_bar*(2 + C₂*ρ) )\n    M[5,5] = -b*ρₐ_bar*( C₂*cos(θ) + C₃*u )\n    M[5,6] = 0\n    M[5,7] = 0\n\n    # x coordinate\n    M[6,1:5] .= 0\n    M[6,6] = 1\n    M[6,7] = 0\n\n    # z coordinate\n    M[7,1:6] .= 0\n    M[7,7] = 1\nend\n\nIn dimensionless form, the only parameter of the system that is relevant to the mass matrix is \\(\\bar{\\rho_a}\\) which is a function of the (dimensionless) elevation.\nThe right-hand-side of the system of equations is below, and is also in place. In this case there are three parameters: \\(\\bar{\\rho_a}\\), \\(\\bar{g}\\) and \\(\\bar{u^{\\prime}} = \\frac{u^{\\prime}}{u_a}\\). These are all functions of elevation, the latter two because \\(u_a\\) is a function of elevation.\n\nfunction ooms_rhs!(f,state,p,s)\n    # unpack variables for readability\n    c, b, u, θ, ρ, x, z = state\n\n    # calculate atmospheric conditons at centerline elevation\n    ρₐ_bar = p.rhoa_bar(z)\n    g_bar = p.g_bar(z)\n    \n    # entrainment\n    u′ = p.uprime_bar(b, z)\n    E  = α₁*abs(u) + α₂*abs(sin(θ))*cos(θ) + α₃*u′\n    sgn = θ&lt;0 ? -1 : +1\n\n    f .= [ 0                     # species balance\n           2E                    # overall mass balance\n           2E + Cd*abs(sin(θ)^3) # x momentum balance\n           -C₂*b*ρ*g_bar + sign(θ)*Cd*sin(θ)^2*cos(θ) # z momentum balance\n           2*(1-ρₐ_bar)*E       # energy balance\n           cos(θ)                # x coordinate\n           sin(θ)]               # z coordinate\nend\n\n\n… as an ODE\nThe most direct way of implementing the model is as an ODE where\n\\[ \\frac{d}{d\\bar{s}} \\mathrm{state} = M^{-1} f \\]\nThough instead of taking the matrix inverse a linear solve is done. This is what you might call the conventional approach, or traditional approach perhaps. People who spend a lot of time with DAEs and numerical computation will tell you not to do this – it can be unstable and fail if M is singular or near-singular – but it is also throughout the literature, especially in older code. For example, this is how DEGADIS implements the right-hand-side of the ODE.\n\nusing OrdinaryDiffEq\n\n\nfunction ode_rhs!(dstate,state,p,s)\n    ooms_matrix!(p.M,state,p,s)\n    ooms_rhs!(p.f,state,p,s)\n    dstate[:] = p.M\\p.f\nend\n\nInstead of allocating (and garbage collecting) a matrix M and vector f every time the right-and-side is called, I pre-allocate them and store them with the model parameters as a kind of scratch space.\nFor a working example, suppose the vent is releasing into a neutral atmosphere with no density gradient and a windspeed at the stack height of 2m/s\n\nconst dρₐdz = 0.0\nconst uₐ₀   = 2.0     # m/s\n\nThe vent itself is basically air but hotter and thus at a lower density. The vent stack is 2m from the ground and 20cm in diameter, the vent is being ejected at 10m/s vertically. I am also ignoring the zone of flow establishment and having the plume start exactly at the vent exit.\n\nconst MWⱼ = MWₐ       # kg/m³\nconst cpⱼ = cpₐ       # kJ/kg/K\nconst ρⱼ  = ρₐ₀/2     # kg/m³\nconst D   = 0.2       # m\nconst u₀  = 10.0      # m/s\nconst h   = 2.0       # m\nconst θ₀  = π/2\nconst c₀  = ρⱼ\n\nThe system parameters are simply the scratch space for M and f, and the three dimensionless groups which are each functions of elevation. In this case I am further assuming that windspeed is uniform.\n\nparams = (M = zeros(7,7),\n          f = zeros(7),\n          rhoa_bar = (z) -&gt; 1.0 + (dρₐdz/ρₐ₀)*D*z,\n          g_bar = (z) -&gt; (g*D)/uₐ₀^2,\n          uprime_bar = (b, z) -&gt; ∛(ϵ*b*D)/uₐ₀)\n\nThe initial state, in dimensionless form, is then\n\nstate0 = [ 1.0             ,# c\n           1/(2√(2))       ,# b\n           u₀/uₐ₀          ,# u\n           θ₀              ,# θ\n           (ρⱼ - ρₐ₀)/ρₐ₀  ,# ρ\n           0.0             ,# x\n           h/D             ]# z\n\n\n\n\n\n\n\nNote\n\n\n\nThe initial value for \\(\\bar{b}_{0}\\) might seem strange and arbitrary, but this comes from matching the initial dimensions of the plume to the exit of the vent stack. Recall the plume radius is \\(\\sqrt{2}{b}\\) so, if the plume initially has a radius equal to the vent \\(\\sqrt{2}b_0 = \\frac{D}{2}\\) and \\(\\bar{b}_0 = \\frac{b}{D} = \\frac{1}{2\\sqrt{2}}\\)\n\n\nIntegrating out 100 stack diameters along the plume\n\nspan = (0.0, 100)\nprob = ODEProblem(ode_rhs!, state0, span, params)\n\n\nsol = solve(prob, Tsit5())\n\nsol.retcode\n\nReturnCode.Success = 1\n\n\n\n\n\n\n\n\n\nFigure 4: The plume height as a function of downwind distance.\n\n\n\n\n\n\n… as a DAE\nNesting the linear solve step within the right-hand-side of the ODE can be dangerous if M ever becomes singular, or close to it. It is probably safer to use a DAE solver instead.\nDAE solvers expect to be solving a differential algebraic equation of the form:\n\\[ f\\left( \\mathrm{state}^{\\prime}, \\mathrm{state}, s \\right) = 0\\]\nUsing the matrix and rhs functions defined earlier this easy enough to do, in this case the function is in-place.\n\nfunction dae_lhs!(resid,dstate,state,p,s)\n    ooms_matrix!(p.M,state,p,s)\n    ooms_rhs!(p.f,state,p,s)\n    resid[:] = p.M*dstate - p.f\nend\n\nThe DAE solver also needs an initial state for all of the derivatives, which can be calculated by solving the linear system for the derivatives given the initial conditions.\n\nM0 = zeros(7,7)\nooms_matrix!(M0,state0,params,0)\n\nf0 = zeros(7)\nooms_rhs!(f0,state0,params,0)\n\ndstate0 = M0\\f0\n\n\ndiff_vars = fill(true, 7)\ndaeprob = DAEProblem(dae_lhs!, dstate0, state0, span, params; \n                     differential_vars = diff_vars)\n\nThe DAEProblem also needs a hint as to which are differential equations, this is what is being passed by the differential_vars keyword argument. In this case they are all differential equations so I pass a vector of seven trues.\nThe DAE solver I am going to use is IDA from Sundials.\n\nusing Sundials\n\n\ndaesol = solve(daeprob, IDA())\n\ndaesol.retcode\n\nReturnCode.Success = 1\n\n\n\n\n\n\n\n\n\nFigure 5: The plume height as a function of downwind distance, solutions using the DifferentialEquations.jl solver Tsit5 and the Sundials DAE solver IDA.\n\n\n\n\nThis works as well as the lazy method, slightly slower but it has not been implemented in a particularly optimal way.\n\n\n… using ModelingToolkit\nIf you know anything about the universe of tools in julia for modelling differential algebraic equations you are probably yelling at your screen “use ModelingToolkit!”. In terms of getting a DAE from nothing to a working model it is by far the easiest way to do it. I deliberately put all of the working out in this blog post because it annoys me that it is so hard to find online and I want it to be somewhere. But if I didn’t care about that, ModelingToolkit is the obvious choice.\n\nusing ModelingToolkit, Symbolics\nusing ModelingToolkit: t_nounits as s, D_nounits as ∂\n\n# I would use D for derivative but I'm already using \n# that for jet diameter so I'm using ∂ instead\n\nFirst I define the system variables, again these are in dimensionless form.\n\nvars = @variables c(s) b(s) u(s) θ(s) ρ(s) x(s) z(s)\n\n\\[ \\begin{equation}\n\\left[\n\\begin{array}{c}\nc\\left( t \\right) \\\\\nb\\left( t \\right) \\\\\nu\\left( t \\right) \\\\\n\\theta\\left( t \\right) \\\\\n\\rho\\left( t \\right) \\\\\nx\\left( t \\right) \\\\\nz\\left( t \\right) \\\\\n\\end{array}\n\\right]\n\\end{equation}\n\\]\n\n\nIf this wasn’t in a notebook that includes other methods of solving the DAE, I would have declared the model constants using the @constants macro. It makes the formulas look nicer for one, e.g. instead of numbers like 0.86466 there would be the appropriate constant \\(C_1\\).\n\n# conservation of mass\n∫ρurdr = b^2*( (C₁ + C₃*ρ)*u + (2 + C₂*ρ)*cos(θ) )\n\nE = α₁*abs(u) + α₂*abs(sin(θ))*cos(θ) + α₃*∛(ϵ*b*D)/uₐ₀\n\neqn1 = expand_derivatives( ∂( ∫ρurdr ) ) ~ 2*b*E\n\n\\[ \\begin{equation}\n2 \\left( u\\left( t \\right) \\left( 0.86466 + 0.5568 \\rho\\left( t \\right) \\right) + \\left( 2 + 1.0431 \\rho\\left( t \\right) \\right) \\cos\\left( \\theta\\left( t \\right) \\right) \\right) \\frac{\\mathrm{d} b\\left( t \\right)}{\\mathrm{d}t} b\\left( t \\right) + \\left( b\\left( t \\right) \\right)^{2} \\left( 0.5568 u\\left( t \\right) \\frac{\\mathrm{d} \\rho\\left( t \\right)}{\\mathrm{d}t} + \\left( 0.86466 + 0.5568 \\rho\\left( t \\right) \\right) \\frac{\\mathrm{d} u\\left( t \\right)}{\\mathrm{d}t} + 1.0431 \\frac{\\mathrm{d} \\rho\\left( t \\right)}{\\mathrm{d}t} \\cos\\left( \\theta\\left( t \\right) \\right) - \\sin\\left( \\theta\\left( t \\right) \\right) \\left( 2 + 1.0431 \\rho\\left( t \\right) \\right) \\frac{\\mathrm{d} \\theta\\left( t \\right)}{\\mathrm{d}t} \\right) = 2 \\left( 0.057 \\left|u\\left( t \\right)\\right| + 0.5 \\left|\\sin\\left( \\theta\\left( t \\right) \\right)\\right| \\cos\\left( \\theta\\left( t \\right) \\right) \\right) b\\left( t \\right)\n\\end{equation}\n\\]\n\n\n\n# conservation of species\n∫curdr = c*b^2*(C₂*cos(θ) + C₃*u)\n\neqn2 = expand_derivatives( ∂( ∫curdr ) ) ~ 0\n\n\\[ \\begin{equation}\n2 \\left( 0.5568 u\\left( t \\right) + 1.0431 \\cos\\left( \\theta\\left( t \\right) \\right) \\right) c\\left( t \\right) \\frac{\\mathrm{d} b\\left( t \\right)}{\\mathrm{d}t} b\\left( t \\right) + \\left( b\\left( t \\right) \\right)^{2} \\left( 0.5568 u\\left( t \\right) + 1.0431 \\cos\\left( \\theta\\left( t \\right) \\right) \\right) \\frac{\\mathrm{d} c\\left( t \\right)}{\\mathrm{d}t} + \\left( b\\left( t \\right) \\right)^{2} \\left( 0.5568 \\frac{\\mathrm{d} u\\left( t \\right)}{\\mathrm{d}t} - 1.0431 \\sin\\left( \\theta\\left( t \\right) \\right) \\frac{\\mathrm{d} \\theta\\left( t \\right)}{\\mathrm{d}t} \\right) c\\left( t \\right) = 0\n\\end{equation}\n\\]\n\n\n\n# conservation of momentum\n# x-direction\n∫ρu²cosθrdr = b^2*cos(θ)*(2u*cos(θ)*(C₁ + C₃*ρ) + 2u^2*(C₄ + C₅*ρ) \n                          + cos(θ)^2*(2 + C₂*ρ))\n\neqn3 = expand_derivatives( ∂( ∫ρu²cosθrdr ) ) ~ \n         b*( 2E + Cd*abs(sin(θ)^3) )\n\n\\[ \\begin{equation}\n2 \\left( 2 \\left( u\\left( t \\right) \\right)^{2} \\left( 0.24542 + 0.18167 \\rho\\left( t \\right) \\right) + 2 u\\left( t \\right) \\left( 0.86466 + 0.5568 \\rho\\left( t \\right) \\right) \\cos\\left( \\theta\\left( t \\right) \\right) + \\cos^{2}\\left( \\theta\\left( t \\right) \\right) \\left( 2 + 1.0431 \\rho\\left( t \\right) \\right) \\right) \\frac{\\mathrm{d} b\\left( t \\right)}{\\mathrm{d}t} \\cos\\left( \\theta\\left( t \\right) \\right) b\\left( t \\right) - \\left( b\\left( t \\right) \\right)^{2} \\left( 2 \\left( u\\left( t \\right) \\right)^{2} \\left( 0.24542 + 0.18167 \\rho\\left( t \\right) \\right) + 2 u\\left( t \\right) \\left( 0.86466 + 0.5568 \\rho\\left( t \\right) \\right) \\cos\\left( \\theta\\left( t \\right) \\right) + \\cos^{2}\\left( \\theta\\left( t \\right) \\right) \\left( 2 + 1.0431 \\rho\\left( t \\right) \\right) \\right) \\sin\\left( \\theta\\left( t \\right) \\right) \\frac{\\mathrm{d} \\theta\\left( t \\right)}{\\mathrm{d}t} + \\left( b\\left( t \\right) \\right)^{2} \\left( 0.36335 \\left( u\\left( t \\right) \\right)^{2} \\frac{\\mathrm{d} \\rho\\left( t \\right)}{\\mathrm{d}t} + 4 u\\left( t \\right) \\left( 0.24542 + 0.18167 \\rho\\left( t \\right) \\right) \\frac{\\mathrm{d} u\\left( t \\right)}{\\mathrm{d}t} + 1.1136 u\\left( t \\right) \\frac{\\mathrm{d} \\rho\\left( t \\right)}{\\mathrm{d}t} \\cos\\left( \\theta\\left( t \\right) \\right) + 2 \\left( 0.86466 + 0.5568 \\rho\\left( t \\right) \\right) \\frac{\\mathrm{d} u\\left( t \\right)}{\\mathrm{d}t} \\cos\\left( \\theta\\left( t \\right) \\right) + 1.0431 \\cos^{2}\\left( \\theta\\left( t \\right) \\right) \\frac{\\mathrm{d} \\rho\\left( t \\right)}{\\mathrm{d}t} - 2 u\\left( t \\right) \\sin\\left( \\theta\\left( t \\right) \\right) \\left( 0.86466 + 0.5568 \\rho\\left( t \\right) \\right) \\frac{\\mathrm{d} \\theta\\left( t \\right)}{\\mathrm{d}t} - 2 \\sin\\left( \\theta\\left( t \\right) \\right) \\left( 2 + 1.0431 \\rho\\left( t \\right) \\right) \\cos\\left( \\theta\\left( t \\right) \\right) \\frac{\\mathrm{d} \\theta\\left( t \\right)}{\\mathrm{d}t} \\right) \\cos\\left( \\theta\\left( t \\right) \\right) = \\left( 0.3 \\left|\\sin^{3}\\left( \\theta\\left( t \\right) \\right)\\right| + 2 \\left( 0.057 \\left|u\\left( t \\right)\\right| + 0.5 \\left|\\sin\\left( \\theta\\left( t \\right) \\right)\\right| \\cos\\left( \\theta\\left( t \\right) \\right) \\right) \\right) b\\left( t \\right)\n\\end{equation}\n\\]\n\n\n\n# z-direction\n∫ρu²sinθrdr = b^2*sin(θ)*(2u*cos(θ)*(C₁ + C₃*ρ) + 2u^2*(C₄ + C₅*ρ) \n                          + cos(θ)^2*(2 + C₂*ρ))\n\neqn4 = expand_derivatives( ∂( ∫ρu²sinθrdr ) ) ~ \n         -C₂*b^2*ρ*(g*D/uₐ₀^2) + sign(θ)*Cd*b*sin(θ)^2*cos(θ)\n\n\\[ \\begin{equation}\n2 \\left( 2 \\left( u\\left( t \\right) \\right)^{2} \\left( 0.24542 + 0.18167 \\rho\\left( t \\right) \\right) + 2 u\\left( t \\right) \\left( 0.86466 + 0.5568 \\rho\\left( t \\right) \\right) \\cos\\left( \\theta\\left( t \\right) \\right) + \\cos^{2}\\left( \\theta\\left( t \\right) \\right) \\left( 2 + 1.0431 \\rho\\left( t \\right) \\right) \\right) \\sin\\left( \\theta\\left( t \\right) \\right) \\frac{\\mathrm{d} b\\left( t \\right)}{\\mathrm{d}t} b\\left( t \\right) + \\left( b\\left( t \\right) \\right)^{2} \\left( 2 \\left( u\\left( t \\right) \\right)^{2} \\left( 0.24542 + 0.18167 \\rho\\left( t \\right) \\right) + 2 u\\left( t \\right) \\left( 0.86466 + 0.5568 \\rho\\left( t \\right) \\right) \\cos\\left( \\theta\\left( t \\right) \\right) + \\cos^{2}\\left( \\theta\\left( t \\right) \\right) \\left( 2 + 1.0431 \\rho\\left( t \\right) \\right) \\right) \\cos\\left( \\theta\\left( t \\right) \\right) \\frac{\\mathrm{d} \\theta\\left( t \\right)}{\\mathrm{d}t} + \\left( b\\left( t \\right) \\right)^{2} \\left( 0.36335 \\left( u\\left( t \\right) \\right)^{2} \\frac{\\mathrm{d} \\rho\\left( t \\right)}{\\mathrm{d}t} + 4 u\\left( t \\right) \\left( 0.24542 + 0.18167 \\rho\\left( t \\right) \\right) \\frac{\\mathrm{d} u\\left( t \\right)}{\\mathrm{d}t} + 1.1136 u\\left( t \\right) \\frac{\\mathrm{d} \\rho\\left( t \\right)}{\\mathrm{d}t} \\cos\\left( \\theta\\left( t \\right) \\right) + 2 \\left( 0.86466 + 0.5568 \\rho\\left( t \\right) \\right) \\frac{\\mathrm{d} u\\left( t \\right)}{\\mathrm{d}t} \\cos\\left( \\theta\\left( t \\right) \\right) + 1.0431 \\cos^{2}\\left( \\theta\\left( t \\right) \\right) \\frac{\\mathrm{d} \\rho\\left( t \\right)}{\\mathrm{d}t} - 2 u\\left( t \\right) \\sin\\left( \\theta\\left( t \\right) \\right) \\left( 0.86466 + 0.5568 \\rho\\left( t \\right) \\right) \\frac{\\mathrm{d} \\theta\\left( t \\right)}{\\mathrm{d}t} - 2 \\sin\\left( \\theta\\left( t \\right) \\right) \\left( 2 + 1.0431 \\rho\\left( t \\right) \\right) \\cos\\left( \\theta\\left( t \\right) \\right) \\frac{\\mathrm{d} \\theta\\left( t \\right)}{\\mathrm{d}t} \\right) \\sin\\left( \\theta\\left( t \\right) \\right) =  - 0.51149 \\left( b\\left( t \\right) \\right)^{2} \\rho\\left( t \\right) + 0.3 \\sin^{2}\\left( \\theta\\left( t \\right) \\right) \\cos\\left( \\theta\\left( t \\right) \\right) b\\left( t \\right) sign\\left( \\theta\\left( t \\right) \\right)\n\\end{equation}\n\\]\n\n\n\n# energy balance\nρₐ_bar = 1 + dρₐdz*D*z/ρₐ₀\n\n∫ρucₚΔTrdr = b^2*(2cos(θ) + C₁*u - ρₐ_bar*( u*(C₁ + C₃*ρ) \n                  + cos(θ)*(2 + C₂*ρ) ))\n\neqn5 = expand_derivatives( ∂( ∫ρucₚΔTrdr ) ) ~ 2*b*(1 - ρₐ_bar)*E\n\n\\[ \\begin{equation}\n2 \\left( 0.86466 u\\left( t \\right) + 2 \\cos\\left( \\theta\\left( t \\right) \\right) - u\\left( t \\right) \\left( 0.86466 + 0.5568 \\rho\\left( t \\right) \\right) - \\left( 2 + 1.0431 \\rho\\left( t \\right) \\right) \\cos\\left( \\theta\\left( t \\right) \\right) \\right) \\frac{\\mathrm{d} b\\left( t \\right)}{\\mathrm{d}t} b\\left( t \\right) + \\left( b\\left( t \\right) \\right)^{2} \\left( 0.86466 \\frac{\\mathrm{d} u\\left( t \\right)}{\\mathrm{d}t} - 0.5568 u\\left( t \\right) \\frac{\\mathrm{d} \\rho\\left( t \\right)}{\\mathrm{d}t} - 2 \\sin\\left( \\theta\\left( t \\right) \\right) \\frac{\\mathrm{d} \\theta\\left( t \\right)}{\\mathrm{d}t} + \\left( -0.86466 - 0.5568 \\rho\\left( t \\right) \\right) \\frac{\\mathrm{d} u\\left( t \\right)}{\\mathrm{d}t} - 1.0431 \\frac{\\mathrm{d} \\rho\\left( t \\right)}{\\mathrm{d}t} \\cos\\left( \\theta\\left( t \\right) \\right) - \\sin\\left( \\theta\\left( t \\right) \\right) \\left( -2 - 1.0431 \\rho\\left( t \\right) \\right) \\frac{\\mathrm{d} \\theta\\left( t \\right)}{\\mathrm{d}t} \\right) = 0\n\\end{equation}\n\\]\n\n\n\n# The full system of equations\n\neqns = [ eqn1\n         eqn2\n         eqn3\n         eqn4\n         eqn5\n         ∂(x) ~ cos(θ)\n         ∂(z) ~ sin(θ) ]\n\nSymbolics.jl has done all the derivatives and set up all the equations, what remains is to build ODESystem and solve.\n\n@named sys = ODESystem(eqns, s)\nsys = structural_simplify(sys)\n\n\\[ \\begin{align}\n\\frac{\\mathrm{d} b\\left( t \\right)}{\\mathrm{d}t} &= \\mathtt{bˍt}\\left( t \\right) \\\\\n\\frac{\\mathrm{d} \\rho\\left( t \\right)}{\\mathrm{d}t} &= \\mathtt{{\\rho}ˍt}\\left( t \\right) \\\\\n\\frac{\\mathrm{d} \\theta\\left( t \\right)}{\\mathrm{d}t} &= \\mathtt{{\\theta}ˍt}\\left( t \\right) \\\\\n\\frac{\\mathrm{d} u\\left( t \\right)}{\\mathrm{d}t} &= \\mathtt{uˍt}\\left( t \\right) \\\\\n0 &= 2 \\left( 0.057 \\left|u\\left( t \\right)\\right| + 0.5 \\left|\\sin\\left( \\theta\\left( t \\right) \\right)\\right| \\cos\\left( \\theta\\left( t \\right) \\right) \\right) b\\left( t \\right) - 2 \\left( u\\left( t \\right) \\left( 0.86466 + 0.5568 \\rho\\left( t \\right) \\right) + \\left( 2 + 1.0431 \\rho\\left( t \\right) \\right) \\cos\\left( \\theta\\left( t \\right) \\right) \\right) b\\left( t \\right) \\mathtt{bˍt}\\left( t \\right) + \\left( b\\left( t \\right) \\right)^{2} \\left(  - 0.5568 u\\left( t \\right) \\mathtt{{\\rho}ˍt}\\left( t \\right) - \\left( 0.86466 + 0.5568 \\rho\\left( t \\right) \\right) \\mathtt{uˍt}\\left( t \\right) - 1.0431 \\mathtt{{\\rho}ˍt}\\left( t \\right) \\cos\\left( \\theta\\left( t \\right) \\right) + \\sin\\left( \\theta\\left( t \\right) \\right) \\left( 2 + 1.0431 \\rho\\left( t \\right) \\right) \\mathtt{{\\theta}ˍt}\\left( t \\right) \\right) \\\\\n0 &=  - 2 \\left( 0.86466 u\\left( t \\right) + 2 \\cos\\left( \\theta\\left( t \\right) \\right) - u\\left( t \\right) \\left( 0.86466 + 0.5568 \\rho\\left( t \\right) \\right) + \\left( -2 - 1.0431 \\rho\\left( t \\right) \\right) \\cos\\left( \\theta\\left( t \\right) \\right) \\right) b\\left( t \\right) \\mathtt{bˍt}\\left( t \\right) + \\left( b\\left( t \\right) \\right)^{2} \\left(  - 0.86466 \\mathtt{uˍt}\\left( t \\right) + 0.5568 u\\left( t \\right) \\mathtt{{\\rho}ˍt}\\left( t \\right) + 2 \\sin\\left( \\theta\\left( t \\right) \\right) \\mathtt{{\\theta}ˍt}\\left( t \\right) - \\left( -0.86466 - 0.5568 \\rho\\left( t \\right) \\right) \\mathtt{uˍt}\\left( t \\right) + 1.0431 \\mathtt{{\\rho}ˍt}\\left( t \\right) \\cos\\left( \\theta\\left( t \\right) \\right) + \\sin\\left( \\theta\\left( t \\right) \\right) \\left( -2 - 1.0431 \\rho\\left( t \\right) \\right) \\mathtt{{\\theta}ˍt}\\left( t \\right) \\right) \\\\\n0 &= \\left( 0.3 \\left|\\sin^{3}\\left( \\theta\\left( t \\right) \\right)\\right| + 2 \\left( 0.057 \\left|u\\left( t \\right)\\right| + 0.5 \\left|\\sin\\left( \\theta\\left( t \\right) \\right)\\right| \\cos\\left( \\theta\\left( t \\right) \\right) \\right) \\right) b\\left( t \\right) - 2 \\left( 2 \\left( u\\left( t \\right) \\right)^{2} \\left( 0.24542 + 0.18167 \\rho\\left( t \\right) \\right) + 2 u\\left( t \\right) \\left( 0.86466 + 0.5568 \\rho\\left( t \\right) \\right) \\cos\\left( \\theta\\left( t \\right) \\right) + \\cos^{2}\\left( \\theta\\left( t \\right) \\right) \\left( 2 + 1.0431 \\rho\\left( t \\right) \\right) \\right) \\cos\\left( \\theta\\left( t \\right) \\right) b\\left( t \\right) \\mathtt{bˍt}\\left( t \\right) + \\left( b\\left( t \\right) \\right)^{2} \\left( 2 \\left( u\\left( t \\right) \\right)^{2} \\left( 0.24542 + 0.18167 \\rho\\left( t \\right) \\right) + 2 u\\left( t \\right) \\left( 0.86466 + 0.5568 \\rho\\left( t \\right) \\right) \\cos\\left( \\theta\\left( t \\right) \\right) + \\cos^{2}\\left( \\theta\\left( t \\right) \\right) \\left( 2 + 1.0431 \\rho\\left( t \\right) \\right) \\right) \\sin\\left( \\theta\\left( t \\right) \\right) \\mathtt{{\\theta}ˍt}\\left( t \\right) + \\left( b\\left( t \\right) \\right)^{2} \\left(  - 0.36335 \\left( u\\left( t \\right) \\right)^{2} \\mathtt{{\\rho}ˍt}\\left( t \\right) - 4 u\\left( t \\right) \\left( 0.24542 + 0.18167 \\rho\\left( t \\right) \\right) \\mathtt{uˍt}\\left( t \\right) - 1.1136 u\\left( t \\right) \\mathtt{{\\rho}ˍt}\\left( t \\right) \\cos\\left( \\theta\\left( t \\right) \\right) - 2 \\left( 0.86466 + 0.5568 \\rho\\left( t \\right) \\right) \\cos\\left( \\theta\\left( t \\right) \\right) \\mathtt{uˍt}\\left( t \\right) - 1.0431 \\cos^{2}\\left( \\theta\\left( t \\right) \\right) \\mathtt{{\\rho}ˍt}\\left( t \\right) + 2 u\\left( t \\right) \\sin\\left( \\theta\\left( t \\right) \\right) \\left( 0.86466 + 0.5568 \\rho\\left( t \\right) \\right) \\mathtt{{\\theta}ˍt}\\left( t \\right) + 2 \\sin\\left( \\theta\\left( t \\right) \\right) \\left( 2 + 1.0431 \\rho\\left( t \\right) \\right) \\cos\\left( \\theta\\left( t \\right) \\right) \\mathtt{{\\theta}ˍt}\\left( t \\right) \\right) \\cos\\left( \\theta\\left( t \\right) \\right) \\\\\n0 &=  - 0.51149 \\left( b\\left( t \\right) \\right)^{2} \\rho\\left( t \\right) + 0.3 \\sin^{2}\\left( \\theta\\left( t \\right) \\right) \\cos\\left( \\theta\\left( t \\right) \\right) b\\left( t \\right) sign\\left( \\theta\\left( t \\right) \\right) - 2 \\left( 2 \\left( u\\left( t \\right) \\right)^{2} \\left( 0.24542 + 0.18167 \\rho\\left( t \\right) \\right) + 2 u\\left( t \\right) \\left( 0.86466 + 0.5568 \\rho\\left( t \\right) \\right) \\cos\\left( \\theta\\left( t \\right) \\right) + \\cos^{2}\\left( \\theta\\left( t \\right) \\right) \\left( 2 + 1.0431 \\rho\\left( t \\right) \\right) \\right) \\sin\\left( \\theta\\left( t \\right) \\right) b\\left( t \\right) \\mathtt{bˍt}\\left( t \\right) + \\left( b\\left( t \\right) \\right)^{2} \\left(  - 2 \\left( u\\left( t \\right) \\right)^{2} \\left( 0.24542 + 0.18167 \\rho\\left( t \\right) \\right) - 2 u\\left( t \\right) \\left( 0.86466 + 0.5568 \\rho\\left( t \\right) \\right) \\cos\\left( \\theta\\left( t \\right) \\right) + \\cos^{2}\\left( \\theta\\left( t \\right) \\right) \\left( -2 - 1.0431 \\rho\\left( t \\right) \\right) \\right) \\cos\\left( \\theta\\left( t \\right) \\right) \\mathtt{{\\theta}ˍt}\\left( t \\right) - \\left( b\\left( t \\right) \\right)^{2} \\left( 0.36335 \\left( u\\left( t \\right) \\right)^{2} \\mathtt{{\\rho}ˍt}\\left( t \\right) + 4 u\\left( t \\right) \\left( 0.24542 + 0.18167 \\rho\\left( t \\right) \\right) \\mathtt{uˍt}\\left( t \\right) + 1.1136 u\\left( t \\right) \\mathtt{{\\rho}ˍt}\\left( t \\right) \\cos\\left( \\theta\\left( t \\right) \\right) + 2 \\left( 0.86466 + 0.5568 \\rho\\left( t \\right) \\right) \\cos\\left( \\theta\\left( t \\right) \\right) \\mathtt{uˍt}\\left( t \\right) + 1.0431 \\cos^{2}\\left( \\theta\\left( t \\right) \\right) \\mathtt{{\\rho}ˍt}\\left( t \\right) - 2 u\\left( t \\right) \\sin\\left( \\theta\\left( t \\right) \\right) \\left( 0.86466 + 0.5568 \\rho\\left( t \\right) \\right) \\mathtt{{\\theta}ˍt}\\left( t \\right) - 2 \\sin\\left( \\theta\\left( t \\right) \\right) \\left( 2 + 1.0431 \\rho\\left( t \\right) \\right) \\cos\\left( \\theta\\left( t \\right) \\right) \\mathtt{{\\theta}ˍt}\\left( t \\right) \\right) \\sin\\left( \\theta\\left( t \\right) \\right) \\\\\n\\frac{\\mathrm{d} c\\left( t \\right)}{\\mathrm{d}t} &= \\mathtt{cˍt}\\left( t \\right) \\\\\n0 &=  - 2 \\left( 0.5568 u\\left( t \\right) + 1.0431 \\cos\\left( \\theta\\left( t \\right) \\right) \\right) c\\left( t \\right) b\\left( t \\right) \\mathtt{bˍt}\\left( t \\right) - \\left( b\\left( t \\right) \\right)^{2} \\left( 0.5568 u\\left( t \\right) + 1.0431 \\cos\\left( \\theta\\left( t \\right) \\right) \\right) \\mathtt{cˍt}\\left( t \\right) - \\left( b\\left( t \\right) \\right)^{2} \\left( 0.5568 \\mathtt{uˍt}\\left( t \\right) - 1.0431 \\sin\\left( \\theta\\left( t \\right) \\right) \\mathtt{{\\theta}ˍt}\\left( t \\right) \\right) c\\left( t \\right) \\\\\n\\frac{\\mathrm{d} z\\left( t \\right)}{\\mathrm{d}t} &= \\sin\\left( \\theta\\left( t \\right) \\right) \\\\\n\\frac{\\mathrm{d} x\\left( t \\right)}{\\mathrm{d}t} &= \\cos\\left( \\theta\\left( t \\right) \\right)\n\\end{align}\n\\]\n\n\nIn this case there are no model parameters as I inserted the equations for the dimensionless groups directly into the model.\n\nmtk_params = ()\n\nThe initial values simply map over the initial state I worked out previously. Because ModelingToolkit generates its own internal structure and shuffles things around, a mapping needs to be provided for the initial conditions.\n\ninitial_vals = [ c =&gt; state0[1],\n                 b =&gt; state0[2],\n                 u =&gt; state0[3],\n                 θ =&gt; state0[4],\n                 ρ =&gt; state0[5],\n                 x =&gt; state0[6],\n                 z =&gt; state0[7] ]\n\n\nmtk_prob = ODEProblem(sys, initial_vals, span)\n\n\nmtk_sol = solve(mtk_prob, Rodas5P())\n\nmtk_sol.retcode\n\nReturnCode.Success = 1\n\n\n\n\n\n\n\n\n\nFigure 6: The plume height as a function of downwind distance, solutions using the lazy approach with the Tsit5 solver and ModelingToolkit using Rodas5P.\n\n\n\n\nIn terms of julia code that needed to be written, and calculus that needed to be done, this the simplest by far. Simply compare to the enormous mass matrix expression above to convince yourself of that. There are also code generation tools that can be used if you want to extract the model either as a julia script or even C code. Furthermore, if you want to go through term by term and look at the coefficients for each derivative, Symbolics.jl can do that too. I actually used Symbolics to check all of my work in the mass matrix.\n\n\nDead Ends and Failures\nAnother approach to the ODE problem is to use a matrix operator. This is a mass matrix problem with a state dependent mass matrix, which is one of the use cases for SciMLOperators.jl\n\nusing SciMLOperators\n\n\nM = MatrixOperator(zeros(7,7); update_func! = ooms_matrix!)\n\n\nmassprob = ODEProblem(ODEFunction(ooms_rhs!, mass_matrix=M), state0, span, params)\n\n\nmass_sol = solve(massprob,Rodas5P(); initializealg=BrownFullBasicInit())\n\nmass_sol.retcode\n\n\n┌ Warning: At t=0.0, dt was forced below floating point epsilon 5.0e-324, and step error estimate = NaN. Aborting. There is either an error in your model specification or the true solution is unstable (or the true solution can not be represented in the precision of Float64).\n\n└ @ SciMLBase ~/.julia/packages/SciMLBase/rvXrA/src/integrator_interface.jl:623\n\n\n\n\nReturnCode.Unstable = 7\n\n\nI tried a bunch of different solvers and initialization algorithms, nothing could get past the first timestep. That there are two working versions of this system, in this post, using the same exact mass matrix function leads me to suspect it is not a model error, or that the solution is unstable. There is probably some aspect to how I’m supposed to be initializing this problem, or some other feature of using matrix operators, that I’m doing wrong, but I find the documentation on that to be mostly absent. I know these can work because I have used this exact method on simpler systems in the past.\nIf I ever figure out what I need to do to make this work, or more definitively why it doesn’t, I’ll come back and update this. Consider this an invitation to tell me all the ways I’m doing this wrong in the comments."
  },
  {
    "objectID": "posts/ooms_plume_model/index.html#the-problem-of-concentration",
    "href": "posts/ooms_plume_model/index.html#the-problem-of-concentration",
    "title": "The Ooms Plume Model",
    "section": "The Problem of Concentration",
    "text": "The Problem of Concentration\nThe plume solution is fundamentally in terms of the plume axis. It is not immediately obvious how to calculate the concentrations at particular points in space relative to the problem coordinate system. The way I see it, there are three related problems that involve calculating concentrations from the Ooms model.\n\nCalculating the isopleths in the x-z plane\nCalculating the isopleths at an arbitrary elevation \\(z=a\\)\nCalculating the concentration at an arbitrary point \\(x,y,z\\)\n\nThese all stem from the problem that for some arbitrary point not on the plume axis, it is not immediately clear which part of the plume axis is governing the concentration there. This is because the concentration profiles are not perpendicular to the x-axis, they are perpendicular to the s-axis and that curves through space.\n\nIsopleths in the x-z Plane\nThe easiest problem to solve is the isopleths in the plane \\(y=0\\). Suppose we want to calculate the isopleth for some concentration \\(c = c_l\\). Recalling the concentration profile:\n\\[ \\bar{c}_l = \\bar{c}_o \\exp \\left( - \\left( \\frac{r}{\\lambda b} \\right)^2 \\right) \\]\nWhere \\(\\bar{c}_o\\) is the center line concentration at that point along the plume axis. We first solve for \\(r\\), the distance from the plume axis:\n\\[ r = b \\lambda \\sqrt{ \\log \\left( \\bar{c}_o \\over \\bar{c}_l \\right) } \\]\nConverting from cylindrical coordinates to Cartesian coordinates, \\(x^{\\prime}, y^{\\prime}, z^{\\prime}\\), aligned such that \\(x^{\\prime}\\) is aligned with the plume axis, the radius is\n\\[ r^2 = \\left(y^{\\prime}\\right)^2 + \\left(z^{\\prime}\\right)^2 \\]\nSince we are confined to the plane \\(y^{\\prime} = 0\\), we find \\(z^{\\prime} = \\pm r\\). Then we rotate the axis to align with the problem coordinate system and translate the origin to the problem origin.\n\\[ x = x_o \\mp r \\sin \\theta \\] \\[ z = z_o \\pm r \\cos \\theta \\]\nWhere \\(x_o\\) and \\(z_o\\) is the location of the particular point on the plume axis we were looking at. The origin relative to the point on the plume axis, hence the subscript o. The positive r gives the upper isopleth and the negative r gives the lower isopleth.\n\n\n\n\n\n\nNote\n\n\n\nCasal7 provides an alternative form of these isopleths:\n\\[ z = z_o \\pm \\sqrt{ { {\\lambda^2 b^2} \\over { 1 + \\tan^2 \\theta }} \\log \\left(\\frac{c_o}{c_l} \\right) } \\]\nand\n\\[ {{z - z_o} \\over {x - x_o}} = -\\cot \\theta \\]\nThese are actually equivalent, using the identity \\(\\sec^2 = 1 + \\tan^2 \\theta\\) and the definition \\(\\sec \\theta = \\frac{1}{\\cos \\theta}\\), the first equation can be written as:\n\\[ z = z_o \\pm \\sqrt{ { {\\lambda^2 b^2} \\over { \\sec^2 \\theta }} \\log \\left(\\frac{c_o}{c_l} \\right) } = z_o \\pm \\sqrt{ \\lambda^2 b^2 \\cos^2 \\theta \\log \\left(\\frac{c_o}{c_l} \\right) } = z_o \\pm \\lambda b \\sqrt{ \\log \\left(\\frac{c_o}{c_l} \\right) } \\cos \\theta = z_o \\pm r \\cos \\theta\\]\nThe second equation can be re-written to solve for x:\n\\[ {{z - z_o} \\over {x - x_o}} = -\\cot \\theta \\] \\[ {z - z_o}  = -\\left(x - x_o\\right)\\cot \\theta \\] \\[ \\pm r \\cos \\theta = -\\left(x - x_o\\right)\\cot \\theta \\] \\[ \\pm r \\cos \\theta = - \\left(x - x_o\\right) \\frac{\\cos \\theta}{\\sin \\theta}\\] \\[ x = x_o \\mp r \\sin \\theta \\]\n\n\n7 Casal, “Atmospheric Dispersion of Toxic or Flammable Clouds,” 306.\nfunction upper_isopleth(solution, s, c)\n    cₒ, bₒ, uₒ, θₒ, ρₒ, xₒ, zₒ = solution(s)\n\n    if c ≈ cₒ\n        return Point(xₒ,zₒ)\n    elseif c &gt; cₒ\n        return nothing\n    else\n        r = bₒ * √(λ²*log(cₒ/c))\n        x = xₒ - r*sin(θₒ)\n        z = zₒ + r*cos(θₒ)\n        return Point(x,z)\n    end\nend\n\n\nfunction lower_isopleth(solution, s, c)\n    cₒ, bₒ, uₒ, θₒ, ρₒ, xₒ, zₒ = solution(s)\n\n    if c ≈ cₒ\n        return Point(xₒ,zₒ)\n    elseif c &gt; cₒ\n        return nothing\n    else\n        r = bₒ * √(λ²*log(cₒ/c))\n        x = xₒ + r*sin(θₒ)\n        z = zₒ - r*cos(θₒ)\n        return Point(x,z)\n    end\nend\n\nFor an example, suppose we want the isopleth for \\(c/c_0 = 2\\%\\)\n\ncₗ = 0.02 # c/c₀ = 2%\n\nFirst, I find the point along the plume axis where the concentration drops below 2%, there is no point in looking for an isopleth past this point since it doesn’t exist.\n\nusing Roots: find_zero\n\n\ni_end = findfirst(sol[1,:] .&lt; cₗ )\n\n20\n\n\n\ns_end = find_zero( (s) -&gt; sol(s, idxs = 1) - cₗ, sol.t[i_end])\n\n46.23790952011145\n\n\nThen I can calculate a series of points for the upper isopleth and the lower isopleth from the origin out to where the plume concentration has dropped below 2%.\n\nupper_points = [ upper_isopleth(sol, s, cₗ) for s in LinRange(0.0, s_end, 100) \n                 if !isnothing(upper_isopleth(sol, s, cₗ))];\nlower_points = [ lower_isopleth(sol, s, cₗ) for s in LinRange(0.0, s_end, 100) \n                 if !isnothing(lower_isopleth(sol, s, cₗ))];\n\n\n\n\n\n\n\n\nFigure 7: Plume vertical isopleths, 2%(vol)\n\n\n\n\n\n\nIsopleths at z=a\nA somewhat more difficult problem is finding the isopleths on the plane z=a. The logic is the same: for each point along the plume axis, work out the distance r to the given concentration, then solve for y given z=a.\n\nfunction cross_isopleth(solution, s, c, a)\n    cₒ, bₒ, uₒ, θₒ, ρₒ, xₒ, zₒ = solution(s)\n\n    if c ≈ cₒ\n        return Point(xₒ,0.0)\n    elseif c &gt; cₒ\n        # isopleth doesn't exist here\n        return nothing\n    else\n        # find the x coordinate\n        xₒ′ = xₒ*cos(θₒ) + zₒ*sin(θₒ)\n        x   = (xₒ′ - a*sin(θₒ))*sec(θₒ)\n        \n        # find the y coordinate\n        r²  = bₒ^2 * λ²*log(cₒ/c)\n        z′² = ((a - zₒ)*cos(θₒ) - (x - xₒ)*sin(θₒ))^2\n\n        if z′² &gt; r²\n            # the isosurface doesn't intersect z=a\n            return nothing\n        else\n            y′ = √( r² - z′²)\n            y = y′\n            return Point(x,y)\n        end\n    end\nend\n\nPicking an arbitraty height of 20 stack diameters in elevation.\n\na = 20 # 20 stack diameters\n\nWe need to find the start and end of the isopleth, which not immediately obvious like it was of the isopleths in the plane y=0. But we can re-use the vertical isopleths – the start of the isopleth is the point where the upper isopleth intersects z=a and the end is where the lower isopleth intersects it. I have used the word isopleth a lot, hopefully it makes sense and has not lost all meaning.\n\n# the start of the isopleth\ns_start = find_zero( (s) -&gt; upper_isopleth(sol, s, cₗ)[2] - a, 14)\n\n14.87383455152286\n\n\n\n# the end of the isopleth\ns_end = find_zero( (s) -&gt; lower_isopleth(sol, s, cₗ)[2] - a, 33)\n\n33.55677883331305\n\n\n\ncross_points = [ cross_isopleth(sol, s, cₗ, a) for s in LinRange(s_start+1e-3, s_end, 100) \n                 if !isnothing(cross_isopleth(sol, s, cₗ, a))]\n\n\nflipped_points = [ Point( pt[1], -1*pt[2] ) for pt in cross_points ]\n\n\n\n\n\n\n\n\nFigure 8: Plume crosswind isopleths at z/D = 20, 2%(vol)\n\n\n\n\n\n\nThe Concentration at an Arbitrary Point\nCalculating the concentration at some arbitrary point involves first backing out where along the plume axis the concentration is coming from, then calculating the concentration using the Gaussian profile.\nTo find the location on the axis that governs the concentration at the point, i.e. the location on the axis where a vector connecting it to the arbitrary point is perpendicular to the plume axis, I basically just rotate the problem coordinate system to align with the plume axis and check. Since the ODE solution includes a set of pre-calculated points, I use it to generate an initial guess of where to look and then use Newton’s method to find the exact location s.\n\nfunction find_centerline(solution,x,y,z)\n    function perp_test(s)\n        θₒ, _, xₒ, zₒ = solution(s, idxs=4:7)\n        x′ = x*cos(θₒ) + z*sin(θₒ)\n        xₒ′ = xₒ*cos(θₒ) + zₒ*sin(θₒ)\n        return x′ - xₒ′\n    end\n\n    # find initial guess\n    i0 = argmin( [ abs(perp_test(s)) for s in sol.t ] )\n    s0 = sol.t[i0]\n\n    # find the zero point\n    return find_zero(perp_test, s0)\nend\n\nThe concentration then builds on this by first finding the location along the plume axis that connects to the arbitrary point, calculating the distance r from the plume axis to the point, and finally returning the concentration.\n\nfunction concentration(solution,x,y,z)\n    # get the point on the centerline that governs this point\n    sₒ = find_centerline(solution,x,y,z)\n    cₒ, bₒ, uₒ, θₒ, ρₒ, xₒ, zₒ = sol(sₒ)\n\n    # rotate the coordinate system to the plume axis\n    y′ = y\n    z′ = (z - zₒ)*cos(θₒ) - (x - xₒ)*sin(θₒ)\n    r² = (y′)^2 + (z′)^2\n\n    # calculate concentration\n    c = cₒ*exp(-r²/(bₒ^2*λ²))\n\n    return c\nend\n\nHow do I know this is actually working? I don’t really have test data to compare against. But I do have some isopleths that I calculated independently (though using the same trig), I can check that the concentration at those points is indeed what it is supposed to be (2%).\n\nupper_concentrations = [ concentration(sol, pt[1], 0, pt[2]) for pt in upper_points ]\nlower_concentrations = [ concentration(sol, pt[1], 0, pt[2]) for pt in lower_points ]\ncross_concentrations = [ concentration(sol, pt[1], pt[2], a) for pt in cross_points ]\n\n\n\n\n\n\n\n\nFigure 9: A scatterplot showing how well the concentration function recovered the concentration at the points on the isopleths\n\n\n\n\nIndeed it does recover the concentrations as expected. There is one massive caveat though, it is assuming that there is only one location on the plume axis where a line connecting the point to the plume is perpendicular to the plume axis. If the plume is strongly curving, such as when a dense plume is emitted and bends back down to earth, this is no longer true. I think the basic assumptions of the plume itself start to break down once the plume bends back and intersects itself. I don’t think there really is a “correct” answer for how to calculate the concentration there."
  },
  {
    "objectID": "posts/ooms_plume_model/index.html#capturing-dense-gas-behaviour",
    "href": "posts/ooms_plume_model/index.html#capturing-dense-gas-behaviour",
    "title": "The Ooms Plume Model",
    "section": "Capturing Dense Gas Behaviour",
    "text": "Capturing Dense Gas Behaviour\nThe plume model only assumes that the vent gas has a similar molar weight and heat capacity to air. It is still possible to have a negatively buoyant plume, this would be equivalent to a vent of cryogenic gas. In this case the plume will crash to the ground and…continue going. There is nothing in the Ooms model that requires z to be positive. If we assume the initial condition is \\(\\bar{z}_0 = \\frac{h+\\delta}{D}\\) where h is the height of the vent stack, we can use a simple callback function to trigger once the integrator has crossed the ground plane and reflect the plume back.\n\nBouncing Plume with a Standard Callback\n\nground_check(state, s, i) = state[7] # z\n\n\nfunction reflect_plume!(integrator)\n    # bounce off the ground\n    integrator.u[4] = abs(integrator.u[4]) # θ\n    integrator.u[7] = 0                    # z\nend\n\n\nground_cb = ContinuousCallback(ground_check, reflect_plume!)\n\nThis makes the plume bounce along the ground. An alternative, and what DEGADIS does, is to terminate the integration once the plume contacts the ground and transition to another model.\n\ndense_state0 = [ 1.0             ,# c\n                 1/(2√(2))       ,# b\n                 u₀/uₐ₀          ,# u\n                 θ₀              ,# θ\n                 10.0            ,# ρ\n                 0.0             ,# x\n                 h/D             ]# z\n\n\ndense_prob =  ODEProblem(ode_rhs!, dense_state0, span.*2, params)\n\n\ndense_sol = solve(dense_prob, Tsit5(); callback=ground_cb)\n\ndense_sol.retcode\n\nReturnCode.Success = 1\n\n\n\n\nBouncing Plume with ModelingToolkit\nModelingToolkit implements callbacks a little bit differently, as symbolic equations.\n\nground  = [ z ~ 0 ]\nreflect = [ θ ~ -Pre(θ) ]\n\nWhich are then added to the ODESystem\n\n@named dense_sys = ODESystem(eqns, s; continuous_events= ground =&gt; reflect)\ndense_sys = structural_simplify(dense_sys)\n\n\ndense_vals = [ c =&gt; dense_state0[1],\n               b =&gt; dense_state0[2],\n               u =&gt; dense_state0[3],\n               θ =&gt; dense_state0[4],\n               ρ =&gt; dense_state0[5],\n               x =&gt; dense_state0[6],\n               z =&gt; dense_state0[7] ]\n\n\ndense_prob_mtk = ODEProblem(dense_sys, dense_vals, span.*2)\n\n\ndense_sol_mtk = solve(dense_prob_mtk, Rodas5P())\n\ndense_sol_mtk.retcode\n\nReturnCode.Success = 1\n\n\n\n\n\n\n\n\n\nFigure 10: The plume height as a function of downwind distance, vent gas eleven times denser than ambient air.\n\n\n\n\nI make no claims that this is a reasonable thing for the plume to do. It is mostly just for fun. If the reflect callback was changed to terminate!, then the plume would halt when the center line impacted the ground. There is also a case to be made that once the plume boundary impacts the ground, \\(z - \\sqrt{2}b \\sin \\theta = 0\\), then the integration should terminate and another model used. This is basically what DEGADIS does, once the plume is at ground level it transitions to another model for grounded plumes."
  },
  {
    "objectID": "posts/ooms_plume_model/index.html#validating-the-model",
    "href": "posts/ooms_plume_model/index.html#validating-the-model",
    "title": "The Ooms Plume Model",
    "section": "Validating the Model",
    "text": "Validating the Model\nIt is all fine and good to say “well, those look like plausible curves,” I would like to have some validation that this is actually working as intended. For some confirmation I pulled data points from figure 3 in Ooms8 using a graph digitizer. I chose that figure since covers most of the range of the z-axis. The other two figures are squashed down, making it difficult to get good resolution on the data points.\n8 Ooms, “A New Method for the Calculation of the Plume Path of Gases Emitted by a Stack,” 907.Unfortunately while Ooms provides most of the dimensionless groups needed to generate the plots, it is missing two important ones:\n\nThe initial plume dimension \\(\\bar{b}_{0}\\)\nThe length of the zone of flow establishment δ\n\nThe actual dimensions and starting location of the plume will depend on how the zone of flow establishment is calculated, and those details are missing from the paper. I assumed the initial plume dimension \\(\\bar{b}_{0} = \\frac{1}{2\\sqrt{2}}\\), which corresponds to the plume starting with an identical width to the jet. Further I just picked a flow establishment of ~6.5 diameters, which is reasonable for a free jet. This recreates the curve really well.\nPutting aside basically guessing the length of the zone of flow establishment, which feels pretty sketchy, that the curve has the correct shape and reproduces figure 3 in the paper is decent validation. Adjusting the initial height simply translates the curve up and down, it doesn’t impact the result otherwise.\n\n#              x/D     z/D\nlfn_data = [ 16.628  19.953;\n             43.054  29.86;\n             46.366  32.233;\n             61.684  36.698;\n             84.591  42.558;\n             109.085 48.977]\n\n\nlfn_prms = (M = zeros(7,7),\n            f = zeros(7),\n            rhoa_bar = (z) -&gt; 1.0,\n            g_bar = (z) -&gt; 4.278,\n            uprime_bar = (b,z) -&gt; 0.0)\n\n\n# initial values\nlfn_state0 = [ 1.0       ,# c\n               1/(2√(2)) ,# b\n               8.0       ,# u\n               π/2       ,# θ\n               -0.148    ,# ρ\n               0.0       ,# x\n               6.5       ]# z\n\n\nlfn_prob = ODEProblem(ode_rhs!, lfn_state0, (0.0,150.0), lfn_prms)\n\n\nlfn_sol = solve(lfn_prob, Tsit5())\n\n\n\n\n\n\n\n\nFigure 11: A recreation of figure 3 from Ooms.\n\n\n\n\nThis actually relates to one of the main difficulties in finding test data to compare against, to validate that my code is working. Results from the first version of DEGADIS are not directly applicable as DEGADIS initializes a jet using a different algorithm and the inputs into the Ooms model are not the jet parameters passed to DEGADIS. That’s assuming that I could even find DEGADIS results where the plume had the same molar weight and heat capacity as air, at which point the DEGADIS model reduces down to the original Ooms model. It has a different energy balance and for all other situations would be expected to generate different results.\nThat I can recreate the figures from the original paper and that the first 4 balance equations given here are equivalent to what is given in the DEGADIS documentation (once rendered dimensionless and with the corresponding constants substituted), and the 5th equation matches in the special case of the vent gas being air and the atmosphere having no density gradient (the right hand side of the equation is zero) leaves me pretty confident that my result is correct. I also have the advantage of being able to cross-check my integrals and all those derivatives using a CAS. But it would be more satisfying if I had some unambiguous test cases to reproduce."
  },
  {
    "objectID": "posts/ooms_plume_model/index.html#future-opportunities",
    "href": "posts/ooms_plume_model/index.html#future-opportunities",
    "title": "The Ooms Plume Model",
    "section": "Future Opportunities",
    "text": "Future Opportunities\nI only implemented the first version of the Ooms model. There are two subsequent papers that make modifications which may be worth implementing. The first significant modification is a more complex energy balance, which is the basis for the DEGADIS implementation of Ooms, in this case the molar weight and heat capacity of the plume are calculated from the concentration in the plume. This makes the integral vastly more complex and it might make sense to try this model out while numerically integrating the balance at each step. The second significant modification is a change to the plume shape. The Ooms model assumes the plume has a circular cross-section, which is known to be incorrect for plumes dispersing in the atmosphere. The plume can be modified to an elliptical cross section in such a way as to preserve the cross-sectional area while better matching the observed shapes of real plumes. I did not implement either of these mostly because I wanted the “minimal viable plume model” first. This can be a known-working starting point on top of which these modifications can be made.\nAnother obvious modification is to add ground-reflection. Once the plume has been solved, and there is a way to calculate concentrations at arbitrary points, it is not a huge challenge to add in ground-reflection. That is, managing the situation once the plume disperses into the ground. For conventional Gaussian plumes the typical assumption is that the plume simply reflects off and the concentration in this zone is the sum of the normal plume concentration and the concentration of reflected plume. Something similar could be done for Ooms as well."
  },
  {
    "objectID": "posts/ooms_plume_model/index.html#references",
    "href": "posts/ooms_plume_model/index.html#references",
    "title": "The Ooms Plume Model",
    "section": "References",
    "text": "References\n\n\nCasal, Joaquim. “Atmospheric Dispersion of Toxic or Flammable Clouds.” Amsterdam: Elsevier, 2018. https://app.knovel.com/hotlink/khtml/id:kt0125Q791/evaluation-effects-consequences/atmospheric-dispersion.\n\n\nHavens, Jerry, and Thomas Spicer. “A Dispersion Model for Elevated Dense Gas Jet Chemical Releases.” Office of Air Quality Planning and Standards, US EPA, 1988. https://nepis.epa.gov/Exe/ZyPURL.cgi?DocKey=2000NACQ.txt.\n\n\nKeffer, J. F., and W. D. Baines. “The Round Turbulent Jet in a Cross-Wind.” Journal of Fluid Mechanics 15, no. 4 (1963): 481–96. https://doi.org/10.1017/S0022112063000409.\n\n\nOoms, Gijsbert. “A New Method for the Calculation of the Plume Path of Gases Emitted by a Stack.” Atmospheric Environment (1967) 6, no. 12 (1972): 899–909. https://doi.org/10.1016/0004-6981(72)90098-4."
  },
  {
    "objectID": "posts/pollen_dispersion/index.html",
    "href": "posts/pollen_dispersion/index.html",
    "title": "Mapping Pollen Dispersion",
    "section": "",
    "text": "It has been a beautiful spring in Edmonton and the trees are tentatively flowering and throwing pollen to the wind. Watching the trees come back from their barren winter state left me wondering about the wind dispersal of pollen. Pollen grains must be small enough to travel quite a distance to encounter any other trees to pollinate, but they do settle out eventually. So, how far do they actually go? I’ve been wanting to play around with the mapping tools in the julia ecosystem, and this question gave me an opportunity to make some maps exploring pollen dispersal in my neighbourhood.\nI live in an older core neighbourhood in Edmonton, which has a beautiful canopy of boulevard trees. Primarily elm, but also ash, maple, and oak. The legacy of people like Gladys Reeves and the Edmonton Tree Planting Committee who, starting in 1923, defended our city green spaces and built up our urban forest. A legacy we still fight for over a century later.\nIn particular I’m going to be looking at American Elm, the most common boulevard tree in my neighbourhood, and I’ll restrict myself to just here (not the whole city)."
  },
  {
    "objectID": "posts/pollen_dispersion/index.html#building-a-model-of-pollen-dispersion",
    "href": "posts/pollen_dispersion/index.html#building-a-model-of-pollen-dispersion",
    "title": "Mapping Pollen Dispersion",
    "section": "Building a Model of Pollen Dispersion",
    "text": "Building a Model of Pollen Dispersion\nFor an initial sketch I’m going to consider a single tree as an elevated point source producing pollen at a constant rate P and with the wind carrying the pollen away with a constant wind speed u. Individual pollen grains settle out of the plume as they are carried downwind with velocity \\(W_{set}\\). The coordinate system is centred at the base of the tree with an x-axis parallel to the wind.\n\n\n\n\n\n\nFigure 1: A sketch of a single Elm tree as an elevated point source.\n\n\n\n\nA Model Elm Tree\nThere are a few things I will need to know about each elm tree in the neighbourhood:\n\nThe height at which pollen is released\nThe rate at which pollen is released\n\nNeither of these are typically measured and available in data sets for urban forests. I will need to use correlations – what foresters call allometric equations. These correlate physical parameters of trees to something easier to measure, such as the diameter at breast height or DBH.\nTo build an example elm tree, suppose it has a DBH of 88 cm\n# Model Elm tree\nDBH = 88u\"cm\" |&gt; u\"m\"\nThe crown height for an American Elm is correlated to DBH by the following equation1 for urban trees in the North climate zone\n1 McPherson, Doorn, and Peper, “Urban Tree Database and Allometric Equations”.# McPherson, van Doorn, and Peper, *Urban Tree Database*\n# Ulmus Americana, North climate zone\ncrown_height(DBH) = 0.44998 + 0.55096*DBH - 0.00666*DBH^2 + 3e-5*DBH^3\n@ucorrel crown_height u\"cm\" u\"m\"\nI am going to assume this is the height at which pollen is released, that’s not particularly accurate but it is a start. A better value would be the “centre of mass” for pollen in the crown of an Elm tree, but that isn’t readily available.\nFor the example tree, this predicts a height of 17.8 m which, just standing around and looking at the trees on my street seems plausible. The example elm tree should be taller than a 5 story building, and there is an elm tree at the end of my block that is about that in both diameter and height.\nThe total amount of pollen in a given elm tree is given by the following equation2 where B is the tree basal area. The total pollen is based on counts of pollen per anther and an estimate of the total number of anthers per tree for urban elm trees in Ann Arbor, Michigan. Maybe not perfectly comparable to Edmonton, but it’s good enough for this exploratory work.\n2 Katz, Morris, and Batterman, “Pollen Production for 13 Urban North American Tree Species”.# Ulmus Americana total pollen per tree\n# Katz, Morris, and Batterman, \"Pollen Production,\" Table 2\nfunction total_pollen(DBH)\n    B = π/4*DBH^2\n    return exp(5.86*B + 23.11)\nend\n@ucorrel total_pollen u\"m\" u\"grains\"\nThis gives a total pollen content of 384,082,918,235 grains for the model elm tree, which sounds like a lot.\nElm trees release their pollen, in Edmonton, somewhere from the end of April to mid May and it usually lasts 1-2 weeks. As a very rough model I’m going to assume each tree releases its pollen at a constant rate over a 2 week period, and that the periods over which each of the trees are releasing overlap.\nΔt = 14u\"d\" |&gt; u\"s\"\npollen_rate(DBH) = total_pollen(DBH)/Δt\nFor the example tree, this gives a pollen release rate of 317,529 grains s^-1.\n\n\nPollen Settling\nElm pollen is relatively large and will settle out of the air. To account for this I am going to assume the pollen settles with a velocity equal to the terminal velocity given by Stokes’ Law, where each individual pollen grain is a solid sphere.\n\n\n\n\n\n\nFigure 2: A single pollen grain as a solid sphere falling at terminal velocity.\n\n\n\nbegin\n\n# Ulmus Americana pollen\n# Brush and Brush, \"Transport of Pollen,\" Tables 3 and 12.\nd  = 31u\"μm\" |&gt; u\"m\"\nSG = 1.1\nρ  = SG*1000u\"kg/m^3\"\n\nend;\nbegin\n\ng  = 9.80665u\"m/s^2\" # standard gravity\nρₐ = 1.225u\"kg/m^3\"  # density of dry air (15°C, 1atm)\nμₐ = 17.89e-6u\"Pa*s\" |&gt; u\"kg/m/s\" # viscosity of dry air (15°C, 1atm)\n\nend;\n# Stokes Law\nvₜ = ((ρ - ρₐ)*g*d^2)/(18*μₐ)\nThis gives a settling velocity for a grain of American Elm pollen in air of 3.22 cm s^-1\n\n\nAtmospheric Dispersion\nWe might naively consider the pollen being launched out of the tree like little cannon balls, with a velocity in the x-direction equal to the wind speed and the velocity in the z-direction equal to the terminal velocity of pollen. Assuming a wind speed of 2 m s^-1, then a pollen grain from our example tree would travel 1107 m before hitting the ground. That’s pretty far and also kind of unrealistic. It ignores all the turbulent mixing in the air column which will both loft it to much greater heights and, at times, push it towards the ground.\nThe turbulent mixing in the air is captured using the dispersion parameters \\(\\sigma_y\\) and \\(\\sigma_z\\) which are functions of the downwind distance. This gives an average view, averaged over all of the pollen grains. In this case I will be using the Briggs’ correlations for Urban terrain.3 I am also assuming class D atmospheric stability.\n3 Briggs, “Diffusion Estimation for Small Emissions. Preliminary Report,” 38; Griffiths, “Errors in the Use of the Briggs Parameterization for Atmospheric Dispersion Coefficients”.# wind speed, assumed\nu = 2u\"m/s\"\nσ_y(x) = 0.16x/√(1+0.0004x)\n@ucorrel σ_y u\"m\" u\"m\"\nσ_z(x) = 0.14x/√(1+0.0003x)\n@ucorrel σ_z u\"m\" u\"m\"\n\n\nThe Ermak Equation\nI will be using the Ermak equation4 to model the dispersion of pollen, which results in a Gaussian-like dispersion but with the pollutant falling out and collecting on the ground. The Ermak equation is the solution to the advection diffusion equation with a constant settling velocity \\(W_{set}\\) and deposition velocity \\(W_{dep}\\)\n4 Ermak, “An Analytical Model for Air Pollutant Transport and Deposition from a Point Source”.\\[\n\\frac{\\partial c}{\\partial r} - \\frac{W_{set}}{K} \\frac{\\partial c}{\\partial z} = \\frac{\\partial^2 c}{\\partial y^2} + \\frac{\\partial^2 c}{\\partial z^2}\n\\]\nwith boundary condition at the ground\n\\[\n\\left( K \\frac{\\partial c}{\\partial z} + W_{set} c \\right)_{z=0} = W_{dep} c|_{z=0}\n\\]\nwhere K is the eddy diffusivity and r is defined as\n\\[\nr = \\frac{1}{u} \\int_0^x K dx^{\\prime}\n\\]\nand the other boundary conditions are as for the conventional Gaussian dispersion (e.g. constant mass emissions, m, at a point h above the origin, etc.). This can be solved and put in terms of \\(\\sigma_y\\) and \\(\\sigma_z\\) as, by definition, \\(\\sigma^2 = 2 r\\)\n\\[\nc = \\frac{m}{2\\pi u \\sigma_y \\sigma_z} \\exp\\left( - \\frac{y^2}{2 \\sigma_y^2} \\right) \\exp\\left( { - {W_{set} (z-h)} \\over {2K_z} } - { {W_{set}^2 \\sigma_z^2} \\over {8K_z^2} } \\right)\n\\]\n\\[\n\\times \\left( \\exp \\left( - \\left(z-h\\right)^2 \\over {2\\sigma_z^2} \\right) + \\exp \\left( - \\left(z+h\\right)^2 \\over {2\\sigma_z^2} \\right) \\right.\n\\]\n\\[\n\\left. - { {\\sqrt{2\\pi} W_o \\sigma_z} \\over K_z} \\exp\\left( { {W_o (z+h)} \\over {K_z} } + { {W_o^2 \\sigma_z^2} \\over {2K_z^2} } \\right) \\mathrm{erfc} \\left( { {W_o \\sigma_z} \\over {\\sqrt{2}K_z} } + { {z + h} \\over {\\sqrt{2}\\sigma_z} } \\right) \\right)\n\\]\nwhere \\(W_o = W_{dep} - \\frac{1}{2}W_{set}\\). In practice, relationships for \\(\\sigma\\)s are much easier to find than Ks and the following is used to recover \\(K_z\\)\n\\[\nK_z = \\frac{1}{2} u \\frac{d \\sigma_z^2}{dx}\n\\]\nThis follows from the definition of \\(\\sigma_z\\) (and r). In this case I am going to generate the \\(K_z\\) using automatic differentiation with ForwardDiff.jl.\nusing ForwardDiff: derivative\n∂ₓσ_z²(x) = 2*σ_z(x)*derivative(σ_z, x)\n@ucorrel ∂ₓσ_z² u\"m\" u\"m\"\nK_z(x; u) = (1/2)*u*∂ₓσ_z²(x)\nModels like this, with a point source emitting mass, have nonphysical results in the vicinity of the emission source. The concentration rises sharply and there is a singularity at the source itself. There are many ways of dealing with this, but the easiest is to define a maximum concentration, usually given from a mass balance, and cut off the dispersion model at that. I don’t have any specific upper bound, so I picked a large number simply to prevent the propagation of Inf or other errors.\nThis is only a problem very close to the source, and I am more interested in concentrations far from the tree, so this is not a concern. A better model would calculate a “virtual origin” for the tree such that the pollen concentration in the crown of the tree was more realistic.\nmax_pollen = 1e6grains/1u\"m^3\"\nusing SpecialFunctions: erfc\nfunction ermak(x, y, z; u=u, h=crown_height(DBH), P=pollen_rate(DBH), \n                        W_set=vₜ, W_dep=vₜ, p_max=max_pollen)\n\n    if x&lt;zero(x) || z&lt;zero(z)\n        return zero(p_max)\n    end\n    \n    s_y = σ_y(x)\n    s_z = σ_z(x)\n    K = K_z(x; u)\n\n    Wₒ = W_dep - 0.5*W_set\n\n    p = (P/(2π*u*s_y*s_z))*exp(-0.5*(y/s_y)^2)*\n        exp(-0.5*W_set*(z-h)/K - 0.125*(W_set*s_z/K)^2)*(\n        exp(-0.5*((z-h)/s_z)^2) + exp(-0.5*((z+h)/s_z)^2)\n        - (√(2π)*Wₒ*s_z/K)*exp(Wₒ*(z+h)/K + 0.5*(Wₒ*s_z/K)^2)*\n            erfc((Wₒ*s_z/K + (z+h)/s_z)/√(2)) )\n\n    return isnan(p) ? zero(p_max) : min(p, p_max)\nend;\nUsing this model, the ground level pollen concentration 100 m downwind of the example tree is 105.82 grains m^-3. As shown in the figures below, the pollen is most concentrated in an area from about 75 m to 300 m downwind of the tree. Which is about 2.5 blocks going east-west (city blocks in Edmonton are longer in the north-south direction)\n\n\n\n\n\n\nFigure 3: Plan view of ground level pollen concentration downwind of the model Elm tree.\n\n\n\n\n\n\n\n\n\nFigure 4: Elevation view of pollen concentration downwind of the model Elm tree, through the centre of the plume (y=0).\n\n\n\nI am left with some questions about how much pollen is actually needed, in the air, for pollination to have a chance. The pollen has to end up on a corresponding flower, so there must be a point where the concentration is just too low to make this likely. Trees do put some effort into improving the odds, they typically flower and disperse pollen before their leaves have meaningfully come back, helping to remove obstructions. The branching structures of trees are both useful for light gathering and provide a large effective area over which their flowers sieve the air for pollen.\nOn the other side, pollen grains are somewhat fragile too, they can dry out or be damaged by excessive UV exposure. While a single pollen grain may have the potential to make it thousands of meters away from the tree, it may not be viable by the time it gets there.\nI would guess, from these calculations, that Elm trees are getting most of their action within 300 m or less. Anything beyond that and the pollen is so dispersed that the odds of it finding a pistil are too low.\n\n\nA Tree Data Structure\nTo move from modelling a single tree to an urban forest, I will need a data structure to contain the relevant parameters of a tree. In this case I need both the map location and the location of the tree relative to the origin of the local coordinate system, \\(x_o, y_o, z_o\\). Each tree also has a diameter, height, pollen release rate, and terminal velocity. In this case all the trees are Elm trees, and have the same pollen, but I’m leaving it general in case I want to model something else in the future.\nbegin\n\nstruct Tree{G,L,P,V}\n    geopt::G\n    xₒ::L\n    yₒ::L\n    zₒ::L\n    DBH::L\n    h::L\n    P::P\n    vₜ::V\nend\n\nfunction Tree(geopt, xₒ, yₒ, DBH; v=vₜ)\n    h = crown_height(DBH)\n    P = pollen_rate(DBH) \n    xₒ, yₒ, zₒ, DBH, h = promote(xₒ, yₒ, zero(yₒ), DBH, h)\n    return Tree(geopt, xₒ, yₒ, zₒ, DBH, h, P, v)\nend\n\nend;\nelm = Tree(nothing, 0u\"m\",0u\"m\",DBH)\nTree{Nothing, Quantity{Float64, 𝐋, Unitful.FreeUnits{(m,), 𝐋, nothing}}, Quantity{Float64, 𝐍 𝐓^-1, Unitful.FreeUnits{(grains, s^-1), 𝐍 𝐓^-1, nothing}}, Quantity{Float64, 𝐋 𝐓^-1, Unitful.FreeUnits{(m, s^-1), 𝐋 𝐓^-1, nothing}}}(\n    geopt = nothing\n    xₒ = 0.0 m\n    yₒ = 0.0 m\n    zₒ = 0.0 m\n    DBH = 0.88 m\n    h = 17.803579999999993 m\n    P = 317528.8675882605 grains s^-1\n    vₜ = 0.032156589905762846 m s^-1\n)\nI then added a method to ermak that takes a Tree object and returns the concentration of its pollen at a point x, y, and z on the local coordinate system.\nfunction ermak(t::Tree, x, y, z; u=u)\n    x′ = x - t.xₒ\n    y′ = y - t.yₒ\n    z′ = z - t.zₒ\n    return ermak(x′, y′, z′; h=t.h, P=t.P, W_set=t.vₜ, W_dep=t.vₜ)\nend;"
  },
  {
    "objectID": "posts/pollen_dispersion/index.html#mapping-elm-pollen-in-wîhkwêntôwin",
    "href": "posts/pollen_dispersion/index.html#mapping-elm-pollen-in-wîhkwêntôwin",
    "title": "Mapping Pollen Dispersion",
    "section": "Mapping Elm Pollen in Wîhkwêntôwin",
    "text": "Mapping Elm Pollen in Wîhkwêntôwin\nThe Ermak equation assumes the local area is a flat Euclidean plane. The earth is not that, and so a central task is going to be defining a local coordinate system that approximates my neighbourhood, Wîhkwêntôwin, as a flat plane. Then I will need to find all of the local trees and place them in this local coordinate system before adding in their individual contributions to the local Elm pollen situation.\n\nDefining the Local Grid\nI arbitrarily picked a point more-or-less in the middle of the neighbourhood to act as the origin. My neighbourhood is pretty flat and so I’m going to assume everything is at the same altitude.\nusing Geodesy\nbegin\n\nlatₒ, lonₒ, altₒ = 53.54100, -113.52141, 671\nΔlat, Δlon = 0.015, 0.035\n\nend;\nI oriented the grid such that the wind goes from west to east – which is usually the case. Another approach would be to look up the local windrose and orient the grid to the most frequent wind direction with the wind speed as the median wind speed.\nI am assuming that the area is locally flat relative to the curvature of the earth. Namely that the distance, in meters, per degree longitude is a constant across the whole neighbourhood – which I calculate from a straight line running through the origin going from the furthest west to the furthest east. Similarly for degrees latitude. This isn’t strictly true but the difference between the distance along the ellipsoid and the locally-flat distance is going to be trivially small, so I can safely ignore it.\nbegin\n\nΔx = euclidean_distance(LLA(latₒ, lonₒ - Δlon/2, altₒ), \n                        LLA(latₒ, lonₒ + Δlon/2, altₒ), wgs84)/Δlon\nΔy = euclidean_distance(LLA(latₒ + Δlat/2, lonₒ, altₒ), \n                        LLA(latₒ - Δlat/2, lonₒ, altₒ), wgs84)/Δlat\nend\nfunction local_coords(lat,lon)\n    x = (lon - lonₒ)*Δx\n    y = (lat - latₒ)*Δy\n    return x, y\nend\n\n\n\n\n\n\nWhy not use Web Mercator?\n\n\n\nAt first glance it looks like I’m doing a lot of additional work for no reason. I ultimately want to overlay my maps on top of satellite imagery, which will require me to convert everything into Web Mercator. Why not use that as the local coordinate system? Points in Web Mercator are northing and easting in meters on a flat plane.\nUnlike UTM, where that kind of thing works out well enough for a lot of situations, there is a lot more distortion with Web Mercator. Especially closer to the poles. I’m not particularly close to the north pole, but more than close enough that the map distortion leads to significant errors when using Web Mercator naively like that.\nTo demonstrate this I’m going to calculate the distance between my favourite coffee shop, stopgap, and a local park on the other side of the neighbourhood, Oliver park.\nbegin\n    \nstopgap = LLA(53.535618490862944, -113.5118491580413)\noliver_park = LLA(53.54542679826651, -113.52603529325418)\n\nend\ndist = euclidean_distance(stopgap, oliver_park, wgs84)\nFirst I calculate the distance along the ellipsoid, which is 1441 m (the same as what Google maps tells me).\nThen I convert the coordinates to Web Mercator, which are northing and easting relative to the equator and the prime meridian.\nWM = WebMercatorfromLLA(wgs84)\nbegin\n\nstopgap_wm = WM(stopgap)\noliver_park_wm = WM(oliver_park)\n\nend\nwm_dist = √( (stopgap_wm[1] - oliver_park_wm[1])^2 \n            + (stopgap_wm[2] - oliver_park_wm[2])^2 )\nThe naive Euclidean distance using Web Mercator is 2423 m, about 68% greater than the true distance. If I set my local grid naively using the northing and easting of Web Mercator, everything would be distorted.\n\n\n\n\nFinding the Neighbourhood Elm Trees\nThankfully, I don’t need to wander the neighbourhood with a GPS unit and a tape measure to find all the local Elm trees and map them. The City of Edmonton has already done that. I filtered the data set to just my neighbourhood and just Ulmus Americana and downloaded it as a csv.\nusing CSV, DataFrames\ntrees_df = CSV.read(\"data/pollen_dispersion/Ulmus_americana_wihkwentowin.csv\", \n                     DataFrame);\ndescribe(trees_df, :min, :max)\n19×3 DataFrame\n Row │ variable                min                                max                               \n     │ Symbol                  Any                                Any                               \n─────┼──────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ ID                      155206                             619701\n   2 │ NEIGHBOURHOOD_NAME      WÎHKWÊNTÔWIN                       WÎHKWÊNTÔWIN\n   3 │ LOCATION_TYPE           Alley                              Park\n   4 │ SPECIES_BOTANICAL       Ulmus americana                    Ulmus americana Patmore\n   5 │ SPECIES_COMMON          Elm, American                      Elm, American\n   6 │ GENUS                   Ulmus                              Ulmus\n   7 │ SPECIES                 americana                          americana\n   8 │ CULTIVAR                Brandon                            Patmore\n   9 │ DIAMETER_BREAST_HEIGHT  5                                  110\n  10 │ CONDITION_PERCENT       0                                  65\n  11 │ PLANTED_DATE            1990/01/01                         2024/09/25\n  12 │ OWNER                   Parks                              Parks\n  13 │ Bears Edible Fruit      false                              false\n  14 │ Type of Edible Fruit                                                                         \n  15 │ COUNT                   1                                  1\n  16 │ LATITUDE                53.5346                            53.5496\n  17 │ LONGITUDE               -113.536                           -113.51\n  18 │ LOCATION                (53.534594143551985, -113.510361…  (53.549586375568154, -113.530880…\n  19 │ Point Location          POINT (-113.50950085882668 53.53…  POINT (-113.53589639202552 53.54…\nWhat I would like is a vector of Trees. I could have added a column to the dataframe with Tree objects when it was created, but I’m not using the dataframe for anything else so I didn’t really see the point.\nbegin\ntrees = Vector{Tree}()\n\nfor row in eachrow(trees_df)\n    lat, lon = row.LATITUDE, row.LONGITUDE\n    DBH = row.DIAMETER_BREAST_HEIGHT*1u\"cm\" |&gt; u\"m\"\n    pt = LLA(lat,lon,altₒ)\n    x, y = local_coords(lat, lon).*1u\"m\"\n    tree = Tree(pt,x,y,DBH)\n    push!(trees, tree)\nend\n\nend\nThere are 996 Elm trees in Wîhkwêntôwin alone. That’s impressive, we have a pretty great urban forest.\ntrees[1]\nTree{LLA{Float64}, Quantity{Float64, 𝐋, Unitful.FreeUnits{(m,), 𝐋, nothing}}, Quantity{Float64, 𝐍 𝐓^-1, Unitful.FreeUnits{(grains, s^-1), 𝐍 𝐓^-1, nothing}}, Quantity{Float64, 𝐋 𝐓^-1, Unitful.FreeUnits{(m, s^-1), 𝐋 𝐓^-1, nothing}}}(\n    geopt = LLA(lat=53.53695945675063°, lon=-113.51201340003675°, alt=671.0)\n    xₒ = 623.0131311454173 m\n    yₒ = -449.74535157065054 m\n    zₒ = 0.0 m\n    DBH = 0.2 m\n    h = 9.04518 m\n    P = 10810.758180096327 grains s^-1\n    vₜ = 0.032156589905762846 m s^-1\n)\nI am assuming that pollen is additive and doesn’t alter the properties of air at all. The concentration of pollen from multiple trees is just the concentration of pollen from each tree added together.\nermak(trees::Vector{Tree}, x, y, z; u=u) = \n    sum( ermak.(trees, x, y, z; u=u) );\n\n\nMapping Wîhkwêntôwin\nNow that I have a set of trees and a bounding box, I need to generate some actual maps. I am going to use Tyler.jl to download the map tiles and make them plot-able in Makie. For which I need to give it a bounding box for the neighbourhood and identify a map provider. I am using the imagery from ESRI.\nusing Tyler\nwihkwentowin = Rect2f(lonₒ - Δlon/2, latₒ - Δlat/2, Δlon, Δlat);\nprovider = Tyler.TileProviders.Esri(:WorldImagery);\nI have defined a helper function to take a tree and return the appropriate Web Mercator coordinates to map on top of the ESRI imagery.\nfunction map_tree(tree::Tree)\n    x, y, _ = WM(tree.geopt)\n    return Point2f(x,y)\nend\nMapping all of the trees in the data set matches what I expected: they are mostly boulevard trees and the northwest corner of the neighbourhood is much more densely forested with Elm.\n\n\n\n\n\n\nFigure 5: Satellite view of Wîhkwêntôwin and surrounding area with neighbourhood Elm trees indicated with blue circles.\n\n\n\n\n\nMapping the Pollen from all Elm Trees\nNow I have all the tools in place to generate concentration contours for Elm pollen and plot them on top of the ESRI imagery for my neighbourhood. First, I create a helper function to convert grid points in Web Mercator to local grid coordinates, then return the concentration at that point with contributions from all 996 Elm trees.\nIf I was doing this for the whole city I might want to first filter out all the Elm trees that are too distant from or downwind of the point of interest – since they won’t contribute anything.\nLLA_WM = LLAfromWebMercator(wgs84)\nfunction map_ermak(x, y)\n    lla = LLA_WM([x,y,altₒ])\n    local_x, local_y = local_coords(lla.lat, lla.lon).*1u\"m\"\n    return ustrip(ermak(trees, local_x, local_y, 0u\"m\"))\nend\nI then divide the neighbourhood into a grid of 10,000 points and calculate the concentration at each point.\n# defining the bounds of the grid\n\nbegin\n\nxₗ, yₗ, _ = WM(LLA(latₒ - Δlat/2, lonₒ - Δlon/2, altₒ))\nxᵤ, yᵤ, _ = WM(LLA(latₒ + Δlat/2, lonₒ + Δlon/2, altₒ))\n\nend;\nbegin\n\nxs = range(xₗ, xᵤ; length=100)\nys = range(yₗ, yᵤ; length=100)\n\nzs = map_ermak.(xs, ys')\n    \nend;\nFinally I overlay a contour plot on top of the ESRI imagery, showing everywhere with a pollen concentration &gt;10 grains m^-3\n\n\n\n\n\n\nFigure 6: Satellite view of Wîhkwêntôwin and surrounding area with pollen concentrations &gt;10 grains m^-3 overlaid.\n\n\n\nA major limitation to this style of dispersion modelling, especially in a neighbourhood like mine dominated by large apartment buildings, is that building downwash effects are not being accounted for. The Elm trees are at a similar height or shorter than the buildings around them. This model essentially ignores the buildings other than their contribution to surface roughness – reflected in the dispersion parameters \\(\\sigma_y\\) and \\(\\sigma_z\\). Short of doing a CFD model of the neighbourhood, I don’t think there is an easy way around that. Probably this would work better in neighbourhoods like Highlands or Ritchie which have mature Elm trees but where housing is mostly older homes, less than 2 stories, with yards spacing them out from each other.\nA limitation to this specific example is that I haven’t included all the Elm trees in adjacent neighbourhoods – Westmount in particular. This under counts the Elm pollen on the west side of Wîhkwêntôwin. I can imagine one producing maps like this, for the whole city, based on which trees are producing pollen in any given week showing where the peak pollen action is. A where not to park your car map, if you want to avoid washing your windshield every morning, or where to avoid if you are allergic to tree pollen."
  },
  {
    "objectID": "posts/pollen_dispersion/index.html#references",
    "href": "posts/pollen_dispersion/index.html#references",
    "title": "Mapping Pollen Dispersion",
    "section": "References",
    "text": "References\n\n\nBriggs, Gary A. “Diffusion Estimation for Small Emissions. Preliminary Report.” Oak Ridge, TN: Air Resources Atmospheric Turbulence; Diffusion Laboratory, National Oceanic; Atmospheric Administration, 1973. https://doi.org/10.2172/5118833.\n\n\nBrush, Grace S., and Lucien M. Brush Jr. “Transport of Pollen in a Sendiment-Laden Channel: A Laboratory Study.” American Journal of Science 272, no. 4 (1972): 359–81.\n\n\nErmak, Donald L. “An Analytical Model for Air Pollutant Transport and Deposition from a Point Source.” Atmospheric Environment 11 (1977): 231–37. https://doi.org/10.1016/0004-6981(77)90140-8.\n\n\nGriffiths, R. F. “Errors in the Use of the Briggs Parameterization for Atmospheric Dispersion Coefficients.” Atmospheric Environment 28, no. 17 (1994): 2861–65. https://doi.org/10.1016/1352-2310(94)90086-8.\n\n\nKatz, Daniel S. W., Jonathan R. Morris, and Stuart A. Batterman. “Pollen Production for 13 Urban North American Tree Species: Allometric Equations for Tree Trunk Diameter and Crown Area.” Astrobiologia (Bologna) 36, no. 3 (2020). https://doi.org/10.1007/s10453-020-09638-8.\n\n\nMcPherson, E. Gregory, Natalie S. van Doorn, and Paula J. Peper. “Urban Tree Database and Allometric Equations.” Albany, CA: U. S. Department of Agriculture, Forest Service, Pacific Southwest Research Station, 2016. https://doi.org/10.2737/PSW-GTR-253."
  },
  {
    "objectID": "posts/integrated_puff/index.html",
    "href": "posts/integrated_puff/index.html",
    "title": "Between a puff and a plume",
    "section": "",
    "text": "In previous examples I used both Gaussian plume and puff models for continuous and instantaneous releases, respectively, but what about the in-between cases? It is more commonly the case for a leak from a process vessel to be a prolonged, but finitely long, release.\nThe guidance is to typically pick one or the other, depending upon the length of the release, or use a more complex model, e.g. the guidance in Lees1 is\nWhere u is the wind speed, Δt the duration of the release, and \\(\\sigma_x\\) is the downwind dispersion evaluated at \\(x = \\frac{u \\Delta t}{2}\\).2\nAn alternative approach is to evaluate the downwind dispersion at a particular point of interest x1 and use the same criteria. This is much less strict than what Lees gives and is what I will do, it motivates investigating anything other than pure plume models.\nOne approach in this in-between zone is to try both and pick the most conservative. But that can lead to extremely conservative results. An alternative might be to take a page from more complex models, such as INTPUFF and SCIPUFF, and treat an intermediate release as a series of smaller puff releases."
  },
  {
    "objectID": "posts/integrated_puff/index.html#motivating-example",
    "href": "posts/integrated_puff/index.html#motivating-example",
    "title": "Between a puff and a plume",
    "section": "Motivating example",
    "text": "Motivating example\nSuppose a release from a process vessel, say a jet of gas issuing from a hole, we suppose the release is a constant rate of 1kg/s for 5s just for some nice round numbers. The release is at ground level and the ambient conditions are class D with a 2m/s windspeed. We have a point of interest 100m down-wind of the release, this could be an inhabited building or the fence-line.3\n3 We are also implicitly assuming the release is neutrally buoyant, and so a Gaussian dispersion model would be appropriate.\nm  = 1 #kg/s\nΔt = 5 #s\nh  = 0 #m\nu  = 2 #m/s\n\nx₁ = 100  #m\nt₁ = x₁/u #s\n\n50.0\n\n\nThe class D dispersion parameters4 are:\n4 AIChE/CCPS, Guidelines for Consequence Analysis of Chemical Releases, 90.\n# class D puff dispersion\n\nσx(x) = 0.06*x^0.92\nσy(x) = σx(x)\nσz(x) = 0.15*x^0.70\n\nσz (generic function with 1 method)\n\n\nThe release, at the point of interest, meets neither the criteria for a puff model nor a plume model.\n\nu*Δt &lt; 2*σx(x₁)\n\nfalse\n\n\n\nu*Δt &gt; 5*σx(x₁)\n\nfalse\n\n\nSo some other kind of model must be used."
  },
  {
    "objectID": "posts/integrated_puff/index.html#single-gaussian-puff",
    "href": "posts/integrated_puff/index.html#single-gaussian-puff",
    "title": "Between a puff and a plume",
    "section": "Single Gaussian puff",
    "text": "Single Gaussian puff\nRecall that a single Gaussian puff is the product of 3 Gaussian distributions\n\\[ c \\left(x,y,z,t \\right) = m \\Delta t \\cdot g_x(x, t) \\cdot g_y(y) \\cdot g_z(z) \\]\nwith\n\\[ g_x(x,t) = {1 \\over \\sqrt{2\\pi} \\sigma_x } \\exp \\left( -\\frac{1}{2} \\left( x-u t \\over \\sigma_x \\right)^2 \\right) \\]\n\\[ g_y(y) = {1 \\over \\sqrt{2\\pi} \\sigma_y } \\exp \\left( -\\frac{1}{2} \\left( y \\over \\sigma_y \\right)^2 \\right) \\]\n\\[ g_z(z) = {1 \\over \\sqrt{2\\pi} \\sigma_z } \\left[ \\exp \\left( -\\frac{1}{2} \\left( z-h \\over \\sigma_z \\right)^2 \\right) + \\exp \\left( -\\frac{1}{2} \\left( z+h \\over \\sigma_z \\right)^2 \\right) \\right]\\]\nwhere, for the sake of clarity, I’ve neglected the fact that the dispersion parameters σ are themselves all functions of t by being functions of the location of the center of the puff.\n\ngx(x, t) = exp((-1/2)*((x-u*t)/σx(u*t))^2)/(√(2π)*σx(u*t))\ngy(y, t) = exp((-1/2)*(y/σy(u*t))^2)/(√(2π)*σy(u*t))\ngz(z, t) = (exp((-1/2)*((z-h)/σz(u*t))^2)+exp((-1/2)*((z+h)/σz(u*t))^2))/(√(2π)*σz(u*t))\n\n\nc_pf(x,y,z,t; m, Δt) = m*Δt*gx(x,t)*gy(y,t)*gz(z,t)\n\nFor some context we can plot the puff as a single, instantaneous, release\n\n\n\n\n\n\n\nFigure 1: The dispersion of a single, instantaneous, puff."
  },
  {
    "objectID": "posts/integrated_puff/index.html#multiple-puffs",
    "href": "posts/integrated_puff/index.html#multiple-puffs",
    "title": "Between a puff and a plume",
    "section": "Multiple puffs",
    "text": "Multiple puffs\nOur first approximation to a release of appreciable duration is to break the release up into n intervals and release a single puff per interval.\n\\[ c(x,y,z,t) = \\sum_{i=0}^{n}  m \\frac{\\Delta t}{n} \\cdot g_x(x, t-\\frac{i}{n}\\Delta t) \\cdot g_y(y) \\cdot g_z(z) \\]\nwhere we have simply taken the sum of n single puffs, each representing a fraction of the overall release, and emitting them one after the other.\n\nfunction sum_of_puffs(x,y,z,t; m, Δt, n)\n    c = 0\n    δt = Δt/n\n    for i in 0:1:n\n        t′ = t-i*δt\n        c′  = t′&gt;0 ? c_pf(x,y,z,t′; m=m, Δt=δt) : 0\n        c += isnan(c′) ? 0 : c′ \n    end\n    return c\nend\n\nWe can plot this for n=5 and we see that, while initially the individual puffs are distinct, they quickly merge into a larger more spread-out cloud.5\n5 To an extent this is a function of using a class D atmosphere. For a much more stable atmosphere, e.g. class F, the puffs remain quite distinct until a size-able number of them have been released.\n\n\n\n\n\n\nFigure 2: The dispersion of a sequence of multiple smaller puffs.\n\n\n\n\nIf we plot the max-concentration experienced at our point of interest, x=100m, against an increasingly finely-divided release (more puffs, but each puff represents a smaller slice of time) it is clear that they are converging towards a number, and relatively quickly.\n\n\n\n\n\n\n\n\nFigure 3: The effect of increasing the number of puffs\n\n\n\n\n\nThis suggests a next step, taking the limit as \\(n \\to \\infty\\)"
  },
  {
    "objectID": "posts/integrated_puff/index.html#integrated-puffs",
    "href": "posts/integrated_puff/index.html#integrated-puffs",
    "title": "Between a puff and a plume",
    "section": "Integrated puffs",
    "text": "Integrated puffs\nReturning to our model of multiple puffs\n\\[ c(x,y,z,t) = \\sum_{i=0}^{n}  m \\frac{\\Delta t}{n} \\cdot g_x(x, t-\\frac{i}{n}\\Delta t) \\cdot g_y(y) \\cdot g_z(z) \\]\nWe can re-arrange this and take the limit as \\(n \\to \\infty\\)\n\\[ c(x,y,z,t) = m\\cdot g_y(y) \\cdot g_z(z) \\cdot \\left( \\lim_{n \\to \\infty} \\sum_{i=0}^{n} g_x(x, t-\\frac{i}{n}\\Delta t) \\frac{\\Delta t}{n} \\right) \\]\n\\[ = m\\cdot g_y(y) \\cdot g_z(z) \\cdot \\int_{t-\\Delta t}^{t} g_x(x, t^{\\prime}) dt^{\\prime}\\]\nWhere we have replaced the limit with the integral.6\n6 I am assuming the dispersion parameters are constants, though they are not in practice as they are correlated to the downwind distance to the center of any given puff. I am assuming for a small enough release this is approximately constant at least.This is an integral of a Gaussian, and so we expect the results to be in terms of the error function\n\\[ \\mathrm{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_0^x \\exp \\left( -t^2 \\right) dt \\]\nFor the integral of the x component of the Gaussian puff we have\n\\[ \\int_{t-\\Delta t}^{t} g_x(x, t^{\\prime}) dt^{\\prime} = \\int_{t-\\Delta t}^{t} {1 \\over \\sqrt{2\\pi} \\sigma_x } \\exp \\left( -\\frac{1}{2} \\left( x-u t^{\\prime} \\over \\sigma_x \\right)^2 \\right) dt^{\\prime}\\]\nmaking the substitution \\[\\xi = { {x - u t^{\\prime} } \\over \\sqrt{2} \\sigma_x} \\]\nwe get7\n7 This model and the sum of puffs model both naively include contributions from releases that haven’t happened yet, e.g. at t=1 only the contribution of material released at times t≤1 should be included, but without any correction the other parts of the release would be included causing slight errors in the vicinity of the release point at t&lt;Δt. The solution is simply to take the duration of the release to be the minimum of either the elapsed time (i.e. when the release is still “happening”) or the total release duration.\\[ \\int_{t-\\Delta t}^{t} g_x(x, t^{\\prime}) dt^{\\prime} = {-1 \\over \\sqrt{\\pi} u} \\int_{a}^{b} \\exp \\left( -\\xi^2 \\right) d\\xi \\]\n\\[ = {-1 \\over \\sqrt{\\pi} u} \\left[ \\frac{\\sqrt{\\pi}}{2} \\mathrm{erf}(b) - \\frac{\\sqrt{\\pi}}{2} \\mathrm{erf}(a) \\right] \\]\n\\[ = \\frac{1}{2u} \\left( \\mathrm{erf}(a) - \\mathrm{erf}(b) \\right)\\]\nwhere\n\\[a = { {x - u (t-\\Delta t)} \\over \\sqrt{2} \\sigma_x }\\]\n\\[b = { {x - u t} \\over \\sqrt{2} \\sigma_x } \\]\n\nusing SpecialFunctions: erf\n\nfunction ∫gx(x,t,Δt)\n    Δt = min(t,Δt)\n    a  = (x-u*(t-Δt))/(√2*σx(u*(t-Δt)))\n    b  = (x-u*t)/(√2*σx(u*t))\n    return erf(b,a)/(2u)\nend\n\nintpuff(x,y,z,t; m, Δt) = m*gy(y,x/u)*gz(z,x/u)*∫gx(x,t,Δt)\n\n\n\n\n\n\n\n\nFigure 4: The dispersion of the integrated puff model, assuming constant dispersion/\n\n\n\n\nThis release model has some convenient properties: clearly as \\(\\Delta t \\to 0\\) it becomes a Gaussian puff again, but also as \\(\\Delta t \\to \\infty\\) also limits to the Gaussian plume.8\n8 This is somewhat hand-wavy but a release of infinite duration is an event that began an infinite amount of time in the past and continues an infinite amount into the future, so the term \\(\\frac{1}{2u} \\left( \\mathrm{erf}(a) - \\mathrm{erf}(b) \\right)\\) goes in the limit to \\(\\frac{1}{2u} \\left( \\mathrm{erf}(\\infty) - \\mathrm{erf}(-\\infty) \\right) = \\frac{1}{u}\\) resulting in a concentration profile of \\(c \\left(x,y,z,t \\right) = \\frac{m}{u} \\cdot g_y(y) \\cdot g_z(z)\\), which is exactly a plume model.\n\n\n\n\n\n\n\nFigure 5: The limiting behaviour of the integrated puff model, it smoothly connects the single puff model and the plume model when using the same dispersion constants."
  },
  {
    "objectID": "posts/integrated_puff/index.html#complications",
    "href": "posts/integrated_puff/index.html#complications",
    "title": "Between a puff and a plume",
    "section": "Complications",
    "text": "Complications\nI’ve been casually treating the dispersion parameters, \\(\\sigma_x, \\sigma_y, \\sigma_z\\), as being constants that are independent of the model and any transformations on the model. Within the context of taking sums and doing integrals it is reasonable: within a reasonable radius of the center of a given puff they are nearly constant. However, in practice, they depend upon time through their dependence on the location of the puff center and are also functions of the model itself.\nThe dispersion parameters for a plume model are not the same as for a puff, and so the nice smooth curve connecting the two doesn’t really work. Not if you are strictly taking dispersion parameters as provided in standard references. It is not at all clear how to transition from the one set to the other either, in a smooth manner, to ensure that there is a smooth transition from puff to plume.\nThat said, multiple puff models use the dispersion parameters for puffs and so using the puff parameters in the integrated puff model at least puts one in good company.\n\n\n\n\n\n\nUpdate\n\n\n\nThere is a follow-up post that discusses the quality of these approximations in more detail.\nWhen I first wrote this post I could not find my final result in the literature – it wasn’t in the standard references I use, and I think I just didn’t know the right search terms. Though it seemed equally obvious to me that it must be in the literature somewhere. Since posting this, I found it: Palazzi et al.9 This is also the model used by ALOHA and the older ARCHIE models.\n\n\n9 “Diffusion from a Steady Source of Short Duration.”"
  },
  {
    "objectID": "posts/integrated_puff/index.html#references",
    "href": "posts/integrated_puff/index.html#references",
    "title": "Between a puff and a plume",
    "section": "References",
    "text": "References\n\n\nAIChE/CCPS. Guidelines for Consequence Analysis of Chemical Releases. New York: American Institute of Chemical Engineers, 1999.\n\n\nLees, Frank P. Loss Prevention in the Process Industries. 2nd ed. Oxford: Butterworth-Heinemann, 1996.\n\n\nPalazzi, E, M De Faveri, Giuseppe Fumarola, and G Ferraiolo. “Diffusion from a Steady Source of Short Duration.” Atmospheric Environment 16, no. 12 (1982): 2785–90. https://www.researchgate.net/publication/328744668_DIFFUSION_FROM_A_STEADY_SOURCE_OF_SHORT_DURATION."
  },
  {
    "objectID": "posts/vapour_cloud_explosion_example/index.html",
    "href": "posts/vapour_cloud_explosion_example/index.html",
    "title": "VCE Example - Butane Vapour Cloud",
    "section": "",
    "text": "In a previous post I worked through estimating the airborne quantity of butane due to a leak from a storage sphere. That example stopped at estimating the total quantity released, here I would like to go further into the potential for a vapour cloud explosion.\nAs a recap the scenario is a leak from a butane storage sphere, the leak is 10ft above grade and results in cloud of mostly aerosolized butane that is initially below ambient temperature. The scenario parameters and results are summarized below.\nAs a quick note, the whole purpose of this exercise is a sort of high-level hazard screening. Detailed enough to decide whether or not the consequences of a hazard warrant more detailed modeling.\nusing Plots\nusing Unitful\nusing Interpolations\nusing CSV\nusing DataFrames\n\ngr()\n\nft = ustrip(u\"m\", 1u\"ft\")     # unit conversion ft-&gt;m\ninch = ustrip(u\"m\", 1u\"inch\") # unit conversion inch-&gt;m\npsi = ustrip(u\"Pa\", 1u\"psi\")  # unit conversion psi-&gt;Pa\n# Scenario parameters\nhᵣ = 10ft       # height of release point, m\npₐ= 14.7psi     # atmospheric pressure, Pa\nTₐ= 25 + 273.15 # the ambient temperature, K\ntd = 10*60      # release duration, 10 minutes, s\n\n# Constants\nR = 8.31446261815324 # universal gas constant, J/mol/K\ng = 9.806            # acceleration due to gravity, m/s2\n\n# Properties of Butane, from Perry's or DIPPR\nMw = 58.122    # molar weight kg/kmol\nρₗ(T) = Mw*( 1.0677/0.27188^(1+ (1-T/425.12)^0.28688) ) # density liquid, kg/m3\nρg(T) = (pₐ*Mw)/(R*T)/1000 # density gas, ideal gas law, kg/m3\nΔHc = 2657.320 # heat of combustion, kJ/mol\nLFL = 1.86e-2  # lower flammability limit, vol/vol\n\n# Properties of Air\nMWₐᵢᵣ = 28.960                  # molar weight air, kg/kmol\nρa(T) = (pₐ*MWₐᵢᵣ)/(R*T)/1000   # density of air, ideal gas law, kg/m^3\n\n# Calculated previously\nTc = -0.6 + 273.15       # cloud temperature, K\nfᵥ = 0.17128269541302374 # flashed fraction\nfₐ = 0.9227949810754577  # aerosol fraction\n\nQaq = 52.82002170865257 # airborne quantity, kg/s\nThis example focuses on a next step in a standard hazard screening, namely estimating the scale of a potential vapour cloud explosion. Typically, for flammable gases, the potential for a vapour cloud explosion is the worst case outcome of a release."
  },
  {
    "objectID": "posts/vapour_cloud_explosion_example/index.html#vapour-cloud-dispersion",
    "href": "posts/vapour_cloud_explosion_example/index.html#vapour-cloud-dispersion",
    "title": "VCE Example - Butane Vapour Cloud",
    "section": "Vapour Cloud Dispersion",
    "text": "Vapour Cloud Dispersion\nThe first step in determining the consequences of a vapour cloud explosion is to estimate the size of the vapour cloud that could take part in the explosion. This is generally done through some sort of dispersion modeling. There are many ways of defining the portion of the vapour cloud that can explode, in this case I am going to assume the flammable portion of the cloud is that with a concentration \\(\\ge \\frac{1}{2} LFL\\) .\nThere is a fair bit of discussion in the literature as to whether to use the LFL or 1/2 LFL, using half the LFL is, at the very least, more conservative and given the simplified methods I am using to estimate the size of the cloud it is probably best to err on the side of an overly large cloud.\n\nAtmospheric Stability and Wind Profile\nPrior to determining the particular dispersion model some meteorological parameters must be decided upon. Atmospheric stability and the wind profile define the extent of vertical mixing, which governs how large the cloud can grow and how dispersed the butane, in this case, will get during the release. They are also important in the decision criteria for which type of dispersion model to use.\nIn general, the worst case atmospheric stability is the most stable, Pasquill stability class F, for neutral to negatively buoyant clouds at ground level. This limits the degree of mixing and leads to a larger explosive mass, since we define the explosive mass as the portion of the cloud greater than half the lower flammability limit. If the cloud mixes thoroughly with the air it will be dispersed to levels well below the LFL and thus cannot explode\nFor this scenario I am supposing class F stability and a moderate windspeed of 3m/s at the release point.\nThere are two other important wind-speeds that will be needed, the friction velocity, \\(u_{\\star}\\), and the wind-speed at the standard elevation of 10m, \\(u_{10}\\) which can be obtained from the wind profile. The wind profile can be estimated using a power law distribution parameterized based on the Pasquill stability class.\n\\[ {u \\over u_r} = \\left( h \\over h_r \\right)^p \\]\nWhere the parameter p is tabulated1 here\n1 AIChE/CCPS, Guidelines for Consequence Analysis of Chemical Releases., 83.\n\n\nStability\nurban\nrural\n\n\n\n\nA\n0.15\n0.07\n\n\nB\n0.15\n0.07\n\n\nC\n0.20\n0.10\n\n\nD\n0.25\n0.15\n\n\nE\n0.40\n0.35\n\n\nF\n0.60\n0.55\n\n\n\nThere are several ways to estimate the friction velocity, but a simple rule of thumb used in the EPA TSCREEN model is to assume\n\\[ u_{\\star} = 0.06 u_{10} \\]\nThis is a very simplified approach and there are many alternatives to calculating the friction velocity, and parameterizing the atmospheric stability. For the purposes of this simple example this is fine but it is an opportunity for future refinement of the hazard screening.\n\n# wind profile\nuᵣ = 3.0     # the wind-speed at release height, m/s\np = 0.55     # parameter, pasquill stability class F\n\nu(h) = uᵣ*(h/hᵣ)^p\n\nu₁₀ = u(10)\n\nu₊ = 0.06 * u₁₀\n\n0.3459905806850393\n\n\n\n\nRelease Type\nThe first important decision is whether to model the release as a continuous plume or an instantaneous puff. In reality the answer is neither but these limiting cases are easier to model and are used as first approximations.\nA simple rule of thumb is that if the distance traveled by a parcel of air over the release duration is greater than 2.5 times the distance to the point of interest, then the release can be modeled as continuous, otherwise it would be treated as instantaneous.\n\\[ 2.5 \\le { u_r t_d \\over x^{\\star} } \\]\nRoughly speaking if the plume is still attached to the release point when the leading edge hits the downwind point of interest then it looks like a continuous plume to an observer at this point.\nThis can be used to define a critical distance, such that any distance less than that is best modeled by a continuous release and any distance greater is best modeled by an instantaneous release. This is useful as the downwind distance is, as of yet, unknown. The distance to half the LFL concentration, which defines the extent of the cloud involved in the explosion, depends upon which model is used and is in fact one of the key parameters we need to solve for.\n\nx⁺ = uᵣ*td/2.5\n\n720.0\n\n\nFrom prior experience, the distance to 1/2 LFL will likely be &lt;~200m and so a continuous release can be assumed. If after performing the calculation the downwind distance turns out to be greater than 720m then this can be re-assessed.\n\n\nDense Gas Dispersion\nThe second critical factor for determining which model to use is whether or not the cloud is significantly denser than air. Dense clouds slump and hug the ground to a far greater extent than neutrally buoyant clouds and models for neutral clouds can lead to significant overestimations of the size of the vapour cloud when used on a dense cloud.\nThe relevant parameter for determining if a dense gas dispersion model should be used is the Richardson number, the ratio of the potential energy from the excess density to the kinetic energy from ambient turbulence. The Richardson number for continuous releases is defined as2\n2 AIChE/CCPS, Guidelines for Use of Vapour Cloud Dispersion Models, 2nd Ed., 50.\\[ \\mathrm{Ri} = { { g_o V_r } \\over { D_c u_{\\star} } } \\]\nwhere\n\\[ g_o = g { {\\rho_c - \\rho_a } \\over \\rho_a } \\]\nwith \\(\\rho_c\\) the density of the cloud and \\(\\rho_a\\) the density of the ambient air. \\(V_r\\) is the volumetric release rate, and \\(D_c\\) a critical distance, in this case we take \\(D_c\\) to be the release height.\nThe density of the cloud is significantly larger than the vapour density of butane as the cloud has a large fraction of aerosolized droplets, overall the cloud density can be estimated by\n\\[ \\frac{1}{\\rho_c} = { f_v \\over \\rho_g } + { \\left(1 - f_v \\right) f_a \\over \\rho_l }\\]\nThe critical Richardson number is about 50 such that if the Richardson number is greater than 50 then a dense gas model must be used.\n\nρc(T) = ((fᵥ/ρg(T)) + ((1-fᵥ)*fₐ/ρₗ(T)))^-1\n\ngₒ = g * ((ρc(Tc) - ρa(Tₐ))/ ρa(Tₐ))\n\nVr = Qaq/ρc(Tc)\n\nRi = (gₒ * Vr)/(hᵣ * u₊)\n\n381.8214520915426\n\n\n\nRi &gt; 50\n\ntrue\n\n\nThis suggests that a dense gas model should be used, which conforms to our expectations as the vapour cloud is significantly denser than the ambient air.\nAn additional check is to use the criteria from Britter and McQuaid3\n3 “Workbook on the Dispersion of Dense Gases”.\\[ \\left( g_o V_r \\over { u_{10}^3 D} \\right)^{1/3} \\ge 0.15 \\]\nWhere \\(D\\) is a critical distance defined as\n\\[ D = \\sqrt{ V_r \\over u_{10} } \\]\n\nD = √(Vr/u₁₀)\n\n( (gₒ * Vr) / (u₁₀^3 * D) )^(1/3) ≥ 0.15\n\ntrue\n\n\n\n\nBritter-McQuaid model\nThe Britter-McQuaid model4 is a dense cloud dispersion model based on dimensional analysis and fitting to experimental data. It is given as a series of correlation curves for six different concentrations and the distance to the concentration of interest is interpolated from these. The concentrations represent a mean concentration over the whole cloud at that distance.\n4 Britter and McQuaid.This is one of the simpler models to use directly, and is appropriate for a screening case. Dense gas dispersion modeling is a large field with many different models that could be used and, like many things, as the models grow in detail they also grow in the number of parameters that must be provided. For the purposes of screening the limiting factor is often not computing power or model complexity per se as much as the information required to even run the models and the time needed to gather that information.\nNote the concentrations in this model are given in volume fraction and it is assumed that the in-cloud concentration of butane is 1.0 (i.e 100%) at the release point.\nThe Britter-McQuaid curves can be approximated with a series of piece-wise linear functions5\n5 AIChE/CCPS, Guidelines for Chemical Process Quantitative Risk Analysis.\\[ \\beta = m \\alpha + b \\]\n\nfunction piecewise(α; αs, ms, bs)\n    i = findnext(x -&gt; x &gt; α, αs, 1)\n    return ms[i]*α + bs[i]\nend\n\nfunction piecewise(; αs, ms, bs)\n    return α -&gt; piecewise(α, αs=αs, ms=ms, bs=bs)\nend\n\n\nBritter_McQuaid_correlations = Dict{Float64, Function}(\n    0.001 =&gt; piecewise(αs=[-0.69, -0.25, -0.13, 1.0],\n                       ms=[0.00, 0.39, 0.00, -0.50],\n                       bs=[2.60, 2.87, 2.77, 2.71]),\n    0.005 =&gt; piecewise(αs=[-0.67, -0.28, -0.15, 1.0],\n                       ms=[0.00, 0.59, 0.00, -0.49],\n                       bs=[2.40, 2.80, 2.63, 2.56]),\n    0.010 =&gt; piecewise(αs=[-0.70, -0.29, -0.20, 1.0],\n                       ms=[0.00, 0.49, 0.00, -0.52],\n                       bs=[2.25, 2.59, 2.45, 2.35]),\n    0.020 =&gt; piecewise(αs=[-0.69, -0.31, -0.16, 1.0],\n                       ms=[0.00, 0.45, 0.00, -0.54],\n                       bs=[2.08, 2.39, 2.25, 2.16]),\n    0.050 =&gt; piecewise(αs=[-0.68, -0.29, -0.18, 1.0],\n                       ms=[0.00, 0.36, 0.00, -0.56],\n                       bs=[1.92, 2.16, 2.06, 1.96]),\n    0.100 =&gt; piecewise(αs=[-0.55, -0.14, 1.0],\n                       ms=[0.00, 0.24, -0.50],\n                       bs=[1.75, 1.88, 1.78]),\n)\n\nDict{Float64,Function} with 6 entries:\n  0.01  =&gt; #7\n  0.005 =&gt; #7\n  0.02  =&gt; #7\n  0.001 =&gt; #7\n  0.1   =&gt; #7\n  0.05  =&gt; #7\n\n\n\n\n\n\n\n\n\n\nFigure 1: The digitized Britter-McQuaid correlation curves.\n\n\n\n\n\nThe correlations are given in terms of the parameters α and β, which are\n\\[ \\alpha = 0.2 \\cdot \\log \\left( g_o^2 V_r u_{10}^{-5} \\right) \\]\nand\n\\[ \\beta = \\log \\left( x \\over D \\right) \\]\nAt first glance it is not obvious how to use these plots, or the associated piecewise functions, since, at least to me, the obvious form of a model is to compute the concentration at a given point whereas the Britter-McQuaid model does the opposite. One supplies a concentration of interest and solves for the downwind distance where the leading edge of the plume hits this concentration.\nThe general procedure is:\n\nCompute the parameter α for the given scenario\nFind the concentration curves that bracket the concentration of interest\nCalculate β at these two concentrations and interpolate to find β at the concentration of interest\nCalculate the downwind concentration, x, from β\n\nThe function below is a convenience function that calculates the β for each concentration curve at a fixed α and returns a function that linearly interpolates to find the downwind distance for a given concentration.\n\n\"\"\"\n    britter_mcquaid_model(α, D; table=Britter_McQuaid_correlations)\n\nGenerate the interpolation function x(c), using the Britter-McQuaid correlations.\nThe system is parameterized by α and D, which are defined in the Britter-McQuaid\nmodel documentation.\n\n\"\"\"\nfunction britter_mcquaid_model(α, D; table=Britter_McQuaid_correlations)\n    interp_data = [ [conc, bfun(α)] for (conc, bfun) in table ]\n    interp_data = hcat(interp_data...)'\n    interp_data = sortslices(interp_data, dims=1)\n    linterp = LinearInterpolation(interp_data[:,1], interp_data[:,2], extrapolation_bc=Line())\n      \n    return c -&gt; 10^(linterp(c))*D\nend\n\nbritter_mcquaid_model\n\n\n\nα = 0.2*log10( gₒ^2 * Vr * u₁₀^-5 )\n\n0.17108241842192004\n\n\n\nx = britter_mcquaid_model(α, D)\n\n#11 (generic function with 1 method)\n\n\nThe Britter-McQuaid model assumes an isothermal case and the following correction is suggested for non-isothermal cases\n\\[ C^\\prime = { C \\over { C + (1-C) \\frac{T_a}{T_c} } }\\]\nIn this case the concentration of interest is half the lower flammability limit and the cloud is assumed to be well below ambient conditions.\n\nc = 0.5*LFL\n\nCᵢ = c / (c + (1 - c)*(Tₐ/Tc) )\n\n0.008508269826866945\n\n\nThe down-wind distance to half the LFL can then be estimated, and checked to ensure it is within the region for which a continuous release is a reasonable approximation.\n\nxᵢ = x(Cᵢ)\n\n165.85001073807788\n\n\n\nxᵢ ≤ x⁺\n\ntrue\n\n\nThe Britter-McQuaid model also has a correlation for short distances, \\(x \\lt 30D\\)\n\\[ C = { { 306 \\left( \\frac{x}{D} \\right)^{-2} } \\over { 1 + 306 \\left( \\frac{x}{D} \\right)^{-2} } } \\]\n\n\n\n\n\n\n\n\nFigure 2: The Britter-McQuaid concentration curve for the example release.\n\n\n\n\n\nThe Britter-McQuaid model does provide some further correlations to calculate the dimensions of the plume, though with many caveats as the model can over-estimate the width of the plume. Also I’ve noticed several sources give an obviously incorrect equation for plume height – it returns heights on the order of a few centimeters for plumes extending hundreds of meters in horizontal directions.\n\n\n\n\n\n\nFigure 3: The Britter-McQuaid plume shape and dimensions.\n\n\n\nInstead of that I am going to use a simple rule-of-thumb for small plumes\n\\[ V_{PES} = 0.03 x_{\\frac{1}{2}LFL}^3 \\]\nwhere \\(V_{PES}\\) is the volume of the potential explosion site. In general this more than just the volume of a plume dispersing in open space, since buildings and equipment can confine the cloud and create multiple potential explosion sites of various sizes. This is a simple rule-of-thumb for screening purposes only\n\nVₚₑₛ = 0.03 * xᵢ^3\n\n136857.23663150807"
  },
  {
    "objectID": "posts/vapour_cloud_explosion_example/index.html#vapour-cloud-explosion",
    "href": "posts/vapour_cloud_explosion_example/index.html#vapour-cloud-explosion",
    "title": "VCE Example - Butane Vapour Cloud",
    "section": "Vapour Cloud Explosion",
    "text": "Vapour Cloud Explosion\nThere are several parameters that can be estimated to characterize explosions with the positive overpressure perhaps being the most useful for simple screening cases. Tables exist relating different levels of positive overpressure to possible damage of nearby structures.\n\n\n\n\n\n\n\n\npsi\nkPa\nDamage\n\n\n\n\n0.02\n0.14\nAnnoying noise\n\n\n0.04\n0.28\nLoud noise, sonic boom, glass failure\n\n\n0.15\n1.03\nTypical pressure for glass breakage\n\n\n0.4\n2.76\nLimited minor structural damage\n\n\n1\n6.9\nPartial demolition of houses, made uninhabitable\n\n\n2 - 3\n13.8 - 20.7\nConcrete or cinder block walls, not reinforced, shattered\n\n\n3\n20.7\nSteel framed buildings distorted and pulled away from foundations\n\n\n4\n27.6\nCladding of light industrial buildings ruptured\n\n\n5\n34.5\nWooden utility poles snapped\n\n\n7\n48.2\nLoaded train wagon overturned\n\n\n10\n68.9\nProbable total destruction of buildings\n\n\n\nIt’s worth taking a moment to talk briefly about deflagrations and detonations. Deflagrations are characterized by subsonic flame propagation where the reaction zone moves through the flammable vapour by diffusion of heat and mass. Deflagrations typically result in relatively modest overpressures. A detonation, on the other hand, is characterized by a supersonic flame propagation and the reaction zone propagates by a pressure wave compressing the flammable vapour adiabatically to a temperature above the autoignition temperature. Detonations typically have significantly higher overpressures than deflagrations. Typically vapour cloud explosions in an open space with little congestion are deflagrations, however confinement and obstacles in the flame path can can accelerate a subsonic deflagration into a supersonic detonation. This is one reason why confinement and obstacles around a potential explosion site are important in the calculations.\nThe simplest way of calculating the overpressure is the TNT model, where the volume previously defined is used to estimate a potential explosion energy in TNT equivalents and this is compared to blast curves for TNT. For most major vapour cloud explosion incidents the TNT equivalences have been estimated to be from 1-10% of the full energy content of the cloud.\nThis is conceptually simple but can lead to very conservative estimates as vapour clouds, basically, don’t explode like TNT. These methods typically overestimate pressure close to the explosion source and underestimate it far afield. Even though the TNT method is generally not recommended, at least not in any of the references I have, it is still used in some places and it does crop up.\nA better approach is to use blast curves specifically for VCEs, in this case I am using the Baker-Strehlow-Tang curves but there are others.\n\nExplosive Energy\nThere are several ways of estimating explosive energy and all depend, in some way, on the size of the vapour cloud. Supposing the vapour cloud explosion is a deflagration and the energy in the explosion, fundamentally, comes from the combustion of the butane in the cloud then the energy can be found by estimating how much butane will combust and multiplying that by the heat of combustion of butane.\nIn some references the entire volume of the cloud will be assumed butane, but that can be excessively conservative – we are assuming the edge of the cloud to be 1/2 LFL or ~0.93% (v/v) butane so assuming it to be 100% butane in that region is a serious over-estimate.\nAn alternative method6 is to assume the cloud overall is at stoichiometric conditions. That is find the value of the stoichiometric concentration \\(\\eta\\)\n6 AIChE/CCPS, Guidelines for Vapour Cloud Explosion, Pressure Vessel Burst, BLEVE and Flash Fire Hazards.\\[ \\eta = { \\textrm{moles fuel} \\over \\textrm{moles fuel + air} } = { n_f \\over { n_f + \\frac{n_o}{f_o} } } \\]\nwhere \\(n_f\\) is the moles of fuel, \\(n_o\\) the moles of oxygen, and \\(f_o\\) the mole fraction of oxygen in air, 20.946%. For the combustion of n-butane\n\\[ C_{4} H_{10} + \\frac{13}{2} O_{2} \\longrightarrow 4 C O_{2} + 5 H_{2} O\\]\nand so for \\(n_f = 1\\) we have \\(n_o = 6.5\\)\n\nη = 1 / (1 + 6.5/0.20946)\n\n0.031218607756809045\n\n\nUsing the ideal gas law the total moles of gas in the cloud can be estimated\n\\[ n_c = { { p_a V_{PES} } \\over { R T_c } }\\]\nand the explosive energy is then\n\\[ E_{PES} = \\eta n_c \\Delta H_c \\]\n\nnc = ( pₐ * Vₚₑₛ )/(R * Tc)\n\nEₚₑₛ = η * nc * ΔHc\n\n5.0778644110258764e8\n\n\n\n\nThe BST model\nThe Baker-Strehlow-Tang model provides a series of correlation curves for different flame speeds and relates the positive overpressure to an energy scaled distance. The curves are based on spherical explosions and so it is important to include ground reflection when calculating the scaled distance.\nThe positive overpressure used in the BST model is a dimensionless pressure\n\\[ P = { { p - p_a } \\over p_a } \\]\nwhere \\(p\\) is the positive overpressure and \\(p_a\\) the atmospheric pressure. This is correlated to the scaled distance\n\\[ R = r \\cdot \\left( p_a \\over E \\right)^{1/3} \\]\nwhere \\(E\\) is the explosive energy and \\(r\\) the distance from the explosion epicentre. Which in this case we can take as the centre of the cloud and estimate to be half-way to \\(x_i\\)\nA spreadsheet with the BST curves is provided along with the AIChE/CCPS Guidelines for Chemical Process Quantitative Risk Analysis7 from which I’ve extracted just the positive overpressure curves as a csv.\n7 AIChE/CCPS, Guidelines for Chemical Process Quantitative Risk Analysis.\nbst_curves = CSV.read(\"data/BST-curves.csv\", DataFrame)\n\n# just show the first 5 rows\nfirst(bst_curves, 5)\n\n5 rows × 3 columnsMfScaled DistanceOverpressureFloat64Float64Float6410.0370.0101790.010096120.0370.01048670.010027130.0370.01080270.010099340.0370.01112870.010100950.0370.01146460.0101025\n\n\n\n\n\n\n\n\n\n\nFigure 4: The Baker-Strehlow-Tank overpressure curves\n\n\n\n\n\nThe following table8 cross-references flame speed – the key parameter of the BST curves – with qualitative descriptors of fuel reactivity, density of surrounding process equipment, and degree of confinement\n8 AIChE/CCPS, Guidelines for Vapour Cloud Explosion, Pressure Vessel Burst, BLEVE and Flash Fire Hazards.\nThe dimensionality given is referencing the number of dimensions along which the pressure wave can travel. For example an explosion confined to a tunnel is considered one dimensional since the pressure wave can only travel along the tunnel, whereas an explosion in an open field is three dimensional since it can expand in all directions. The entries labeled DDT are where a deflagration to detonation transition may occur, due to the flame speed and potential congestion. In these cases it is recommended to use the \\(Mf = 5.2\\) curve, sometimes referred to as the detonation blast curve.\n\n\n\n\n\n\n\nDimension\nDescription\n\n\n\n\n3-D\nUnconfined volume, almost completely free expansion\n\n\n2.5-D\nBlockage partially prevents flame in one direction, such as piperacks with tightly packed pipes, lightweight roofs, or frangible panels\n\n\n2-D\nPlatforms carrying process equipment, space beneath cars, open sided multistory buildings\n\n\n1-D\nTunnels, corridors, or sewage systems\n\n\n\nFor the storage sphere I am assuming it is standing freely on its own without any platforms or structures confining it, so the dimension is 3-D\nThe density of surrounding equipment is defined qualitatively in terms of how much the surrounding area obstructs the expansion of the pressure wave. This can be defined in terms of the percentage of area in a plane occupied by obstacles.\n\n\n\n\n\n\n\n\nType\nBlockage Ratio\nPitch for Obstacle Layers\n\n\n\n\nLow\n&lt; 10%\nOne or two layers of obstacles\n\n\nMedium\n10 - 40%\nTwo to three layers of obstacles\n\n\nHigh\n&gt; 40%\nThree of more fairly closely spaced obstacle layers\n\n\n\nFor the storage sphere suppose that there is some process equipment nearby but it is not highly confined, defaulting to medium.\nThe fuel reactivity categories are defined in terms of the laminar burning velocity\n\n\n\nReactivity\nLaminar Burning Velocity\n\n\n\n\nLow\n&lt; 45 cm/s\n\n\nMedium\n45 - 75 cm/s\n\n\nHigh\n&gt; 75 cm/s\n\n\n\nThe best resource for finding these tabulated is Appendix D of NFPA 68.9 For butane the laminar burning velocity is 45cm/s10 and thus is medium reactivity\n9 NFPA 68.10 NFPA 68 Table D.1(a).Returning to the table we find that the flame speed for a medium reactivity fuel, medium obstacle density, 3D case is 0.44 (in terms of Mach number)\n\nMf = 0.44\n\n0.44\n\n\nNote the flame speeds given in the table do not correspond to the flame speeds given in the BST curves. In general one will have to double-interpolate to get the results. Find the curves that bracket the desired flame speed, interpolate to find the corresponding pair of overpressures at the given scaled distance then interpolate to find the overpressure at the desired flame speed.\nThe following code sets this up in an easy way, though probably a very sub-optimal one, by doing the following:\n\nAn interpolation function is created for each flame speed, these are stored in bst_interps\nThe function Δp₊ calculates the positive overpressure by stepping through the array bst_interps and calculating the overpressures at each tabulated flame speed for a given distance, then interpolates for the desired flame speed\n\n\nbst_interps = []\nflame_speeds = unique(bst_curves[!, \"Mf\"])\n\nfor speed in flame_speeds\n    data = bst_curves[ bst_curves.\"Mf\" .== speed, :]\n    interp = LinearInterpolation(data[!, \"Scaled Distance\"], \n                                 data[!, \"Overpressure\"], \n                                 extrapolation_bc=Line())\n    push!(bst_interps, (speed, interp))\nend\n\n\n\"\"\"\n    Δp₊(r ; Mf=Mf, E=2*Eₚₑₛ, p₀=pₐ, curves=bst_interps)\n\nCalculate the positive overpressure at distance r from the explosion epicentre.\nThe model parameters are the apparent flame speed, Mf, in terms of Mach number,\nthe explosion energy, E, and the atmospheric pressure p₀. The units of E and p₀\nmust agree, e.g. kJ and kPa.\n\nThe Baker-Strehlow-Tang curves are supplied through the curves keyword.\n\"\"\"\nfunction Δp₊(r ; Mf=Mf, E=2*Eₚₑₛ, p₀=pₐ, curves=bst_interps)\n    E = 1000*E # Energy must be in J\n    R = r*(p₀/E)^(1/3)\n    \n    Mfs = []\n    Ps = []\n    \n    for (speed, interp) in bst_interps\n        push!(Mfs, speed)\n        push!(Ps, interp(R))\n    end\n    \n    P = LinearInterpolation(Mfs, Ps)\n    \n    return P(Mf)*p₀\n    \nend\n\nΔp₊\n\n\nThis generates a new blast curve interpolated between the curves supplied with the BST model, as shown in the plot below. Though care should be taken when the curve is used outside the range of the original curves.\nNote that the explosion energy is being multiplied by 2. This is to account for ground reflection as the BST curves are based on spherical explosions. In general the explosion energy is multiplied by a factor, which ranges from 1 to 2, to account for ground reflection where a factor of 2 is for explosions exactly at ground level and a factor of 1 is for explosions at significant elevation. The simpler and more conservative approach is to use a factor of 2.\n\n\n\n\n\n\n\n\nFigure 5: The Baker-Strehlow-Tank overpressure curves with the current scenario indicated.\n\n\n\n\n\nThe explosion epicentre is assumed to be the centre of the vapour cloud, here approximated to be half way between the release point and the downwind distance to 1/2 LFL, and in this simple model the explosion is a hemispherical pressure wave expanding in all directions unobstructed. More advanced modeling will take into account buildings and equipment and their impact on shaping the pressure wave.\nThe plot below shows the maximum positive overpressure experienced at that distance. Which is what is typically tabulated for different types of consequences. In general when I refer to overpressure this is what I am referring to.\n\n\n\n\n\n\n\n\nFigure 6: The overpressure contours for the given example, showing the maximum overpressure experienced at the location.\n\n\n\n\n\n\n\nSensitivity\nA useful question to ask at this point is how sensitive is the predicted overpressure to the parameters of the model.\nThe figure below shows the impact of varying reactivity, while holding all other parameters constant. Clearly whether or not the explosion is treated as a detonation or deflagration matters hugely, the high reactivity curve corresponds to a detonation. The difference between a medium and low reactivity material is a approximately a factor of 4 in terms of max overpressure. So finding an appropriate value of reactivity while not being overly conservative is important.\n\n\n\n\n\n\n\n\nFigure 7: The sensitivity of the overpressure curve to reactivity.\n\n\n\n\n\nThe following figure shows the impact of varying the levels of congestion. There is a fair amount of sensitivity going from low to medium but less from medium to high, which is perhaps what you would expect. I think the somewhat strange shape of the peaks is an artifact of linear interpolation.\n\n\n\n\n\n\n\n\nFigure 8: The sensitivity of the overpressure curve to levels of congestion.\n\n\n\n\n\nAs is clear from the figure below the results are much less sensitive to changes in the level of confinement. At least while neglecting the one dimensional case, which should always be treated as a special case regardless.\n\n\n\n\n\n\n\n\nFigure 9: The sensitivity of the overpressure curve to levels of confinement.\n\n\n\n\n\nThe last parameter worth investigating is the explosive energy, a significant portion of this exercise was in estimating the size of the vapour cloud and the consequent explosive energy. As is clear from the following figure the model is much less sensitive to changes in the explosive energy than the other parameters.\n\n\n\n\n\n\n\n\nFigure 10: The sensitivity of the overpressure curve to +/- 50% change in explosive energy"
  },
  {
    "objectID": "posts/vapour_cloud_explosion_example/index.html#references",
    "href": "posts/vapour_cloud_explosion_example/index.html#references",
    "title": "VCE Example - Butane Vapour Cloud",
    "section": "References",
    "text": "References\n\n\nAIChE/CCPS. Guidelines for Chemical Process Quantitative Risk Analysis. 2nd ed. New York: American Institute of Chemical Engineers, 2000.\n\n\n———. Guidelines for Consequence Analysis of Chemical Releases. New York: American Institute of Chemical Engineers, 1999.\n\n\n———. Guidelines for Use of Vapour Cloud Dispersion Models, 2nd Ed. New York: American Institute of Chemical Engineers, 1996.\n\n\n———. Guidelines for Vapour Cloud Explosion, Pressure Vessel Burst, BLEVE and Flash Fire Hazards. 2nd ed. New York: American Institute of Chemical Engineers, 2000.\n\n\nBritter, Rex E., and J. McQuaid. “Workbook on the Dispersion of Dense Gases. HSE Contract Research Report No. 17/1988,” 1988.\n\n\nNFPA 68: Standard on Explosion Protection by Deflagration Venting. Boston, MA: National Fire Protection Association, 2018."
  },
  {
    "objectID": "posts/turbulent_jet_notes/index.html",
    "href": "posts/turbulent_jet_notes/index.html",
    "title": "Turbulent Jets",
    "section": "",
    "text": "In a previous post I worked through a chemical release modeled as a turbulent jet and while I mentioned there were several ways modeling the jet, I didn’t go into any of them. I’m taking the opportunity here to collect my notes on turbulent jets, some different ways of modeling the jets, and the relative performance of each approach."
  },
  {
    "objectID": "posts/turbulent_jet_notes/index.html#observations-on-turbulent-jets",
    "href": "posts/turbulent_jet_notes/index.html#observations-on-turbulent-jets",
    "title": "Turbulent Jets",
    "section": "Observations on Turbulent Jets",
    "text": "Observations on Turbulent Jets\nWe are considering a submerged circular jet, issuing from a surface, with the coordinate system centered on the jet. Since it is circular, the natural coordinate system is cylindrical with a downstream distance z, radial distance r, and angular coordinate θ. The jet is fully turbulent when the Reynolds number, \\(Re \\gt 2000\\), where the Reynolds number is calculated with respect to the initial jet velocity and jet diameter\n\\[ Re = { \\rho_j v_0 d_0 \\over \\mu_j } \\]\nWe are also considering the case where the densities of the two fluids are similar, where we take “similar” to mean \\[ \\frac{1}{4} \\le { \\rho_{a} \\over \\rho_{j} } \\le 4 \\]\nWhere subscript a indicates the ambient fluid and j the jet. For much the experimental data the jet and ambient fluid are the same fluid, e.g. a jet of air into air or water into water.\nTurbulent jets expand by entraining ambient fluid, tracing out a cone defined by a jet angle \\(\\alpha \\approx 15-25^\\circ\\). The mixing layer penetrates into the jet forming the potential core, inside is pure jet material and outside is mixed. After approximately 6 diameters the region is fully developed.\n\n\n\n\n\n\nFigure 1: A turbulent jet emitted from a circular orifice.\n\n\n\nEmpirical approximations of the velocity profile are often given with respect to this jet angle or, equivalently, the slope of the line (i.e. \\(\\tan \\frac{\\alpha}{2}\\)). A related way of parameterizing the jet is in terms of a width parameter b. Typically this is the width of the velocity profile at half-height \\(b_{1/2}\\) (though not always). With a constant jet angle and a self-similar velocity profile the width is directly proportional to the downstream distance \\(b_{1/2} = \\tan \\left( \\frac{\\alpha_{1/2} }{2} \\right) z = c z\\).\nWhere the value of c is can be found in the literature\n\n\n\nc\nReference\n\n\n\n\n0.082 - 0.097\nGarde1\n\n\n0.0848\nBird, Stewart, and Lightfoot2\n\n\n0.10\nRajaratnam3\n\n\n\n1 Turbulent Flows.2 Transport Phenomena.3 Turbulent Jets.At this point it is common to introduce a variable \\(\\xi = {r \\over b_{1/2} }\\) or \\(\\xi = {r \\over z }\\) where we are taking advantage of the fact that \\(b_{1/2} \\propto z\\). This is a scaled radial distance, using the width at half-height as a characteristic length. It is important to keep track of which definition of ξ is being used as they differ by a scaling factor. The reason for this change of variables is the observation that the shape of the velocity profile is the same at any downstream point, it is merely scaled down in height and wider as one travels downstream. That is \\({ \\bar{v}_z \\over \\bar{v}_{max} } = f \\left( \\xi \\right)\\) is the same for all downstream distances (in the region where the jet is fully developed).\nAnother important observation is that the center-line velocity, the max velocity in the jet, decays with the inverse of the downstream distance, i.e.\n\\[ \\bar{v}_{max} \\propto z^{-1} \\]\nPutting those two observations together we expect the velocity profile to have the form\n\\[ \\bar{v}_z = { \\mathrm{const} \\over z } f \\left( \\xi \\right)\\]"
  },
  {
    "objectID": "posts/turbulent_jet_notes/index.html#modeling-turbulent-jets",
    "href": "posts/turbulent_jet_notes/index.html#modeling-turbulent-jets",
    "title": "Turbulent Jets",
    "section": "Modeling Turbulent Jets",
    "text": "Modeling Turbulent Jets\nTo set up our system we consider the case of a jet coming out of a point on an infinite surface into a quiescent medium, and that the jet and medium have the same density. This is a major simplification, but it makes the math easier to deal with. The coordinate system is centered at this point and all momentum in the jet ultimately comes from the origin.\nThe boundary conditions for the problem are:\n\nat the center-line, r=0, the velocity is entirely in the z-direction\nat the center-line, r=0, the velocity in the z-direction is at a maximum\nas the radius increases, r → ∞ , the velocity in the z-direction goes to zero\n\n\nTime Averaged Values\nSince we are concerned with turbulent flow, we can employ Reynolds decomposition to transform the velocities like so\n\\[ v_z = \\bar{v}_z + v^{\\prime}_{z} \\]\n\\[ v_r = \\bar{v}_r + v^{\\prime}_{r} \\]\nwhere \\(\\bar{v}\\) is the time-smoothed velocity and \\(v^{\\prime}\\) is an instantaneous deviation such that \\(\\bar{v^{\\prime} } = 0\\) and the time-averaging operator follows the Reynolds criteria.\n\n\nEquations of Motion\nThe equations of motion in terms of time-smoothed velocities are \\[ \\rho {D \\mathbf{\\bar{v} } \\over D t } = - \\nabla \\bar{p} - \\nabla \\cdot \\mathbf{ \\bar{\\tau} } + \\rho \\mathbf{g}  \\]\nWhere \\(\\mathbf{ \\bar{\\tau} }\\) is the turbulent stress and includes the Reynolds stresses.\nWith the z component, in cylindrical coordinates4\n4 Bird, Stewart, and Lightfoot, Transport Phenomena, 847.\\[ \\rho \\left( {\\partial \\over \\partial t} \\bar{v}_z + \\bar{v}_r {\\partial \\bar{v}_z \\over \\partial r} + {\\bar{v}_\\theta \\over r} {\\partial \\bar{v}_z \\over \\partial \\theta} + \\bar{v}_z {\\partial \\bar{v}_z \\over \\partial z} \\right) \\]\n\\[ = - {\\partial \\bar{p} \\over \\partial z} - {1 \\over r} {\\partial \\left( r \\bar{\\tau}_{rz} \\right) \\over \\partial r } - {1 \\over r} {\\partial \\bar{\\tau}_{\\theta z} \\over \\partial \\theta } - {\\partial \\bar{\\tau}_{z z} \\over \\partial z } + \\rho g_z\\]\nMaking the assumptions:\n\nZero pressure gradient ( \\({\\partial p \\over \\partial z} = 0\\) )\nSteady state ( \\({\\partial \\over \\partial t} \\left( \\cdots \\right) = 0\\) )\nAxisymmetric ( \\({\\partial \\over \\partial \\theta} \\left( \\cdots \\right) = 0\\) )\nEffect of gravity can be neglected ( \\(\\rho g_z \\approx 0\\) )\nWithin the jet \\(\\mid v_z \\mid \\gg \\mid v_r \\mid\\) and, by boundary layer approximation, \\(\\bar{\\tau}_{z z}\\) can be neglected5\n\n5 The boundary layer approximation is that\n\\[ { \\partial^2 \\bar{v}_z \\over \\partial z^2 } \\ll { \\partial^2 \\bar{v}_z \\over \\partial r^2 } \\]\nand if we suppose that\n\\[ \\bar{\\tau}_{z z} \\propto { \\partial \\bar{v}_z \\over \\partial z } \\]\nand\n\\[ \\bar{\\tau}_{r z} \\propto {\\partial \\bar{v}_z \\over \\partial r} \\]\nwe find\n\\[ { \\partial \\bar{\\tau}_{z z} \\over \\partial z } \\propto {\\partial^2 \\bar{v}_z \\over \\partial z^2} \\ll { \\partial r \\bar{\\tau}_{r z} \\over \\partial r} \\propto {\\partial^2 \\bar{v}_z \\over \\partial r^2} \\]\nand thus we can assume the free turbulence is dominated by \\(\\bar{\\tau}_{r z}\\) and\n\\[ { \\partial \\bar{\\tau}_{z z} \\over \\partial z} \\approx 0 \\]The equations of motion, in the z direction, simplifies to\n\\[ \\bar{v}_r {\\partial \\bar{v}_z \\over \\partial r} + \\bar{v}_z {\\partial \\bar{v}_z \\over \\partial z} = - {1 \\over \\rho r} {\\partial \\left( r \\bar{\\tau}_{rz} \\right) \\over \\partial r } \\]\n\n\nEquation of Continuity\nThe continuity equation in terms of time-smoothed velocities is\n\\[ {\\partial \\rho \\over \\partial t} + \\nabla \\cdot \\rho \\mathbf{ \\bar{v} } = 0 \\]\nIn cylindrical coordinates6\n6 Bird, Stewart, and Lightfoot, Transport Phenomena.\\[ {\\partial \\rho \\over \\partial t} + {1 \\over r} {\\partial \\rho r \\bar{v}_r \\over \\partial r} + {1 \\over r} { \\partial \\rho \\bar{v}_\\theta \\over \\partial \\theta} + {\\partial \\rho \\bar{v}_z \\over \\partial z} = 0 \\]\nMaking the assumptions:\n\nSteady state ( \\({\\partial \\over \\partial t} \\left( \\cdots \\right) = 0\\) )\nAxisymmetric ( \\({\\partial \\over \\partial \\theta} \\left( \\cdots \\right) = 0\\) )\nIncompressible ( \\({\\partial \\rho \\over \\partial z} = {\\partial \\rho \\over \\partial r} = {\\partial \\rho \\over \\partial \\theta} = 0\\) )\n\nThe equation of continuity simplifies to\n\\[ {1 \\over r} {\\partial r \\bar{v}_r \\over \\partial r} + {\\partial \\bar{v}_z \\over \\partial z} = 0 \\]\n\n\nStokes Stream Function\nTo simplify things down to working with one dependent variable we introduce a Stokes stream function \\(\\psi\\) defined such that\n\\[ \\bar{v}_z = -{1 \\over r} {\\partial \\psi \\over \\partial r} \\]\nand\n\\[ \\bar{v}_r = {1 \\over r} {\\partial \\psi \\over \\partial z} \\]\nThis definition ensures that the equation of continuity is satisfied. Suppose that \\(\\psi = k z F\\left(\\xi\\right)\\), where F is a unitless function of \\(\\xi = \\frac{r}{z}\\) and \\(k\\) is a constant with units \\([[ \\mathrm{length} ]]^2 \\times [[ \\mathrm{time} ]]^{-1}\\), then\n\\[ \\bar{v}_z = -{1 \\over r} {\\partial \\xi \\over \\partial r} {\\partial \\psi \\over \\partial \\xi}\n= -{1 \\over r} {1 \\over z} {k z F^{\\prime} } \\]\n\\[= -{k \\over z} {F^{\\prime} \\over \\xi} = { \\mathrm{const} \\over z } f \\left( \\xi \\right)\\]\nWhich matches what we expect from the empirical observations (which is why we supposed that form of the stream function in the first place). We can use this definition to work out some other useful terms\n\\[ {\\partial \\bar{v}_z \\over \\partial z} = {k \\over z^2} F^{\\prime \\prime} \\]\n\\[ {\\partial \\bar{v}_z \\over \\partial r} = -{k \\over z^2} \\left( { F^{\\prime \\prime} \\over \\xi} - { F^{\\prime} \\over \\xi^2} \\right) \\]\n\\[ \\bar{v}_r = { k \\over z } \\left( { F \\over \\xi } - F^{\\prime} \\right) \\]\nSubstituting these back into the equation of motion, in the z direction, leads to\n\\[ \\left( k \\over z \\right)^2 \\left[ { F F^{\\prime \\prime} \\over \\xi } - {F F^{\\prime} \\over \\xi^2} + { \\left( F^{\\prime} \\right)^2 \\over \\xi } \\right]  = {1 \\over \\rho} {\\partial \\over \\partial r} \\left( r \\bar{\\tau}_{rz} \\right) \\]\n\\[ \\left( k \\over z \\right)^2 { d \\over d \\xi } \\left( F F^{\\prime} \\over \\xi \\right) = {1 \\over \\rho} {\\partial \\over \\partial r} \\left( r \\bar{\\tau}_{rz} \\right) \\]\nWhich is suggestive of the overall approach to follow: find an expression for the right hand side of this differential equation, integrate both sides with respect to ξ, and solve for F(ξ)\n\n\nBoundary Conditions\nThe initial boundary conditions of the problem were that:\n\n\\(\\bar{v}_r = 0\\) at r=0\n\\({\\partial \\bar{v}_z \\over \\partial r} = 0\\) at r=0 (i.e. the velocity is at a maximum)\n\\(\\bar{v}_z \\to 0\\) as r → ∞ (i.e. the velocity decays to zero)\n\nIn terms of F and ξ these become:\n\n\\({F \\over \\xi} - F^{\\prime} = 0\\) at ξ=0, which implies F=0 at ξ=0\n\\(F^{\\prime \\prime} - {F^{\\prime} \\over \\xi}  = 0\\) at ξ=0\n\\({F^{\\prime} \\over \\xi} \\to 0\\) as ξ → ∞\n\n\n\nMomentum Balance\nTo determine the constant k we use a momentum balance: the momentum flux, J, in the z direction is constant. Initially the momentum flux is\n\\[ J = \\rho v_0^2 A_0 = \\rho v_0^2 {\\pi \\over 4} d_0^2\\]\nand at some point z downstream of the origin we have\n\\[ J = \\int_{0}^{2\\pi} \\int_{0}^{\\infty} \\rho \\bar{v}_z^2 r dr d\\theta \\]\n\\[   = 2 \\pi \\rho \\int_{0}^{\\infty} \\bar{v}_{z,max}^2 \\left( \\bar{v}_z \\over \\bar{v}_{z,max} \\right)^2 r dr \\]\n\\[   = 2 \\pi \\rho \\bar{v}_{z,max}^2 \\int_{0}^{\\infty} \\left( \\bar{v}_z \\over \\bar{v}_{z,max} \\right)^2 r dr \\]\n\\[   = 2 \\pi \\rho k^2 \\int_{0}^{\\infty} \\left( f\\left( \\xi \\right) \\right)^2 \\xi d \\xi \\]\nTaking the integral to be I, and equating the initial momentum flux with the momentum flux at point z7\n7 I’ve played a little fast and loose with the definition of \\(\\bar{v}_z\\) in that I am implicitly assuming \\(f(\\xi) = {-F^{\\prime}(\\xi) \\over \\xi}\\) which isn’t strictly true, there can be scaling factor. In practice all of these are collected together into one constant so it doesn’t matter, but that is something to be aware of as the definition of k here is really \\(k\\times \\mathrm{const}\\) where \\(\\mathrm{const} = {-F^{\\prime}(\\xi) \\over \\xi} \\div f(\\xi)\\)\\[ J = \\rho v_0^2 {\\pi \\over 4} d_0^2 = 2 \\pi \\rho k^2 I \\]\n\\[ k = \\sqrt{1 \\over 8 I } v_0 d_0 \\]"
  },
  {
    "objectID": "posts/turbulent_jet_notes/index.html#prandtl-mixing-length",
    "href": "posts/turbulent_jet_notes/index.html#prandtl-mixing-length",
    "title": "Turbulent Jets",
    "section": "Prandtl Mixing Length",
    "text": "Prandtl Mixing Length\nThe Prandtl mixing length model makes the assumption that momentum transfer occurs over some “mixing length” l such that\n\\[ \\bar{\\tau}_{rz} = -\\rho l^2 \\left| {\\partial \\bar{v}_z \\over \\partial r} \\right| \\left( {\\partial \\bar{v}_z \\over \\partial r} \\right)\\]\nWe suppose that the mixing length is proportional to the width of the velocity profile \\(b_{1/2}\\), the characteristic length for the velocity profile, which we know is proportional to the downstream distance z\n\\[ l \\propto b_{1/2} \\propto z\\]\n\\[ l = c z \\]\nWhere \\(c\\) is some unitless constant. Making the observation that \\({\\partial \\bar{v}_z \\over \\partial r} &lt; 0\\) we can make the simplification\n\\[ \\bar{\\tau}_{rz} = \\rho c^2 z^2 \\left( {\\partial \\bar{v}_z \\over \\partial r} \\right)^2\\]\n\nSetting up the ODE\nRecall that the equation of motion in the z direction is (in terms of the unitless function F) is\n\\[ \\left( k \\over z \\right)^2 { d \\over d \\xi } \\left( F F^{\\prime} \\over \\xi \\right) = {1 \\over \\rho} {\\partial \\over \\partial r} \\left( r \\bar{\\tau}_{rz} \\right) \\]\nSubstituting the expression for \\(\\bar{\\tau}_{rz}\\) we have\n\\[ \\left( k \\over z \\right)^2 { d \\over d \\xi } \\left( F F^{\\prime} \\over \\xi \\right) = c^2 z^2 {\\partial \\over \\partial r} \\left( r \\left( \\partial \\bar{v}_{z} \\over \\partial r \\right)^2 \\right) \\]\n\\[ \\left( k \\over z \\right)^2 { d \\over d \\xi } \\left( F F^{\\prime} \\over \\xi \\right) = c^2 z^2 \\left( \\left( \\partial \\bar{v}_{z} \\over \\partial r \\right)^2 + 2r \\left( \\partial \\bar{v}_{z} \\over \\partial r \\right) \\left( \\partial^2 \\bar{v}_{z} \\over \\partial r^2 \\right) \\right) \\]\nSubstituting in the expressions for \\({\\partial \\bar{v}_z \\over \\partial r}\\) and \\({\\partial^2 \\bar{v}_z \\over \\partial r^2}\\) we arrive at\n\\[ \\left( k \\over z \\right)^2 { d \\over d \\xi } \\left( F F^{\\prime} \\over \\xi \\right) = c^2 \\left(k \\over z \\right)^2 \\left( 1 \\over \\xi \\right)\\left( F^{\\prime \\prime} - { F^{\\prime} \\over \\xi } \\right) \\left(  2 F^{\\prime \\prime \\prime} - 3 { F^{\\prime \\prime} \\over \\xi } + { F^{\\prime} \\over \\xi^2 }\\right) \\]\n\\[ { d \\over d \\xi } \\left( F F^{\\prime} \\over \\xi \\right) = c^2 { d \\over d \\xi } \\left( 1 \\over \\xi \\right)\\left( F^{\\prime \\prime} - { F^{\\prime} \\over \\xi } \\right)^2 \\]\nIntegrating both sides\n\\[ \\left( F F^{\\prime} \\over \\xi \\right) = c^2 \\left( 1 \\over \\xi \\right)\\left( F^{\\prime \\prime} - { F^{\\prime} \\over \\xi } \\right)^2 + \\mathrm{const}\\]\nBy applying the boundary conditions we find the constant of integration is zero, thus\n\\[ F F^{\\prime} = c^2 \\left( F^{\\prime \\prime} - { F^{\\prime} \\over \\xi } \\right)^2 \\]\nMaking the substitution \\(\\phi = a^{-1} \\xi\\) where \\(a = c^{2/3}\\)\n\\[ F F^{\\prime} = \\left( F^{\\prime \\prime} - { F^{\\prime} \\over \\phi } \\right)^2 \\]\n\\[ F^{\\prime \\prime} = { F^{\\prime} \\over \\phi } + \\sqrt{ F F^{\\prime} } \\]\nWhich is in a form that can be solved numerically.\n\n\nSolving the ODE\nWe can solve the ODE and perform the integral needed for the momentum balance at the same time. First we define a vector u such that:\n\\[ \\mathbf{u} =  \\begin{bmatrix} u_{1} \\\\ u_{2} \\end{bmatrix} = \\begin{bmatrix} F \\\\ F^{\\prime} \\end{bmatrix}\\]\nThe ODE then becomes:\n\\[ {d \\mathbf{u} \\over dt } = \\begin{bmatrix} F^{\\prime} \\\\ F^{\\prime \\prime} \\end{bmatrix}  = \\begin{bmatrix} u_{2} \\\\ \\frac{ u_{2} }{t} + \\sqrt{ u_{1} u_{2} } \\end{bmatrix} \\]\nWhich has a singularity at t=0, but one that can be easily dealt with by setting the initial value of the derivatives to8\n8 From the boundary conditions we know F’(0) = 0 but what about F’’? Taking the ratio\n\\[ { \\bar{v}_z \\over \\bar{v}_{z,max} }_{r=0} = - {F^{\\prime} \\over \\phi }_{\\phi=0} = 1 \\]\nwe find \\({F^{\\prime} \\over \\phi } = -1\\) at φ = 0 and, from the boundary conditions,\n\\[ F^{\\prime \\prime} = {F^{\\prime} \\over \\phi } \\]\nat φ = 0, therefore F’’(0) = -1\\[ {d \\mathbf{u} \\over dt }_{t=0} = \\begin{bmatrix} 0 \\\\ -1 \\end{bmatrix}\\]\nPutting that together, the ODE can be integrated easily9\n9 Because of how \\(\\bar{v}_z\\) and \\(\\bar{v}_r\\) were defined \\(-{ F^{\\prime} \\over \\phi } \\ge 0\\), i.e. \\({ F^{\\prime} \\over \\phi } \\le 0\\). For the signs to work out, \\(F \\le 0\\) and \\(F^{\\prime} \\le 0\\) (since \\(F F^{\\prime} \\ge 0\\))\nusing StaticArrays\nusing DifferentialEquations: ODEProblem, Tsit5, solve, TerminateSteadyState\n\nfunction sys(u,p,t)\n    u₁, u₂ = u[1], u[2]\n    if t &gt; 0.0\n        du₁ = u₂\n        du₂ = u₂/t + √(max((u₁*u₂),0))\n    else\n        du₁ = 0.0\n        du₂ = -1.0\n    end\n    \n    return SA[du₁; du₂]\nend\n\nu0    = SA[0.0; 0.0]\ntspan = (0.0, 6.0)\nprob  = ODEProblem(sys, u0, tspan)\nsol   = solve(prob, Tsit5(), dtmax=0.1, callback=TerminateSteadyState())\n\nprint(sol.retcode)\n\nSuccess\n\n\n\n\n\n\n\n\n\n\nFigure 2: The numerical integration of F(φ), Prandtl mixing length theory.\n\n\n\n\n\nUsing the solution in terms of φ we can write a function f(ξ)\n\nfunction f_pml(ξ; a=0.066)\n    ϕ = abs(ξ)/a\n    \n    if ϕ &gt;0\n        F, F′ = sol(ϕ)\n        f = -F′/ϕ\n        f  = max(f, 0)\n    else\n        f = 1\n    end\n    \n    return f\nend\n\n\n\nComparison with Tollmien\nThe classic treatment of the Prandtl mixing length model is from Tollmien10 in which, instead of solving numerically in the way shown above, the ODE is further transformed and a series expansion is used to generate a table of results. More often than not it is these tabulated values, or similar ones,11 that are presented as the solution to the model.\n10 “Berechnung Turbulenter Ausbreitungsvorgänge”.11 Rajaratnam, Turbulent Jets, 39. The table has an error at φ=1: the value of \\({F^{\\prime} \\over \\phi }\\) should be 0.606 but is given as 0.505 (presumably a typo).We can easily compare the result here with the tabulated values and verify for ourselves that we have indeed solved the right differential equation. Though by solving numerically in this way we can control the level of precision and easily generate smooth interpolations. In my opinion, this makes using the ODE solution far more convenient than the tabulated values.\n\n\n\n\n\n\n\n\nFigure 3: This solution versus the tabulated results of Tollmien, demonstrating that this is the correct solution but by a different means.\n\n\n\n\n\n\n\nWidth at Half Height\nThe width at half height, \\(b_{1/2}\\), is an important parameter and often velocity profiles are scaled relative to this. To compare different models on a fair basis, it is a good idea to determine what the model parameters are relative to \\(b_{1/2}\\). Then each model can be scaled to the same \\(b_{1/2}\\) and compared, apples-to-apples.\nIn this case we don’t have a closed form for the velocity profile so we need to solve for φ such that f(φ)=0 numerically.\n\nusing Roots: find_zero\n\nϕ_half = find_zero( ϕ -&gt; f_pml(ϕ; a=1)-0.5, (1, 1.25))\n\n1.2277665940765845\n\n\nand we then write the model parameter a in terms of \\(b_{1/2}\\)\n\n\n\\[ a = \\frac{1}{\\phi_{1/2}} \\frac{b_{1/2}}{z} = 0.814 \\frac{b_{1/2}}{z} \\]\n\n\nUsing a default value for \\({ b_{1/2} \\over z } = 0.0848\\) we arrive at\n\nb_half = 0.0848\n\na = b_half/ϕ_half\n\n0.06906850244103516\n\n\nSeveral sources have tabulated values for a\n\n\n\na\nReference\n\n\n\n\n0.063\nTollmien12\n\n\n0.066\nRajaratnam13\n\n\n\n12 “Berechnung Turbulenter Ausbreitungsvorgänge”.13 Turbulent Jets.and the result of this notebook compares with those\n\n\nVelocity Profile\nNow that we have completed the integration we can calculate the parameter k, using the equation derived from the momentum balance\n\\[ k = \\sqrt{1 \\over 8 I } v_0 d_0 \\]\nwith the value of the integral coming directly from the ode solver\n\nusing NumericalIntegration: integrate\n\nϕ, F′ = sol.t, sol[2,:]\n\n# trim any unphysical values\nF′[F′.&gt;0] .= 0.0\n\nfunction integrand(ϕ, F′)\n    if ϕ&gt;0\n        return F′^2/ϕ\n    else\n        return 0\n    end\nend\n\nI = integrate(ϕ, integrand.(ϕ, F′))\nI = a^2 * I\n\n0.002573069044757039\n\n\nAllowing us to write the velocity profile as\n\n\n\\[ \\bar{v}_z = 6.97 { v_0 d_0 \\over z} f(\\xi) \\]"
  },
  {
    "objectID": "posts/turbulent_jet_notes/index.html#eddy-viscosity",
    "href": "posts/turbulent_jet_notes/index.html#eddy-viscosity",
    "title": "Turbulent Jets",
    "section": "Eddy Viscosity",
    "text": "Eddy Viscosity\nThe eddy viscosity model makes the assumption that the turbulent shear stress depends on the rate of strain in a manner that is analogous to laminar flow, with the constant of proportionality being the eddy viscosity ε:\n\\[ \\bar{\\tau}_{rz} = - \\rho \\varepsilon {\\partial \\bar{v}_z \\over \\partial r}\\]\n\nSetting up the ODE\nRecall that the equation of motion in the z direction is (in terms of the unitless function F)\n\\[ \\left( k \\over z \\right)^2 { d \\over d \\xi } \\left( F F^{\\prime} \\over \\xi \\right) = {1 \\over \\rho} {\\partial \\over \\partial r} \\left( r \\bar{\\tau}_{rz} \\right) \\]\nSubstituting the expression for \\(\\bar{\\tau}_{rz}\\) we have\n\\[ \\left( k \\over z \\right)^2 { d \\over d \\xi } \\left( F F^{\\prime} \\over \\xi \\right) = - \\varepsilon {\\partial \\over \\partial r} \\left( r \\left( \\partial \\bar{v}_{z} \\over \\partial r \\right) \\right) \\]\n\\[ \\left( k \\over z \\right)^2 { d \\over d \\xi } \\left( F F^{\\prime} \\over \\xi \\right) = - \\varepsilon \\left( \\left( \\partial \\bar{v}_{rz} \\over \\partial r \\right) + r \\left( \\partial^2 \\bar{v}_{rz} \\over \\partial r^2 \\right) \\right) \\]\nSubstituting in the expressions for \\({\\partial \\bar{v}_z \\over \\partial r}\\) and \\({\\partial^2 \\bar{v}_z \\over \\partial r^2}\\) we arrive at\n\\[ \\left( k \\over z \\right)^2 { d \\over d \\xi } \\left( F F^{\\prime} \\over \\xi \\right) = { k \\varepsilon \\over z^2} \\left( F^{\\prime \\prime \\prime} - { F^{\\prime \\prime} \\over \\xi } + { F^{\\prime} \\over \\xi^2 }  \\right) \\]\n\\[ \\left( k \\over z \\right)^2 { d \\over d \\xi } \\left( F F^{\\prime} \\over \\xi \\right) = { k \\varepsilon \\over z^2} { d \\over d \\xi } \\left( F^{\\prime \\prime} - { F^{\\prime} \\over \\xi } \\right) \\]\nat this point we note that k and ε have the same units of \\([[ \\mathrm{length} ]]^2 \\times [[ \\mathrm{time} ]]^{-1}\\) and are independent of z and ξ, so we propose that \\(\\varepsilon = c k\\) where c is some unknown constant of proportionality.\n\\[ \\left( k \\over z \\right)^2 { d \\over d \\xi } \\left( F F^{\\prime} \\over \\xi \\right) = c \\left( k \\over z \\right)^2 { d \\over d \\xi } \\left( F^{\\prime \\prime} - { F^{\\prime} \\over \\xi } \\right) \\]\n\\[ { d \\over d \\xi } \\left( F F^{\\prime} \\over \\xi \\right) = c { d \\over d \\xi } \\left( F^{\\prime \\prime} - { F^{\\prime} \\over \\xi } \\right) \\]\nIntegrating both sides\n\\[ { F F^{\\prime} \\over \\xi } = c \\left( F^{\\prime \\prime} - { F^{\\prime} \\over \\xi } \\right) + \\mathrm{const}\\]\nBy applying the boundary conditions we find the constant of integration is zero, thus\n\\[ F F^{\\prime} = c \\left( \\xi F^{\\prime \\prime} -  F^{\\prime} \\right) \\]\n\\[{ d \\over d \\xi } \\left( \\frac{1}{2} F^2 \\right) = c { d \\over d \\xi } \\left( \\xi F^{\\prime} -  2 F \\right) \\]\nIntegrating both sides\n\\[ \\frac{1}{2} F^2 = c \\left( \\xi F^{\\prime} -  2 F \\right) + \\mathrm{const}\\]\nBy applying the boundary conditions we find the constant of integration is zero, thus\n\\[ c \\xi F^{\\prime} = \\frac{1}{2} F^2 +  2c F \\]\nWhich is separable\n\\[ \\int { d \\xi \\over \\xi} = \\int { c \\over {\\frac{1}{2} F^2 +  2c F} } dF \\]\nIntegrating one last time\n\\[ \\log \\left( C_1 \\xi \\right) = \\frac{1}{2} \\log \\left( F \\over F + 4 c \\right) \\]\nWhere C1 is an undetermined constant of integration. Re-arranging and solving for F we arrive at\n\\[ F\\left( \\xi \\right) = { 4 c C_1 \\xi^2 \\over {1 - C_1 \\xi^2 } } \\]\nA common substitution is \\(C_1 = - \\left( C_2 \\over 2 \\right)^2\\) then\n\\[ F\\left( \\xi \\right) = { - c \\left( C_2 \\xi \\right)^2 \\over {1 + \\frac{1}{4} \\left( C_2 \\xi \\right)^2 } } \\]\nWhat we need, for the velocity profile, is the first derivative of F, which is\n\\[ F^{\\prime}\\left( \\xi \\right) = { - 2 c C_2^2 \\xi \\over \\left( 1 + \\left( C_2 \\xi \\over 2 \\right)^2 \\right)^2 } \\]\nand finally\n\\[ \\bar{v}_z = -{k \\over z} {F^{\\prime} \\over \\xi} \\]\n\\[ = -{k \\over z} { 1 \\over \\xi }{ - 2 c C_2^2 \\xi \\over \\left( 1 + \\left( C_2 \\xi \\over 2 \\right)^2 \\right)^2 } \\]\n\\[ = {2 \\varepsilon C_2^2 \\over z} \\left( 1 + \\left( C_2 \\xi \\over 2 \\right)^2 \\right)^{-2} \\]\n\\[ f \\left( \\xi \\right) = { \\bar{v}_z \\over \\bar{v}_{z,max} } =  \\left( 1 + \\left( C_2 \\xi \\over 2 \\right)^2 \\right)^{-2} \\]\n\nf_ev(ξ; C₂=15.1) = ( 1 + (C₂*ξ/2)^2 )^-2\n\n\n\nWidth at Half Height\nSince we have a convenient closed form for the velocity profile, we can calculate what the parameter \\(C_2\\) is in terms of the width at half height rather easily.\n\\[ f(\\xi) =  \\left( 1 + \\left( C_2 \\xi \\over 2 \\right)^2 \\right)^{-2} \\]\n\\[ \\frac{1}{2} =  \\left( 1 + \\left( {C_2 \\over 2} { b_{1/2} \\over z }\\right)^2 \\right)^{-2} \\]\n\\[ C_2 = 2 \\sqrt{\\sqrt{2}-1} \\frac{z}{ b_{1/2} } \\]\nusing the same parameterization as above we get\n\nC₂ = 2*√(√(2)-1)/b_half\n\n15.179109738339214\n\n\n\n\nVelocity Profile\nReturning to the momentum balance, we need to solve the integral:\n\\[ I = \\int_{0}^{\\infty} f\\left( \\xi \\right)^2 \\xi d \\xi\\\\\n= \\int_{0}^{\\infty} \\xi \\left( 1 + \\left( C_2 \\xi \\over 2 \\right)^2 \\right)^{-4} d\\xi \\]\nWhich can be integrated to give \\[ I = {2 \\over 3} C_2^{-2} \\]\nand finally\n\\[ k = \\sqrt{ 3 \\over 16 } C_2 v_0 d_0 \\]\nwith the velocity profile as\n\n\n\\[ \\bar{v}_z = 6.57 { v_0 d_0 \\over z} \\left( 1 + 57.6 \\xi^2 \\right)^{-2} \\]"
  },
  {
    "objectID": "posts/turbulent_jet_notes/index.html#empirical-velocity-profiles",
    "href": "posts/turbulent_jet_notes/index.html#empirical-velocity-profiles",
    "title": "Turbulent Jets",
    "section": "Empirical Velocity Profiles",
    "text": "Empirical Velocity Profiles\nPerhaps the most widely used turbulent jet model is simply an empirical gaussian fit to the data. These are easy to use – no solving of ODEs required – and fitting them to data is relatively straight forward. There is no real theoretical basis that I am aware of, merely based on the observation that a gaussian function fits the velocity profile well.\n\\[ f \\left( \\xi \\right) = \\exp \\left( -c \\xi^2 \\right) \\]\nWhere c is a parameter determined by fitting to a dataset.\n\nf_emp(ξ; c=72) = exp(-c*ξ^2)\n\n\nWidth at Half Height\nSince we have a convenient closed form for the velocity profile, we can calculate what the parameter \\(c\\) is in terms of the width at half height rather easily\n\\[ f(\\xi) =  \\exp \\left( -c \\xi^2 \\right) \\]\n\\[ \\frac{1}{2} =  \\exp \\left( - c  \\left( \\frac{ b_{1/2} }{z} \\right)^2 \\right) \\]\n\\[ c = \\ln \\left( 2 \\right) \\left( \\frac{z}{ b_{1/2} } \\right)^2 \\]\nusing the same parameterization as above we get\n\nc = log(2)/b_half^2\n\n96.39039423504045\n\n\n\n\nVelocity Profile\nReturning to the momentum balance, we need to solve the integral:\n\\[ I = \\int_{0}^{\\infty} f\\left( \\xi \\right)^2 \\xi d \\xi\\\\\n= \\int_{0}^{\\infty} \\xi \\exp \\left( -2 c \\xi^2 \\right) d\\xi \\]\nWhich can be integrated to give\n\\[ I = {1 \\over 4 c} \\]\nand finally\n\\[ k = \\sqrt{ c \\over 2 } v_0 d_0 \\]\nwith the velocity profile as\n\n\n\\[ \\bar{v}_z = 6.94 { v_0 d_0 \\over z} \\exp\\left( -193.0 \\xi^2 \\right) \\]"
  },
  {
    "objectID": "posts/turbulent_jet_notes/index.html#comparing-the-models",
    "href": "posts/turbulent_jet_notes/index.html#comparing-the-models",
    "title": "Turbulent Jets",
    "section": "Comparing the Models",
    "text": "Comparing the Models\nAt this point two models of velocity were derived using different models of the free turbulent stress and one purely empirical model was introduced. Each of these models uses a different set of parameters, and have different strengths and weaknesses in terms of usability. To compare them like-for-like we can scale each to the same width at half height, which is shown below along with some measured data14\n14 Pope, Turbulent Flows, points captured from a figure using WebPlotDigitizer.We can also calculate a Mean Square Error (MSE) and evaluate which model is a better fit to the observed velocity profile.\n\n\n\n\n\n\n\n\nFigure 4: Comparing all three turbulent jet models to the observed velocity profile.\n\n\n\n\n\nPrandtl Mixing Length Model MSE 0.00051\nEddy Viscosity Model        MSE 0.00092\nGaussian (empirical) Model  MSE 0.00054\n\n\nInterestingly the Prandtl mixing length model works the best, though the gaussian fit is close enough as to be essentially the same given this data set. Which is convenient as a gaussian fit is easier to work with. The eddy viscosity model is the easiest to derive, however it clearly does not work as well for the outer parts of the jet.\nThe above approach, the one you will most likely see in the literature, compares each model scaled to the same height and width. Which is sensible if one is planning on fitting data, and allowing that the height and width to be free parameters. However we know, from the analysis above, that the height of each model is dependent upon the width, so might be instructive to look at how that plays out in practice.\nSuppose we are looking at a velocity profile far enough downstream to be in the fully developed flow, say \\(z = 7 d_0\\)\n\n\n\n\n\n\n\n\nFigure 5: A comparison of the three turbulent jet models with an identical half-width, with the height calculated from the momentum balance.\n\n\n\n\n\nNote that in the region near the center-line the three models are no longer particularly close to one another and the eddy viscosity and prandtl mixing length models have changed places. Relative to the predicted \\(v_{max}\\) the the eddy viscosity model stays high when compared to the prandtl mixing length model, however the eddy viscosity model predicts a lower \\(v_{max}\\) such that the effect is entirely reversed.\nIt’s also worth noting that the gaussian fit and the prandtl mixing length model track one another reasonably well. I have seen a gaussian fit of the Tollmien tabulated results used in some papers when a smooth interpolation of the intermediate values is required and this suggests that may not be a bad idea. Though, to me, just solving the ode is easier. On a modern machine it takes milliseconds or less and a good ode package like DifferentialEquations.jl provides a higher-order interpolation for free.\nThis comparison has been done with each of the model parameters set based on a shared width. However there are as many different ways of arriving at the model parameters as there are datasets to fit against. There is a wide spread in tabulated values in the literature and so the predictions of two independently arrived at models can be quite different due all of these factors coming together."
  },
  {
    "objectID": "posts/turbulent_jet_notes/index.html#where-to-go-from-here",
    "href": "posts/turbulent_jet_notes/index.html#where-to-go-from-here",
    "title": "Turbulent Jets",
    "section": "Where to go from here",
    "text": "Where to go from here\nAll of this work was to determine the velocity field, which is not necessarily what anyone cares about. In a release scenario, for example, it is concentration that is most relevant. For a heat transfer application, perhaps, you may care about the temperature field instead. However, with the velocity field the concentrations, temperatures, total entrained flow, etc. can be easily derived."
  },
  {
    "objectID": "posts/turbulent_jet_notes/index.html#references",
    "href": "posts/turbulent_jet_notes/index.html#references",
    "title": "Turbulent Jets",
    "section": "References",
    "text": "References\n\n\nBird, R. Byron, Warren E. Stewart, and Edwin N. Lightfoot. Transport Phenomena. 2nd ed. Hoboken, NJ: John Wiley & Sons, 2007.\n\n\nGarde, R. J. Turbulent Flows. 3rd ed. London: New Academic Science, 2010.\n\n\nPope, Stephen B. Turbulent Flows. Cambridge: Cambridge University Press, 2000.\n\n\nRajaratnam, N. Turbulent Jets. Amsterdam: Elsevier, 1974.\n\n\nTollmien, Walter. “Berechnung Turbulenter Ausbreitungsvorgänge.” Zeitschrift Für Angewandte Mathematik Und Mechanik 6 (1926): 468–78. https://doi.org/10.1002/zamm.19260060604reprinted and translated in NACA-TM-1085."
  },
  {
    "objectID": "posts/fugitive-hydrogen/index.html",
    "href": "posts/fugitive-hydrogen/index.html",
    "title": "Estimating the impact of fugitive emissions",
    "section": "",
    "text": "As Alberta continues down it’s path to the hydrogen economy, with more industrial facilities transitioning to hydrogen as a fuel, and more producers of hydrogen announcing new plants and expansions, questions around the impact of fugitive hydrogen emissions linger."
  },
  {
    "objectID": "posts/fugitive-hydrogen/index.html#the-climate-impacts-of-fugitive-hydrogen",
    "href": "posts/fugitive-hydrogen/index.html#the-climate-impacts-of-fugitive-hydrogen",
    "title": "Estimating the impact of fugitive emissions",
    "section": "The climate impacts of fugitive hydrogen",
    "text": "The climate impacts of fugitive hydrogen\nHydrogen is not, itself, a greenhouse gas, in the sense that hydrogen does not significantly absorb infrared radiation. However hydrogen does have a significant global warming potential. Hydrogen influences chemical processes in the atmosphere that impact other greenhouse gases. In particular hydrogen preferentially reacts with oxidants in the air, oxidants that would otherwise be available to oxidize methane, leading to methane having a longer lifetime in the atmosphere. It also increases tropospheric ozone, both an important actor in ground-level pollution and a greenhouse gas.1 There has been increased recognition of this in the literature,2 as there are growing plans to transition many sectors of the economy to hydrogen. But this concern has not, as of yet, lead to hydrogen being listed on the standard tables of greenhouse gases used for emissions reporting, national inventories, and, importantly, “carbon tax” programs.3 As a consequence I haven’t seen a lot of effort, from industry, to quantify the climate impact of switching to hydrogen due to those fugitive emissions. Typical modeling of a hydrogen transition project (i.e. transitioning from natural gas to hydrogen as a fuel source for combustion) focuses on the combustion products and, if there is any attention paid to fugitive emissions, it is to claim that fugitive emissions will “disappear” as hydrogen “is not a greenhouse gas”.\n1 Sand et al., “A Multi-Model Assessment of the Global Warming Potential of Hydrogen,” 2.2 Dutta et al., “The Role of Fugitive Hydrogen Emissions in Selecting Hydrogen Carriers”; Bertagni et al., “Risk of the Hydrogen Economy for Atmospheric Methane”; Ocko and Hamburg, “Climate Consequences of Hydrogen Emissions”.3 Hydrogen is not listed on the most recent IPCC table of greenhouse gases, Smith et al., “The Earth’s Energy Budget, Climate Feedbacks, and Climate Sensitivity Supplementary Material”, Table 7.SM.7.\nHydrogen is also not listed in Schedule 1 of the Technology Innovation and Emissions Reduction Regulation, AR 133/2019, which is the industrial “carbon tax” in Alberta.4 Hydrogen GWP100 from Sand et al., “A Multi-Model Assessment of the Global Warming Potential of Hydrogen” page 5. Methane GWP100 is that for fossil fuel derived methane from Forster et al., “The Earth’s Energy Budget, Climate Feedbacks, and Climate Sensitivity” page 1017.Taking the broader view of hydrogen’s impact on atmospheric chemistry, it has a GWP100 of 11.6 as compared to the methane’s GWP100 of 29.84 and so, assuming similar leak rates, one would expect that a transition from natural gas (primarily methane) to hydrogen would lead to a reduction in overall climate impact. Though this is also another point towards hydrogen not actually being a zero emissions fuel."
  },
  {
    "objectID": "posts/fugitive-hydrogen/index.html#a-first-look-at-estimating-leak-rates",
    "href": "posts/fugitive-hydrogen/index.html#a-first-look-at-estimating-leak-rates",
    "title": "Estimating the impact of fugitive emissions",
    "section": "A first look at estimating leak rates",
    "text": "A first look at estimating leak rates\nThe first time this question landed on my desk it was related to a project to transition a large petrochemical facility from natural gas to hydrogen fuel gas. I did some back of the envelope calculations to estimate the climate impact, in CO2-e, of hydrogen fugitive emissions from this system with a few basic assumptions:\n\nNatural gas is entirely methane and the hydrogen fuel gas is pure hydrogen.\nMethane and hydrogen are ideal gases\nFugitive emissions all come from leaks, which are just holes in the pressure envelope\nThe system pressure is high enough that flow is choked\n\nThe first assumption is not as close as you might think, at least in this part of Alberta, the utility natural gas to the site is ~90% (mol) methane (that the hydrogen is essentially pure was a much closer approximation in this case). The second assumption is probably closer, though it will depend on the actual line pressure, it is something of a joke among chemical engineers that all gases are ideal gases unless we’re absolutely forced to do it otherwise.\nThe third assumption is at least superficially reasonable, here I am imagining leaks at flanges to be basically holes in the gaskets, gaps due to misaligned fittings, or possibly pinhole leaks in the metal itself (hopefully less likely, though that depends on how seriously you take mechanical integrity). The standard way of estimating flow from a hole or orifice uses a discharge coefficient cD which is a function of geometry and not the gas moving through it.\nThe other main component of fugitive emissions from this system would be low level venting, typically seen when burners start and stop. During start-up some volume of fuel gas is purged before the burner actually lights and similarly a small volume leaks out after the burner is turned off. For some systems, where the burners are starting and stopping frequently, this can be a major component of fugitive emissions. I’m choosing to neglect those, or consider those part of stack emissions.\nThe fourth assumption is pretty reasonable for the fuel gas distribution system at an industrial facility, where the line pressures are relatively high. This means that the leak rate for any given hole is independent of the system pressure and the flow will be turbulent.\nPulling these together and assuming that for any given leak in the distribution network the mass flow is given by the equation for an ideal gas through an isentropic nozzle:\n\\[  \\dot{m} = c_d A_h \\sqrt{ \\rho_1 P_1 k \\left( 2 \\over k+1 \\right)^{k+1 \\over k-1} } \\]\nThe ratio of mass flow of hydrogen to that of methane is then:\n\\[  {\\dot{m}_{H2} \\over \\dot{m}_{CH4} } = \\sqrt{ {\\rho_{H2} \\over \\rho_{CH4}} {P_{1,H2} \\over P_{1,CH4}} { {k_{H2} \\left( 2 \\over k_{H2}+1 \\right)^{k_{H2}+1 \\over k_{H2}-1} } \\over {k_{CH4} \\left( 2 \\over k_{CH4}+1 \\right)^{k_{CH4}+1 \\over k_{CH4}-1} } } }\\]\nAssuming the system pressure, P1, after having switched to hydrogen, is the same as the system pressure when operating natural gas.\n\\[  {\\dot{m}_{H2} \\over \\dot{m}_{CH4} } = \\sqrt{ {\\rho_{H2} \\over \\rho_{CH4}} { {k_{H2} \\left( 2 \\over k_{H2}+1 \\right)^{k_{H2}+1 \\over k_{H2}-1} } \\over {k_{CH4} \\left( 2 \\over k_{CH4}+1 \\right)^{k_{CH4}+1 \\over k_{CH4}-1} } } }\\]\nFor a system delivering fuel gas there is a good reason to assume this as the system will deliver approximately the same energy (in terms of HHV) when operated at the same pressure (pure methane versus pure hydrogen). Though this is worth keeping in mind as the hydrogen line can operate at slightly lower pressures while delivering the same heating value, which also reduces the leak rate. This effect is small at low and moderate pressures but could be important at high pressures.\nBecause everything related to the particular hole and the conditions around it canceled out, we have gone from a relation for a single leak in a network to a relation that holds for the whole system. Since this was a back of the envelope calculation, I further assumed that as \\(k_{H2}\\) is within 10% of \\(k_{CH4}\\) then\n\\[  { {k_{H2} \\left( 2 \\over k_{H2}+1 \\right)^{k_{H2}+1 \\over k_{H2}-1} } \\over {k_{CH4} \\left( 2 \\over k_{CH4}+1 \\right)^{k_{CH4}+1 \\over k_{CH4}-1} } } \\approx 1 \\]\nand thus\n\\[  {\\dot{m}_{H2} \\over \\dot{m}_{CH4} } = \\sqrt{ {\\rho_{H2} \\over \\rho_{CH4}} } = \\sqrt{ {MW_{H2} \\over MW_{CH4}} } \\]\nputting this in terms of emissions in CO2-e, with \\(E_i = GWP_i \\cdot \\dot{m}_i\\)\n\\[ { E_{H2} \\over E_{CH4} } = { {GWP}_{H2} \\over {GWP}_{CH4} } \\sqrt{ MW_{H2} \\over MW_{CH4} } \\approx \\frac{12}{30} \\sqrt{ \\frac{2}{16} } \\approx 0.13 \\]\nand so we expect a ~87% reduction in fugitive emissions (in CO2-e) after having transitioned the system from natural gas to hydrogen.\nSince I’m now sitting in front of a computer, I can loosen off some of the aggressive approximations, using gas properties from Crane’s.5\n5 Crane, “TP410M”.\nusing Unitful\n\n# GWPs: Forster et al. \"The Earth's Energy Budget,\" 1017.\n#       Sand et al. \"Multi Model Assessment,\" 5.\n#\n# Fluid properties: Crane's *Flow of Fluids*, A-6 and A-9\n\n# Methane\nGWP_CH4 = 29.8  # t-CO2e/t\nMW_CH4 = 16.043u\"g/mol\"\nμ_CH4 = 0.01103u\"cP\" # at 20°C\nk_CH4 = 1.31\n\n# Hydrogen\nGWP_H2 = 11.6   # t-CO2e/t\nMW_H2 = 2.016u\"g/mol\"\nμ_H2 = 0.008804u\"cP\" # at 20°C\nk_H2 = 1.41\n\n\ng(k) = k*(2/(k+1))^((k+1)/(k-1))\n\nE_H2 = GWP_H2*√(MW_H2*g(k_H2))\nE_CH4 = GWP_CH4*√(MW_CH4*g(k_CH4))\n\nE_H2/E_CH4\n\n0.1415674991761294\n\n\nI assumed, above, that the fuel gas distribution system was at a high enough pressure for flow to be choked, but how high would that have to be? Choking flow for an isentropic nozzle is when\n\\[ {P_1 \\over P_2} \\lt \\left( 2 \\over {k+1} \\right)^{ -k \\over {k-1} } \\]\nwhere (1) is upstream of the nozzle and (2) is downstream of the jet, in this case atmospheric pressure since the leaks are all to atmosphere. From this we can back calculate the critical system pressure above which all jets are choked.\n\n# choking condition\nη_c(k) = (2/(k+1))^(-k/(k-1))\n\nP₂ = 101.325u\"kPa\" # atmospheric pressure\n\nP₁(k) = η_c(k)*P₂\n\nPₘᵢₙ = min(P₁(k_H2),P₁(k_CH4))\n\n186.28417600555758 kPa\n\n\nor in terms of psi (absolute)\n\nuconvert(u\"psi\",Pₘᵢₙ)\n\n27.018235462782194 psi\n\n\nSystem pressures for the fuel gas distribution networks within chemical plants within them are often above 100psia, though by the time this has been stepped down to a burner it can be around 25psia. This is quite different from the operating pressures of the distribution network to residential customers, where typical pressures are in the range of 0.1-0.4psig.6\n6 For plant piping I have no references that are not confidential to the companies I have worked for, so I guess you’ll just have to trust me. For the residential distribution network see Mejia, Brouwer, and Kinnon, “Hydrogen Leaks at the Same Rate as Natural Gas in Typical Low-Pressure Gas Infrastructure” page 8815."
  },
  {
    "objectID": "posts/fugitive-hydrogen/index.html#leaks-as-a-series-of-tubes",
    "href": "posts/fugitive-hydrogen/index.html#leaks-as-a-series-of-tubes",
    "title": "Estimating the impact of fugitive emissions",
    "section": "Leaks as a series of tubes",
    "text": "Leaks as a series of tubes\nAfter getting a general sense of how I would expect fugitive emissions to change, I spent some time looking for more specific data, in particular measured performance of actual systems. In industrial settings, actual leak data from systems in hydrogen service is available. Hydrogen has been a common industrial gas for over a century. However the relevant question is not “what are the fugitive emissions from a system designed for hydrogen service?” it is the subtly different question “what are the fugitive emissions from a system designed for natural gas service, but operating in hydrogen service?”. Maybe switching from natural gas to hydrogen will lead to a system that leaks like a sieve with hydrogen leaking from fittings that would otherwise be gas-tight.\nThe literature is pretty consistent that this is not the case. Hydrogen leaks from fuel gas systems switched over from natural gas at rates that are entirely consistent with what you would expect, given the differences in density and viscosity.7 What is different, from my analysis, is the model of fluid flow primarily used in the literature.\n7 Mejia, Brouwer, and Kinnon; Swain and Swain, “A Comparison of \\(H_2\\), \\(CH_4\\) and \\(C_3 H_8\\) Fuel Leakage in Residential Settings”; Schefer et al., “Characterization of Leaks from Compressed Hydrogen Dispensing Systems and Related Components”.I assumed all leaks would be essentially turbulent flow, through a nozzle, using a modified Bernoulli equation. That model works well for large, macroscopic, jets of gases much like what one typically encounters when modeling leaks of process safety relevance. However most fugitive emissions are not big jets of gas like that, somebody would notice that and get it fixed. Fugitive emissions from flanges and fittings come through minuscule gaps in the pressure envelope that involve flow paths that are longer than they are wide, more analogous to pipe flow. Thus the model of fluid flow more commonly seen in the literature treats leaks like a series of tiny, tortuous, tubes.\nStarting from the Darcy-Weisbach equation, in terms of the Fanning friction factor, f, for incompressible flow\n\\[ \\Delta P = 2 f \\frac{L}{D} \\rho u^2 \\]\n\\[ u = \\sqrt{ {\\Delta P D} \\over {2 \\rho f L} } \\]\nThe volumetric flow, Q, would be\n\\[ Q = \\frac{\\pi}{4} u D^2 \\]\n\\[ Q = \\frac{ \\sqrt{2} }{8} \\pi \\sqrt{  {\\Delta P D^5} \\over {2 \\rho f L} } \\]\nwhere ΔP is the pressure drop, D the hydraulic diameter, L the effective length and ρ the density. The relative leak rate is then the volumetric flow for hydrogen over that for methane\n\\[ { Q_{H2} \\over Q_{CH4} } = \\sqrt{ { \\rho_{CH4} \\over \\rho_{H2} } { f_{CH4} \\over f_{H2} } }\\]\nThis is the typical starting point in the literature. If we assume fully developed turbulent flow, f is a constant and independent of the Reynolds number, then (for ideal gases)\n\\[ { Q_{H2} \\over Q_{CH4} } = \\sqrt{ { \\rho_{CH4} \\over \\rho_{H2} } }  = \\sqrt{ MW_{CH4} \\over MW_{H2} }\\]\nIf we assume laminar flow \\(f = \\frac{16}{ \\mathrm{Re} }\\) and\n\\[ { Q_{H2} \\over Q_{CH4} } = \\sqrt{ { \\rho_{CH4} \\over \\rho_{H2} } { \\mathrm{Re}_{H2} \\over \\mathrm{Re}_{CH4} } } \\]\nFor pipe-flow \\(\\mathrm{Re} = \\frac{4}{\\pi} { { \\rho Q } \\over { \\mu D } }\\), which after substitution gives\n\\[ { Q_{H2} \\over Q_{CH4} } = \\sqrt{ { Q_{H2} \\over Q_{CH4} } { \\mu_{CH4} \\over \\mu_{H2} } } \\]\nand, after squaring both sides and canceling\n\\[ { Q_{H2} \\over Q_{CH4} } = { \\mu_{CH4} \\over \\mu_{H2} } \\]\nThese two equations are the ultimate source for most of the bounds given on the relative leak-rate of hydrogen fugitives versus natural gas fugitives.\n\nturbulent_leak_ratio = √(MW_CH4/MW_H2)\n\n2.8209638958319374\n\n\n\nlaminar_leak_ratio = μ_CH4/μ_H2\n\n1.2528396183552932\n\n\nI think it is important to show where these numbers come from, in particular the assumptions that go into them, as I have seen these values – 1.2× to 2.8× the leak rate of methane/natural gas – used directly in relation to GWP100s and other measures that are on a mass basis. This is incorrect. These are the ratios for volumetric flow. Hydrogen has a density ~1/8th that of methane, the mass flow rate is much less for both the turbulent and laminar regimes.\nFor turbulent flow:\n\\[ { \\dot{m}_{H2} \\over \\dot{m}_{CH4} } = \\sqrt{ { \\rho_{H2} \\over \\rho_{CH4} } }  = \\sqrt{ MW_{H2} \\over MW_{CH4} }\\]\nand for laminar flow:\n\\[ { \\dot{m}_{H2} \\over \\dot{m}_{CH4} } = { \\rho_{H2} \\over \\rho_{CH4} } { \\mu_{CH4} \\over \\mu_{H2} }  = { MW_{H2} \\over MW_{CH4} } { \\mu_{CH4} \\over \\mu_{H2} } \\]\n\nturbulent_mass_ratio = √(MW_H2/MW_CH4)\n\n0.3544887623260728\n\n\n\nlaminar_mass_ratio = (MW_H2/MW_CH4)*(μ_CH4/μ_H2)\n\n0.1574346861936216\n\n\nThe mass emission ratio for the turbulent case is entirely what I came up with in my back of the envelope calculations, and I think you could extend this to include compressibility.8\n8 Schefer does this, replicating the same result as my model above, and goes further to provide a model for non-ideal gases that accounts for differences in compressibility factor, Schefer et al., “Characterization of Leaks from Compressed Hydrogen Dispensing Systems and Related Components” page 1251.9 Frazer-Nash Consultancy, “Fugitive Hydrogen Emissions in a Future Hydrogen Economy,” 25; Mejia, Brouwer, and Kinnon, “Hydrogen Leaks at the Same Rate as Natural Gas in Typical Low-Pressure Gas Infrastructure,” 8813–14; Swain and Swain, “A Comparison of \\(H_2\\), \\(CH_4\\) and \\(C_3 H_8\\) Fuel Leakage in Residential Settings,” 808.At this point we are drifting away from the original problem, the laminar regime is unlikely to occur at the high system pressures of typical transmission lines and plant fuel gas systems. We’ve basically just circled around to the answer I arrived at originally, but with more footnotes.9\n\nMolecular flow\nIt is worth noting that for very low system pressures, like what is seen with residential distribution lines, an entirely different flow regime is encountered. In these mechanically assembled piping systems, e.g. NPS piping, leaks are primarily through the gaps in the threads or mechanical joints. These gaps, due to manufacturing defects or damage, form micro channels that are small enough for the continuum hypothesis to breakdown and flow is in a molecular flow regime.10 In this case the volumetric leak rate is identical for both hydrogen and natural gas.\n10 Mejia, Brouwer, and Kinnon, “Hydrogen Leaks at the Same Rate as Natural Gas in Typical Low-Pressure Gas Infrastructure,” 8814–15.\nmolecular_flow_mass_ratio = MW_H2/MW_CH4\n\n0.12566228261547094"
  },
  {
    "objectID": "posts/fugitive-hydrogen/index.html#relative-importance-of-fugitive-emissions",
    "href": "posts/fugitive-hydrogen/index.html#relative-importance-of-fugitive-emissions",
    "title": "Estimating the impact of fugitive emissions",
    "section": "Relative importance of fugitive emissions",
    "text": "Relative importance of fugitive emissions\nFugitive emissions are generally small compared to combustion emissions for fossil fuels. The large majority of the emissions, in CO2 equivalents, is what is coming out of the stack. In the case of hydrogen, very little is coming out of the stack other than water and nitrous oxide. So it is worth checking to see how important, relatively, fugitive emissions have become.\nAs a first pass I am going to divide emissions into combustion and fugitive wherein the combustion emissions are the direct emissions of combustion products and the fugitive emissions are all the leaks in the entire system (burners included).\nMy model for fugitive emissions will be quite simple: some fraction η of flow is lost from the system and the emissions, in CO2 equivalents is\n\\[ E_f = GWP_{H2} \\cdot \\rho_{H2} \\cdot \\eta \\cdot Q_{H2} \\]\nWhen hydrogen undergoes combustion it produces water\n\\[ H_2 + \\frac{1}{2}O_2 \\rightarrow H_2O \\]\nSince there is no carbon in the fuel, no carbon dioxide is generated. Similarly, there is no possibility of generating methane through incomplete combustion. However nitrous oxide can be generated from any gaseous flame that uses air as a source of oxygen, though the chemistry of this process is complex.11 Thus the combustion emissions for hydrogen are\n11 Colorado, McDonell, and Samuelsen, “Direct Emissions of Nitrous Oxide from Combustion of Gaseous Fuels” lists 23 different reactions involved in the formation of N2O in gaseous flames..\\[ E_c = GWP_{N2O} \\cdot EF_{N2O} \\cdot HHV_{H2} \\cdot (1-\\eta) \\cdot Q_{H2} \\]\nWhere EF is the emission factor for nitrous oxide and HHV is the higher heating value of hydrogen.\nThe ratio of fugitive to combustion emissions is then12\n12 I am using the nitrous oxide emission factor for natural gas combustion, for lack of any more appropriate emission factor. This factor is highly dependent upon the actual burner design/operation, fuel gas, and host of other parameters relating to the actual stationary combustion device. I am implicitly assuming that whatever the nitrous oxide emission factor would be for hydrogen, it would be of the same order of magnitude as that for natural gas.\\[ { E_f \\over E_c } = { {GWP_{H2} \\cdot \\rho_{H2}} \\over { GWP_{N2O} \\cdot EF_{N2O} \\cdot HHV_{H2} } } \\cdot {\\eta \\over {1-\\eta}} \\]\n\nSG_H2 = 0.0696 # GPSA\nρ_air = 1.225u\"kg/m^3\"   # GPSA, at 15°C and 1atm\nρ_H2 = SG_H2*ρ_air\n\nGWP_N2O = 273            # Forster et al., 1017.\nEF_N2O = 8.7e-7u\"kg/MJ\"  # AEPA, 1-9 Industrial\nHHV_H2 = 12.102u\"MJ/m^3\" # GPSA, at 15°C and 1atm\n\nfugitives_to_combustion(η) = ((GWP_H2*ρ_H2)/(GWP_N2O*EF_N2O*HHV_H2))*(η/(1-η));\n\nAssuming that the leak rate is 1% we then have\n\nfugitives_to_combustion(0.01)\n\n3.4755942870304137\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: The ratio of fugitive emissions to combustion emissions, as a function of leakage rate.\n\n\n\n\nAt any appreciable leak percentage the amount of hydrogen lost to fugitive emissions rivals the stack emissions for climate impact."
  },
  {
    "objectID": "posts/fugitive-hydrogen/index.html#fugitive-hydrogen-and-net-zero",
    "href": "posts/fugitive-hydrogen/index.html#fugitive-hydrogen-and-net-zero",
    "title": "Estimating the impact of fugitive emissions",
    "section": "Fugitive hydrogen and “net zero”",
    "text": "Fugitive hydrogen and “net zero”\nMore relevant to a fuel switching program is to re-assess how much of a reduction switching to hydrogen achieves. Instead of comparing hydrogen to itself, we should compare hydrogen to the natural gas system that preceded it.\nFor the natural gas system the fugitive emissions are similar, except that I am assuming the only climate relevant component of natural gas is methane\n\\[ E_f = GWP_{CH4} \\cdot \\rho_{CH4} \\cdot x_{CH4} \\cdot \\eta \\cdot Q_{NG}\\]\nand the combustion emissions now include carbon dioxide and methane along with nitrous oxide\n\\[ E_c = \\left( GWP_{CO2} \\cdot EF_{CO2} + GWP_{CH4} \\cdot EF_{CH4} + GWP_{N2O} \\cdot EF_{N2O} \\right) \\left( 1 - \\eta \\right) Q_{NG} \\]\nTotal emissions are just \\(E_T = E_f + E_c\\).\nWhat we are interested in is the ratio\n\\[ { E_{T,H2} \\over E_{T,NG} } \\]\n\nSome more simplifying assumptions\nThere are a few assumptions we need to make to proceed. The first is to assume that the system with natural gas and the system with hydrogen are operating under the same pressure. At the same pressure the hydrogen system will deliver about the same energy in HHV as the natural gas system, slightly more (depending on the exact natural gas, etc.). Which makes this a plausible assumption. The whole point of the fuel delivery system is to deliver sufficient energy to a combustion device, in the form of fuel heating value. This is not exact, so a more detailed analysis would work out the actual pressure of the hydrogen system and that would add a whole layer of complication.\nThe second assumption is that the fraction of gas lost between the two systems is the same. At first blush this seems like a crazy assumption. I spent two sections talking about how significantly different the leak rates were, so what is going on here? Well the volumetric leak rate is higher with hydrogen but the line flow rate is also higher, and they are both higher by the same amount. It cancels out.\nSuppose the leaks are all in the turbulent regime, so\n\\[ {Q_{leak,H2} \\over Q_{leak,NG}} = \\sqrt{\\rho_{NG} \\over \\rho_{H2}} \\]\nFor fully developed turbulent pipe flow we know the ratio of line flow rates is also\n\\[ { Q_{H2} \\over Q_{NG} }  = \\sqrt{\\rho_{NG} \\over \\rho_{H2}} \\]\nBy the definition of η\n\\[ \\eta_{H2} = { Q_{leak,H2} \\over Q_{H2} } = { Q_{leak,H2} \\over Q_{leak,CH4} } { Q_{CH4} \\over Q_{H2} } { Q_{leak,CH4} \\over Q_{CH4} } \\]\n\\[ \\eta_{H2} = \\sqrt{\\rho_{NG} \\over \\rho_{H2}} \\sqrt{\\rho_{H2} \\over \\rho_{NG}} \\eta_{CH4} \\]\n\\[ \\eta_{H2} = \\eta_{CH4} \\]\n\n\nRelative emissions of switching to hydrogen\nTo make the math a little less tedious to type out, I am going to define two emission factors, the fugitive emission factor13\n13 Note that the flowrates here are at standard state. The volumetric emission factors, heating values, and densities are also at standard state thus this is equivalent to the relation at actual conditions.\\[ EF_f = {E_f \\over Q_T} \\]\nand the combustion emission factor\n\\[ EF_c = {E_c \\over Q_T} \\]\nFinally we can answer the question of “how much do the total emissions go down after switching to hydrogen?”\n\\[ { E_{T,H2} \\over E_{T,NG} }  = { \\left[ EF_{c} ( 1 - \\eta ) + EF_{f} \\eta \\right]_{H2} \\over \\left[ EF_{c} ( 1 - \\eta ) + EF_{f} \\eta \\right]_{NG} } { Q_{H2} \\over Q_{NG} }\\]\n\\[ { E_{T,H2} \\over E_{T,NG} }  = { \\left[ EF_{c} ( 1 - \\eta ) + EF_{f} \\eta \\right]_{H2} \\over \\left[ EF_{c} ( 1 - \\eta ) + EF_{f} \\eta \\right]_{NG} } \\sqrt{ \\rho_{NG} \\over \\rho_{H2} }\\]\n\\[ { E_{T,H2} \\over E_{T,NG} }  = { \\left[ EF_{c} ( 1 - \\eta ) + EF_{f} \\eta \\right]_{H2} \\over \\left[ EF_{c} ( 1 - \\eta ) + EF_{f} \\eta \\right]_{NG} } \\sqrt{ {SG}_{NG} \\over {SG}_{H2} }\\]\n\n# hydrogen\nEF_f_H2 = GWP_H2*ρ_H2\nEF_c_H2 = GWP_N2O*EF_N2O*HHV_H2\n\n# methane\nSG_CH4 = 0.5539 # GPSA\nρ_CH4 = SG_CH4*ρ_air\n\n# natural gas\nx_CH4 = 0.90 # Alberta typical\nSG_NG = 0.61 # Alberta typical\nEF_CO2_NG = 1.962u\"kg/m^3\"  # ECCC, 3.\nEF_CH4_NG = 3.7e-5u\"kg/m^3\" # ECCC, 3.\nEF_N2O_NG = 3.3e-5u\"kg/m^3\" # ECCC, 3.\n\nEF_f_NG = GWP_CH4*ρ_CH4*x_CH4\nEF_c_NG = EF_CO2_NG + GWP_CH4*EF_CH4_NG + GWP_N2O*EF_N2O_NG\n\n# Final answer\nemissions_ratio(η) = ((EF_c_H2*(1-η)+EF_f_H2*η)/(EF_c_NG*(1-η)+EF_f_NG*η))*√(SG_NG/SG_H2);\n\n\nemissions_ratio(0.01)\n\n0.017665064441514864\n\n\nSo switching to hydrogen has reduced the overall emissions from this system by ~98.2%. Which is pretty significant, though it is not zero even though this analysis is assuming pure hydrogen.\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: The total emissions, in CO2-e, of hydrogen relative to natural gas."
  },
  {
    "objectID": "posts/fugitive-hydrogen/index.html#final-thoughts",
    "href": "posts/fugitive-hydrogen/index.html#final-thoughts",
    "title": "Estimating the impact of fugitive emissions",
    "section": "Final thoughts",
    "text": "Final thoughts\nEven at relatively high leak rates, the total greenhouse gas emissions from a hydrogen system are a small fraction of that of a natural gas system. Transitioning to hydrogen does what you would expect: it radically reduces the climate impact of stationary combustion equipment. That said, it is not zero emissions. Which shifts the perspective on where hydrogen fits in the energy transition. If the goal is zero then hydrogen will not get us there by the simple fact that hydrogen has a significant global warming potential and fugitive emissions are unavoidable. If the goal is to radically decarbonize existing systems and run out the remaining life of a vast global fleet of process equipment, then transitioning to hydrogen may be a major player.\nHydrogen may also be limited by the fact that it is not a zero impact fuel with regards to all of the other air emissions that are more locally important, such as nitrogen oxides (NOx), VOCs, and ground level ozone. Hydrogen combustion does directly produce nitrogen oxides and direct hydrogen emissions impact atmospheric chemistry increasing VOC and ground level ozone concentrations. If the choice is between hydrogen combustion and electrification, well electrification actually is zero emissions – both greenhouse gas emissions as well as other air pollutants – and while electrification projects are more complex than hydrogen as a “drop-in” solution, that can be a pretty strong advantage. For example in airsheds that are already stressed for NOx, switching to hydrogen fuel gas may also require the installation post-combustion NOx reduction technology such as SCR, as hydrogen combustion generally produces more NOx than natural gas. Replacing stationary combustion equipment with their electric equivalents has the advantage that it reduces all air emissions."
  },
  {
    "objectID": "posts/fugitive-hydrogen/index.html#references",
    "href": "posts/fugitive-hydrogen/index.html#references",
    "title": "Estimating the impact of fugitive emissions",
    "section": "References",
    "text": "References\n\n\nAlberta Greenhouse Gas Quantification Methodologies (version 2.3). Edmonton, AB: Alberta Environment; Protected Areas, 2023. https://open.alberta.ca/publications/alberta-greenhouse-gas-quantification-methodologies.\n\n\nBertagni, Matteo B., Stephen W. Pacala, Fabien Paulot, and Amilcare Porporato. “Risk of the Hydrogen Economy for Atmospheric Methane.” Nature Communications 13 (2023). https://doi.org/10.1038/s41467-022-35419-7.\n\n\nColorado, Andrés, Vincent McDonell, and Scott Samuelsen. “Direct Emissions of Nitrous Oxide from Combustion of Gaseous Fuels.” International Journal of Hydrogen Energy 42, no. 1 (2017): 711–19. https://doi.org/10.1016/j.ijhydene.2016.09.202.\n\n\nCrane. “TP410M: Flow of Fluids.” Stamford, CT: Crane, 2013.\n\n\nDutta, Indranil, Rajesh Kumar Parsapur, Sudipta Chatterjee, Amol M. Hengne, Davin Tan, Karthik Peramaiah, Theis I. Solling, Ole John Nielsen, and Kuo-Wei Huang. “The Role of Fugitive Hydrogen Emissions in Selecting Hydrogen Carriers.” ACS Energy Letters 8, no. 7 (2023): 3251–57. https://doi.org/10.1021/acsenergylett.3c01098.\n\n\nEmission Factors and Reference Values (version 1.1). Gatineau, QC: Environment; Climate Change Canada, 2023. https://publications.gc.ca/collections/collection_2023/eccc/En84-294-2023-eng.pdf.\n\n\nForster, Piers, Trude Storelvmo, Kyle Armour, William Collins, Jean-Louis Dufresne, David Frame, Daniel J. Lunt, et al. “The Earth’s Energy Budget, Climate Feedbacks, and Climate Sensitivity.” In Climate Change 2021: The Physical Science Basis. Contribution of Working Group i to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change, edited by Valérie Masson-Delmonte, Panmao Zhai, Anna Pirani, Sarah L. Connors, Clotilde Péan, Yang Chen, Leah Goldfarb, et al., 923–1054. Cambridge: Cambridge University Press, 2023.\n\n\nFrazer-Nash Consultancy. “Fugitive Hydrogen Emissions in a Future Hydrogen Economy.” London, UK: UK Department for Business, Energy,; Industrial Strategy, 2022. https://www.gov.uk/government/publications/fugitive-hydrogen-emissions-in-a-future-hydrogen-economy/.\n\n\nGPSA. Engineering Data Book. 13th ed. Tulsa, OK: Gas Processors Suppliers Association, 2012.\n\n\nMasson-Delmonte, Valérie, Panmao Zhai, Anna Pirani, Sarah L. Connors, Clotilde Péan, Yang Chen, Leah Goldfarb, et al., eds. Climate Change 2021: The Physical Science Basis. Contribution of Working Group i to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change. Cambridge: Cambridge University Press, 2023.\n\n\nMejia, Alejandra Hormaza, Jacob Brouwer, and Michael Mac Kinnon. “Hydrogen Leaks at the Same Rate as Natural Gas in Typical Low-Pressure Gas Infrastructure.” International Journal of Hydrogen Energy 45, no. 15 (2020): 8810–25. https://doi.org/10.1016/j.ijhydene.2019.12.159.\n\n\nOcko, Ilissa B., and Steven P. Hamburg. “Climate Consequences of Hydrogen Emissions.” Atmospheric Chemistry and Physics 22, no. 14 (2022): 9349–68. https://doi.org/10.5194/acp-22-9349-2022.\n\n\nSand, Maria, Ragnhild Bieltvedt Skeie, Marit Sandstad, Srinath Krishnan, Gunnar Myhre, Hannah Bryant, Richard Derwent, et al. “A Multi-Model Assessment of the Global Warming Potential of Hydrogen.” Communications Earth & Environment 4 (2023): 203. https://doi.org/10.1038/s43247-023-00857-8.\n\n\nSchefer, R. W., W. G. Houf, C. San Marchi, W. P. Chernicoff, and L. Englom. “Characterization of Leaks from Compressed Hydrogen Dispensing Systems and Related Components.” International Journal of Hydrogen Energy 31, no. 9 (2006): 1247–60. https://doi.org/10.1016/j.ijhydene.2005.09.003.\n\n\nSmith, Chris, Zebedee R. J. Nicholls, Kyle Armour, William Collins, Piers Forster, Malte Meinshausen, Matthew D. Palmer, et al. “The Earth’s Energy Budget, Climate Feedbacks, and Climate Sensitivity Supplementary Material.” In Climate Change 2021: The Physical Science Basis. Contribution of Working Group i to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change, edited by Valérie Masson-Delmonte, Panmao Zhai, Anna Pirani, Sarah L. Connors, Clotilde Péan, Yang Chen, Leah Goldfarb, et al. Cambridge: Cambridge University Press, 2023.\n\n\nSwain, M. R., and M. N. Swain. “A Comparison of \\(H_2\\), \\(CH_4\\) and \\(C_3 H_8\\) Fuel Leakage in Residential Settings.” International Journal of Hydrogen Energy 17, no. 10 (1992). https://doi.org/10.1016/0360-3199(92)90025-R."
  },
  {
    "objectID": "posts/gaussian_dispersion_example/index.html",
    "href": "posts/gaussian_dispersion_example/index.html",
    "title": "Air Dispersion Example - Gaussian Dispersion Model of Stack Emissions",
    "section": "",
    "text": "This is an interesting example that came up in conversation with another engineer related to a construction project happening at an existing facility. Imagine construction involving scaffolding and workers at an elevation that potentially puts them within the plume of an existing stack – say from an adjacent boiler. If the facility is still operating while this construction work happens then it is possible that workers will be exposed to combustion products in excess of the occupational exposure limits. The operating boiler does not have to be all that close by for the plume – which is very visible this time of year in the cold weather – to envelope a similarly tall set of scaffolding.\nSo, how would one determine whether or not the operating stack presents a hazard to the workers? In practice by hiring a consultant to do detailed modelling, because safety issues like this are not the time to pencil-whip some number. But we may want to come up with a rough estimate regardless, and for that a Gaussian dispersion model of the stack can be a useful first start."
  },
  {
    "objectID": "posts/gaussian_dispersion_example/index.html#the-scenario",
    "href": "posts/gaussian_dispersion_example/index.html#the-scenario",
    "title": "Air Dispersion Example - Gaussian Dispersion Model of Stack Emissions",
    "section": "The Scenario",
    "text": "The Scenario\nSuppose a natural gas boiler with a rated capacity, \\(W\\), of 300GJ/h, stack height, \\(h_s\\), of 10m and diameter, \\(D_s\\) of 2m, and an exit temperature of 450K.\nIn this case we are interested in the carbon monoxide concentrations at a work platform at the same height as the stack and 100m away, we also would like to know the concentration for a worker at ground level. I am approximately 2m tall let’s suppose the relevant height is 2m (for anyone shorter than that the concentration should be lower and thus this is conservative).\nAdditionally I am assuming ambient conditions of 25°C and 1atm\n\nusing Unitful\n\nW = uconvert(u\"GJ/s\", 300u\"GJ/hr\")\nhₛ = 10u\"m\"   # stack height\nDₛ = 2u\"m\"    # stack diameter\nTₛ = 450u\"K\"  # stack exit temperature\n\nh₁ = hₛ      # height of platform, m\nx₁ = 100u\"m\" # distance to platform, m\n\npₐ = 101.325u\"kPa\" # ambient pressure, 1atm\nTₐ = 298.15u\"K\"    # ambient temperature, 25°C\n\n298.15 K\n\n\nPrior to any dispersion modelling, the following parameters need to be collected:\n\nthe mass emission rate of the species, carbon monoxide, in kg/s\nthe concentration of interest, in this case the occupational exposure limit of carbon monoxide in kg/m3\nthe wind speed and atmospheric stability\nthe effective stack height, in m\n\n\nMass Emission Rate\nThe EPA has tabulated emission factors for most combustion products in EPA AP-42 and for a natural gas boiler it is 84 lb/10^6 SCF1 with a reference higher heating value of 1020 MMBTU/10^6 SCF.\n1 EPA, “AP 42” Table 1.4-1. The emission factor is relative to the volume of natural gas consumed not the volume of stack gas emitted.If we suppose the boiler is operating at max rates then the mass emission rate is\n\\[ Q = { W \\cdot EF \\over HV}  \\]\nWhere EF is the emission factor and HV the higher heating value.\n\nHV = uconvert(u\"GJ/m^3\", 1020u\"btu/ft^3\")\nEF = uconvert(u\"kg/m^3\", 84*1e-6u\"lb/ft^3\")\n\nQ = EF * W / HV # mass emission rate in kg/s\n\n0.002950437713234783 kg s^-1\n\n\nThis gives a mass flow rate of carbon monoxide in the plume, but we will also need some sense of how large the plume is in general, i.e. what is the volumetric flow rate of stack gas exiting the stack?\n\n\nVolumetric Flow Rate of Flue Gas\nThere are several ways the volumetric flow rate of flue gas could be estimated. One simple method is to use EPA Method 192 with the equation\n2 EPA, “Method 19”.\\[ V_s^o = F_w { 20.9 \\over 20.9 \\left( 1 - B_{wa} \\right) - \\%O_{2w} } \\cdot W\\]\nWhere \\(V_s^o\\) is the volumetric flow of flue gas at standard conditions, \\(B_{wa}\\) the moisture fraction of ambient air, \\(\\%O_{2w}\\) the percentage of oxygen on a wet basis, and the parameter \\(F_w\\) captures the differences in combustion stoichiometry for different fuels and is tabulated. Alternatively one could work out the volume of stack gas from the stoichiometry of combustion, this is just a shortcut.\n\nthe default value for \\(B_{wa} = 0.027\\)\n\\(\\%O_{2w}\\) usually ranges from 2-6% and for this case I am assuming \\(\\%O_{2w} = 4\\)\nfrom Method 19 for natural gas, \\(F_w = 2.85 \\times 10^{-7} \\mathrm{sm^3 \\over J}\\)\n\n\nFw = 2.85e-7u\"m^3/J\"\npct_O2 = 4\nBwa = 0.027\n\nVₛᵒ = Fw * (20.9 / (20.9*(1-Bwa) - pct_O2)) * W\n\nVₛᵒ = upreferred(Vₛᵒ)\n\n30.385903267077627 m^3 s^-1\n\n\nThe actual volumetric flow rate can be calculated assuming the ideal gas law\n\\[ { p^o V_s^o \\over T^o } = { p_a V_s \\over T_s } \\]\n\\[ V_s = { T_s \\over T^o } { p^o \\over p_a } V_s^o \\]\nWhere the standard conditions of Method 19 are \\(T^o = 20 \\mathrm{C}\\) and \\(p^o = 760 \\mathrm{mm Hg}\\)\n\n# Unitful doesn't know what \"mm Hg\" is\n@unit mmHg \"mm Hg\" MillimetersMercury 133.322387415u\"Pa\" false \n\nTᵒ = uconvert(u\"K\", 20u\"°C\")\npᵒ = uconvert(u\"kPa\", 760mmHg)\n\nVₛ = (Tₛ / Tᵒ) * (pᵒ / pₐ) * Vₛᵒ\n\n46.6438970432218 m^3 s^-1\n\n\n\n\nThe Concentration of Interest\nThis analysis is fundamentally about identifying whether a worker on the work platform would experience flue gases in excess of some concentration of interest. In this case I am supposing the Occupational Exposure Limit (OEL) for carbon monoxide alone because it is simple. In practice, since flue gas is a mixture of many substances that each have an associated OEL, one would have to look at the cumulative impact of all of these substances instead of treating them all individually3\n3 For example CCOHS recommends calculating the sum \\[ \\sum_i {C_i \\over T_i } \\] for each substance i where C is the observed concentration and T is the threshold, and this sum should be less than one.4 From the NIOSH Handbook, using the conversion 1.15 mg/m^3 per ppmFor carbon monoxide there are three concentrations of interest worth considering4\n\nthe Time Weighted Average (TWA) concentration which represents the limit for workers in that environment for a standard shift and 40 hours per week\nthe Ceiling concentration which is the level that the concentration cannot exceed\nthe Immediately Dangerous to Life and Health (IDLH) limit which is a concentration that could either kill a worker outright or render them incapable of saving themselves\n\n\n\n\nLimit\nppm\nmg/m^3\n\n\n\n\nTWA\n35\n40\n\n\nCeiling\n200\n229\n\n\nIDLH\n1200\n1380\n\n\n\n\nTWA = uconvert(u\"kg/m^3\", 40u\"mg/m^3\")\nCeil = uconvert(u\"kg/m^3\", 229u\"mg/m^3\")\nIDLH = uconvert(u\"kg/m^3\", 1380u\"mg/m^3\");\n\nWe can check if the stack gas concentration at the exit exceeds the TWA. If it does not exceed the TWA then there is no reason to proceed with the calculations as a worker could work in the stack and not exceed the limits and they certainly would not exceed the limits after the plume mixed with ambient air.\n\nuconvert(u\"mg/m^3\", Q/Vₛᵒ)\n\n97.0988977125952 mg m^-3\n\n\n\nQ/Vₛᵒ &gt; TWA\n\ntrue\n\n\nThe concentration in the flue gas is above the limit for long term work exposure but below the ceiling. At this point we are justified in continuing on to estimate the concentration at the work platform."
  },
  {
    "objectID": "posts/gaussian_dispersion_example/index.html#meteorological-conditions",
    "href": "posts/gaussian_dispersion_example/index.html#meteorological-conditions",
    "title": "Air Dispersion Example - Gaussian Dispersion Model of Stack Emissions",
    "section": "Meteorological Conditions",
    "text": "Meteorological Conditions\nThe ambient conditions impact the release in some obvious ways and in some non-obvious ways. Obviously the wind speed impacts how far the plume is moved, through advection. Somewhat non-obviously the ambient conditions also govern how high the plume will rise due to buoyancy as well as the extent of mixing as the plume moves through the air.\nSuppose a wind speed of 1.5m/s at the stack height, just arbitrarily.\n\nuₛ = 1.5u\"m/s\"\n\n1.5 m s^-1\n\n\n\nAtmospheric Stability\nThe atmospheric stability relates to the vertical mixing of the air due to a temperature gradient, during the day air temperature decreases with elevation and this temperature gradient induces a vertical flow that leads to vertical mixing.\n\n\n\n\n\n\nFigure 2: The effect of atmospheric stability on plume dispersion.\n\n\n\nThis is captured by the atmospheric stability parameter \\(s\\) which is given by5\n5 EPA, “EPA-454/b-95-003b,” 2:1–9.\\[ s = \\frac{g}{T_a} { \\partial \\theta \\over \\partial z } \\]\nWhere \\(\\partial \\theta \\over \\partial z\\) is the lapse rate in K/m\nThe “worst case” is the case with the least mixing and corresponds to a class F Pasquill stability, i.e. very stable, which has a corresponding default lapse rate of \\({ \\partial \\theta \\over \\partial z } = 0.035 K/m\\).6\n6 EPA, 2:1–9.\n\n\n\n\n\nAddendum\n\n\n\nThis isn’t entirely true. For neutrally buoyant plumes released at ground level, or in this case level with the elevated work platform, class F is likely the worst case. For buoyant plumes released at elevation the minimal vertical dispersion with stable atmospheres means the bulk of the plume will rise and be dispersed far above the ground and another class and wind speed should be considered. See Guidelines for Use of Vapour Cloud Dispersion Models, 2nd ed. section 5.8 for more details\n\n\n\n# acceleration due to gravity\ng = 9.80616u\"m/s^2\"\n\n# default lapse rate for class F\nΓ = 0.035u\"K/m\"\n\n# stability parameter\ns = (g/Tₐ) * Γ\n\n0.0011511507630387393 s^-2\n\n\n\n\nEffective Stack Height\nThe plume rising out of the stack will rise higher than the stack height due to buoyancy – in this case because the stack gas is at a higher temperature than the ambient air – and because the stack gas is ejected with some kinetic energy. What follows is essentially a simplified version of the Brigg’s model for plume rise for stable plumes.\nAs a first check, verify that stack down wash will not be relevant. For low momentum releases the effective stack height of the plume is reduced by vortices shed downwind of the stack that pull the plume downwards. This is only really relevant when \\(v_s \\lt 1.5 u\\)\nWhere \\(v_s\\) is the stack exit velocity and is calculated from the volumetric flow as\n\\[ v_s = { V_s \\over A_s} = { V_s \\over \\frac{\\pi}{4} D^2 } \\]\n\nvₛ = Vₛ / ((π/4)*Dₛ^2)\n\nvₛ &gt; 1.5uₛ\n\ntrue\n\n\nThe following assumes a stable plume rise, recall that Pasquill stability class F corresponds to very stable conditions.\nThe first question that must be answered is whether or not the plume rise is dominated by buoyancy or by momentum. For buoyant plume rise to dominate the actual temperature difference – the difference between the stack exit temperature and the ambient temperature – must be greater than a critical temperature difference7\n7 EPA, 2:1–9.\\[ T_s - T_a = \\Delta T \\gt \\left( \\Delta T \\right)_c = 0.019582 T_s v_s \\sqrt{s} \\]\n\nΔTc = 0.019582u\"m^-1*s^2\" * Tₛ * vₛ * √(s)\n\n(Tₛ - Tₐ) &gt; ΔTc\n\ntrue\n\n\nIn this case buoyant plume rise is dominant, and the stable plume rise equation is8\n8 EPA, 2:1–9.\\[ \\Delta h = 2.6 \\left( F_b \\over u_s s \\right)^{1/3} \\]\nwhere \\(\\Delta h\\) is the increase in effective stack height due to plume rise, and \\(F_b\\) is the buoyancy flux parameter9\n9 EPA, 2:1–6.\\[ F_b = g v_s D_s^2 { \\left( T_s - T_a \\right) \\over 4 T_s } \\]\nPlume rise is not instantaneous and the distance to the final rise, \\(x_f\\) is given by10\n10 EPA, 2:1–9.\\[ x_f = 2.0715 {u_s \\over \\sqrt{s} } \\]\nwith any distance closer to the source than \\(x_f\\) experiencing a lesser plume rise, given by11\n11 EPA, 2:1–10.\\[ \\Delta h = 1.60 \\left( F_b x^2 \\over u_s^3 \\right)^{1/3} \\]\nthis can be put together into a function that calculates \\(\\Delta h\\) as a function of distance x\n\nFb = g * vₛ * Dₛ^2 * (Tₛ - Tₐ) / (4Tₛ)\n\n49.1299376393856 m^4 s^-3\n\n\n\nxf = 2.0715*uₛ/√(s)\n\n91.58199372993636 m\n\n\n\nfunction Δh(x)\n    xf = 2.0715*uₛ/√(s)\n    \n    if x &lt; xf\n        return 1.60*(Fb*x^2/uₛ^3)^(1/3)\n    else\n        return 2.6*(Fb/(uₛ*s))^(1/3)\n    end\n    \nend;\n\n\n\n\n\n\n\n\n\nFigure 3: Plume rise as a function of downwind distance.\n\n\n\n\n\nPlume rise is impacted by the wind speed at the stack height, as the following plot shows, but with several large caveats. For one the model for plume rise given is not defined at no wind speed and for very low wind speeds the value should be treated with suspicion. Similarly for very large wind speeds the assumption of stable rise is likely quite invalid.\n\n\n\n\n\n\n\n\nFigure 4: Plume rise as a function of windspeed."
  },
  {
    "objectID": "posts/gaussian_dispersion_example/index.html#gaussian-dispersion-model",
    "href": "posts/gaussian_dispersion_example/index.html#gaussian-dispersion-model",
    "title": "Air Dispersion Example - Gaussian Dispersion Model of Stack Emissions",
    "section": "Gaussian Dispersion Model",
    "text": "Gaussian Dispersion Model\nAs the plume is carried downwind it will mix with the ambient air and the pollutant, carbon monoxide, will be dispersed. A simple model of this is a Gaussian dispersion model, the derivation for which is sketched out as follows.\n\nA Differential Mass Balance\nStarting with a coordinate system centred at the top of the stack, emitting a mass flow of Q kg/s, which is assumed to be released from a point, the advection-diffusion equation for mass can be written as\n\\[ {\\partial C \\over \\partial t} = - \\nabla \\cdot \\mathbf{D} \\cdot \\nabla C + \\nabla \\cdot \\mathbf{u} C \\]\nWhere \\(\\mathbf{D}\\) is the diffusivity, \\(C\\) the concentration of the species, and \\(\\mathbf{u}\\) the wind speed. The diffusivity in this case is a vector and depends upon the direction, i.e. \\(D_x \\ne D_y \\ne D_z\\) and represents an eddy diffusion as opposed to a simple Fickian diffusion.\nSome simplifying assumptions can be made\n\nthe wind speed u is a constant everywhere\nthe air is moving entirely in the x direction, i.e. \\(u_{y} = u_{z} = 0\\) and \\(u_x = u\\) and thus \\(\\nabla \\cdot \\mathbf{u} C = u {\\partial C \\over \\partial x}\\)\nthe diffusivities \\(D_x\\), \\(D_y\\), and \\(D_z\\) are constant everywhere\nadvection is much more significant than diffusion in the x direction i.e. \\(\\left \\vert {\\partial \\over \\partial x} C u \\right \\vert \\gg \\left \\vert {\\partial^{2} \\over \\partial x^{2} } D_{x} C \\right \\vert\\), leading to \\(\\nabla \\cdot \\mathbf{D} \\cdot \\nabla C = D_y {\\partial^2 C \\over \\partial y^2} + D_z {\\partial^2 C \\over \\partial z^2}\\)\nthe system is at steady state, \\({\\partial C \\over \\partial t} = 0\\)\n\nReducing the PDE to\n\\[ u {\\partial C \\over \\partial x} = D_{y} {\\partial^{2} C \\over \\partial y^{2} } + D_{z} {\\partial^{2} C \\over \\partial z^{2} } \\]\nWhich has solutions for particular boundary conditions\n\\[ C = {k \\over x} \\exp \\left[ - \\left( {y^{2} \\over D_{y} } + {z^{2} \\over D_{z} } \\right) { u \\over 4x } \\right] \\]\nWhere k is a constant set by the boundary conditions.\n\n\nBoundary Conditions\nTo solve for k note that Q is assumed to be constant and that mass must be conserved as it is carried downwind which has the effect that for any given x the flux through the y-z plane is Q.\n\\[ Q = \\int \\int C  u  dy  dz \\]\n\\[ Q = \\int_{0}^{\\infty} \\int_{-\\infty}^{\\infty} {k u \\over x} \\exp \\left[ - \\left( {y^{2} \\over D_{y} } + {z^{2} \\over D_{z} } \\right) { u \\over 4x } \\right] dy dz \\]\nwhere the release point is assumed to be at ground level (z=0).\nMaking the change of variables \\(y' = {y \\over \\sqrt{D_{y} } }\\) and \\(z' = {z \\over \\sqrt{D_{z} } }\\) gives\n\\[ Q = {k u \\over x} \\sqrt{D_{y} D_{z} } \\int_{-\\infty}^{\\infty}  \\exp \\left[ - {u \\over 4 x} y'^{2} \\right] dy' \\int_{0}^{\\infty} \\exp \\left[ - {u \\over 4 x} z'^{2} \\right] dz' \\]\nwhich are gaussian integrals that can be integrated\n\\[ Q = {k u \\over x} \\sqrt{D_{y} D_{z} } \\left( \\sqrt{\\pi} \\over \\sqrt{u \\over 4x} \\right) \\left( \\sqrt{\\pi} \\over 2 \\sqrt{u \\over 4x} \\right) \\]\nsimplifying\n\\[ Q = 2 \\pi k \\sqrt{D_{y} D_{z} } \\]\nand solving for k\n\\[ k = {Q \\over 2 \\pi \\sqrt{D_{y} D_{z} } }\\]\n\n\nGaussian Model\nSubstituting k back into the model gives the gaussian dispersion model.\n\\[ C = {Q \\over 2 \\pi x \\sqrt{D_{y} D_{z} } } \\exp \\left[ - \\left( {y^{2} \\over D_{y} } + {z^{2} \\over D_{z} } \\right) { u \\over 4x } \\right] \\]\nHowever this is more commonly expressed in terms of dispersion by letting\n\\[ \\sigma_{y}^{2} = {2 D_{y} x \\over u}\\]\n\\[ \\sigma_{z}^{2} = {2 D_{z} x \\over u}\\]\nwhich gives a more explicitly gaussian distribution of concentration at a given point x\n\\[ C = {Q \\over \\pi u \\sigma_{y} \\sigma_{z} } \\exp \\left[ -\\frac{1}{2} \\left( {y^{2} \\over \\sigma_{y}^{2} } + {z^{2} \\over \\sigma_{z}^{2} } \\right) \\right] \\]\nNote the parameters \\(\\sigma_y\\) and \\(\\sigma_z\\) have units of length.\n\n\nGround Reflection\nWhen solving for k, I assumed the release point was at ground level, this simplified the integration by making one of the bounds of the integral zero.\nHowever what we want is a generalized equation with the emissions released at some elevation h. The plume can disperse downwards but only to a distance h below the release point, at which point the mass can neither disperse further downwards (pass through the ground) nor does it just disappear. This is ground reflection.\n\n\n\n\n\n\nFigure 5: A sketch of ground reflection by method of images.\n\n\n\nOne way to capture this is to integrate z from \\(-\\infty\\) to \\(\\infty\\) (recall that the release point is at the origin) and introduce a mirror image of the stack shifted 2h below. The ground being the x-y plane at z = -h. By symmetry the portion of the mirror plume extending up above this plane is the same as the portion of the plume that, in this simple model, has extended below the ground. By adding the stack and the mirror stack together and shifting the z-coordinate so z = 0 is the ground, ground reflection is captured and the expression for a release point at elevation h is given by\n\\[ C = {Q \\over 2 \\pi u \\sigma_{y} \\sigma_{z} } \\exp \\left[ -\\frac{1}{2} \\left( y \\over \\sigma_{y} \\right)^2 \\right] \\]\n\\[ \\times \\left\\{ \\exp \\left[ -\\frac{1}{2} \\left( { z -h } \\over \\sigma_{z} \\right)^2 \\right] + \\exp \\left[ -\\frac{1}{2} \\left( { z + h } \\over \\sigma_{z} \\right)^2 \\right] \\right\\} \\]"
  },
  {
    "objectID": "posts/gaussian_dispersion_example/index.html#pasquill-gifford-model",
    "href": "posts/gaussian_dispersion_example/index.html#pasquill-gifford-model",
    "title": "Air Dispersion Example - Gaussian Dispersion Model of Stack Emissions",
    "section": "Pasquill-Gifford Model",
    "text": "Pasquill-Gifford Model\nThe \\(\\sigma_{y}\\) and \\(\\sigma_{z}\\) are functions of the downwind distance x. In the derivation of the model they were assumed to be linear in x however in practice they are typically of the form:\n\\[ \\sigma_{y} = a x^{b} \\]\n\\[ \\sigma_{z} = c x^{d} \\]\nWith the constants tabulated based on the Pasquill stability class criteria.\nThese particular correlations come from Lees12 and are for a Pasquill stability class F\n12 Lees, Loss Prevention in the Process Industries, 15/113. There is a typo in the 4th edition of Lees’ for the \\(\\sigma_{z}\\) corresponding to class F stability. For \\(x&gt;500\\) it is given as \\[ \\sigma_{z} = 10^{(1.91 - 1.37 \\log(x) - 0.119 \\log(x)^2)} \\] when it should be (note the signs) \\[ \\sigma_{z} = 10^{(-1.91 + 1.37 \\log(x) - 0.119 \\log(x)^2)} \\]. I happen to have the paper version of the 2nd edition at home, which does not have the typo, whereas the standard version I use at work is the 4th edition on Knovel.\nσy(x) = 0.067*x^0.90\n\nfunction σz(x)\n    if x &lt; 500.0\n        return 0.057*x^0.80\n    else\n        # Note: Lee's gives the commented out form but it is wrong\n        # 10^(1.91 - 1.37*log10(x) - 0.119*log10(x)^2)\n        return 10^(-1.91 + 1.37*log10(x) - 0.119*log10(x)^2)\n    end\nend;\n\nThese correlations are currently not unit-aware, so we can add that using a macro\n\n# this macro adds a method to handle units\nmacro correl(f, in_unit, out_unit)\n    quote\n        function $(esc(f))(x::Quantity)::Quantity\n            x = ustrip($in_unit, x)\n            res = $f(x)\n            return res*$out_unit\n        end\n    end\nend\n\nσy = @correl(σy, u\"m\", u\"m\")\nσz = @correl(σz, u\"m\", u\"m\");\n\n\n\n\n\n\n\n\n\nFigure 6: Pasquill-Gifford dispersion parameters as a function of downwind distance, for class F atmospheric stability.\n\n\n\n\n\n\nEffect of Plume Rise\nThe effect of plume rise on this model is to shift from the actual stack height to an effective stack height \\(h_e = h_s + \\Delta h\\) with \\(\\Delta h\\) given by the plume rise model already discussed. Additionally the dispersion is adjusted by the following13\n13 Vallero, Fundamentals of Air Pollution, 696–97.\\[ \\sigma_{ze}^2 = \\left( \\Delta h \\over 3.5 \\right)^2 + \\sigma_z^2 \\]\n\\[ \\sigma_{ye}^2 = \\left( \\Delta h \\over 3.5 \\right)^2 + \\sigma_y^2 \\]\nand the final model of concentration is given in respect to the effective stack height\n\\[ C = {Q \\over 2 \\pi u \\sigma_{ye} \\sigma_{ze} } \\exp \\left[ -\\frac{1}{2} \\left( y \\over \\sigma_{ye} \\right)^2 \\right] \\]\n\\[ \\times \\left\\{ \\exp \\left[ -\\frac{1}{2} \\left( { z -h_e } \\over \\sigma_{ze} \\right)^2 \\right] + \\exp \\left[ -\\frac{1}{2} \\left( { z + h_e } \\over \\sigma_{ze} \\right)^2 \\right] \\right\\} \\]\n\nfunction C(x, y, z)\n    hₑ  = hₛ + Δh(x)\n    σyₑ = √( (Δh(x)/3.5)^2 + σy(x)^2 )\n    σzₑ = √( (Δh(x)/3.5)^2 + σz(x)^2 )\n    \n    C = (Q/(2*π*uₛ*σyₑ*σzₑ)) *\n         exp(-0.5*(y/σyₑ)^2) *\n         ( exp(-0.5*((z-hₑ)/σzₑ)^2) + exp(-0.5*((z+hₑ)/σzₑ)^2) )\n    \nend\n\nC (generic function with 1 method)"
  },
  {
    "objectID": "posts/gaussian_dispersion_example/index.html#modelling-dispersion",
    "href": "posts/gaussian_dispersion_example/index.html#modelling-dispersion",
    "title": "Air Dispersion Example - Gaussian Dispersion Model of Stack Emissions",
    "section": "Modelling Dispersion",
    "text": "Modelling Dispersion\nThere are two cases worth considering\n\nwithout accounting for plume rise\nwith plume rise\n\nThe first case would be very conservative and the stack plume would immediately point directly downwind, at the stack height, this is far more likely to impact the work platform and any workers on the ground, though it is also quite unrealistic.\nNote the following contour plots max out at the time weighted average concentration, shown in mg/m^3\n\n\n\n\n\n\n\n\nFigure 7: Contour plots of concentration with no plume rise.\n\n\n\n\n\nThis clearly represents something of an extreme case, and I believe illustrates something of interest. While the work platform is ultimately below the TWA, to get even close to that concentration at the work platform the model is assuming extremely little mixing and no plume rise.\nA more realistic model would take into account the buoyant rise of hot stack gases.\n\n\n\n\n\n\n\n\nFigure 8: Contour plots of concentration with using Briggs’ plume rise equations.\n\n\n\n\n\nIn this model the plume clearly rises significantly and, as it goes, mixes into the air column to such an extent that there is hardly any carbon monoxide at the elevations of interest downwind of the stack.\n\nuconvert(u\"mg/m^3\",C(x₁, 0u\"m\", h₁))\n\n0.0014282911474771348 mg m^-3\n\n\n\nC(x₁, 0u\"m\", h₁) &gt; TWA\n\nfalse"
  },
  {
    "objectID": "posts/gaussian_dispersion_example/index.html#concluding-remarks",
    "href": "posts/gaussian_dispersion_example/index.html#concluding-remarks",
    "title": "Air Dispersion Example - Gaussian Dispersion Model of Stack Emissions",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\nThis model assumed a continuous, steady-state, flow of stack gases. Boilers don’t always operate that way and the model did not, for example, consider startup or upset conditions that could lead to higher in-stack concentrations of carbon monoxide.\nThe model also assumed mixing was captured by a simple Gaussian dispersion model. This model does not, for example, account for variability of wind speed either with time or spatially – wind speed typically increases with height – in this case I believe the model underestimates the degree of mixing. Nor does it account for interactions with buildings and potential down wash, which can be very significant.\nThis also assumes no other sources of carbon monoxide, both at the facility surrounding the worksite but also potentially from some portable equipment.\nI think that, while modelling like this might be informative about the potential hazards, it is always good practise to develop a monitoring plan for the work area that includes the flue gases and any other potential substances to ensure workers on the scaffolding are not being exposed."
  },
  {
    "objectID": "posts/gaussian_dispersion_example/index.html#references",
    "href": "posts/gaussian_dispersion_example/index.html#references",
    "title": "Air Dispersion Example - Gaussian Dispersion Model of Stack Emissions",
    "section": "References",
    "text": "References\n\n\nAIChE/CCPS. Guidelines for Use of Vapour Cloud Dispersion Models, 2nd Ed. New York: American Institute of Chemical Engineers, 1996.\n\n\nEPA. “AP 42: Compilation of Air Emissions Factors.” 5th ed. Research Triangle Park, NC: Environmental Protection Agency, 1995. https://www.epa.gov/air-emissions-factors-and-quantification/ap-42-compilation-air-emissions-factors.\n\n\n———. “EPA-454/b-95-003b: User’s Guide for the ISC3 Dispersion Models.” Vol. 2. Environmental Protection Agency, 1995.\n\n\n———. “Method 19: Determination of Sulfur Dioxide Removal Efficiency and Particulate Matter, Sulfur Dioxide, and Nitrogen Oxide Emission Rates.” Environmental Protection Agency, 2017. https://www.epa.gov/sites/default/files/2017-08/documents/method_19.pdf.\n\n\nLees, Frank P. Loss Prevention in the Process Industries. 2nd ed. Oxford: Butterworth-Heinemann, 1996.\n\n\nVallero, Daniel. Fundamentals of Air Pollution. 5th ed. Amsterdam: Elsevier, 2014."
  },
  {
    "objectID": "posts/plastics-recycling-microplastics/index.html",
    "href": "posts/plastics-recycling-microplastics/index.html",
    "title": "Plastics Recycling and Microplastics",
    "section": "",
    "text": "As perhaps just a hazard of my profession, any time an article comes out on the merits (or lack of) of recycling and plastic waste in general, people send it my way. Several times in the last month I was sent this article in Quillette.1 (and associated YouTube video) about how plastics recycling may be a massive source of the microplastics being discharged into the environment, adding to the long list of reasons why recycling has not lived up to the promises made by industry, and undermining our path towards a more circular economy. At first glance though, some of the numbers presented and the math struck me as rather sus, so I would like to take a moment to dive into it a bit more. tl;dr much the math in that essay doesn’t really work or comes with big caveats, but the broader point about the value of recycling and how we may not be fully appreciating the environmental impacts may hold up."
  },
  {
    "objectID": "posts/plastics-recycling-microplastics/index.html#how-large-of-a-source-of-microplastics-is-the-recycling-industry",
    "href": "posts/plastics-recycling-microplastics/index.html#how-large-of-a-source-of-microplastics-is-the-recycling-industry",
    "title": "Plastics Recycling and Microplastics",
    "section": "How Large of a Source of Microplastics is the Recycling Industry?",
    "text": "How Large of a Source of Microplastics is the Recycling Industry?\nCelia estimates that up to 2/3rds of the microplastics discharged directly into the environment2 come from the recycling industry. This is a huge number. One that should immediately raise eye-brows. So lets break that down, it comes from two numbers:\n2 These are so called “primary” microplastics, as opposed to “secondary” microplastics which are generated from plastic waste already in the environment\nthat the recycling industry discharges up to 2Mt/y of microplastics into the environment\nthat the total amount of primary microplastics discharged into the environment from all sources is 3Mt/y\n\n\nThe Direct Discharge of Microplastics\nCelia takes the value of about 2Mt/y of microplastics emissions from the recycling industry from an interview given by an author of a recent study,3 and leaves it rather mysterious as to where exactly it comes from. However, this is a really easy number to calculate yourself: Approximately 9% of total plastic waste, globally, is recycled, that study estimated that up to 6 - 13% of recycled plastic could be lost to the environment as primary microplastics, which equates to about 2 - 4Mt/y.\n3 Brown et al., “The Potential for a Plastic Recycling Facility to Release Microplastic Pollution and Possible Filtration Remediation Effectiveness”.4 OECD, “Global Plastics Outlook,” 20.For example, the OECD estimated that, in 2019, global plastic waste generation was 353Mt of which 33Mt were recycled (~9%)4 6 - 13% of that is 1.98 - 4.29 Mt. So in some sense, taking the high end of that, makes the argument more dramatic.\nThe main reference around which the entire essay revolves is that one study of a single plastics recycling plant in the UK. In that study, the authors looked at a relatively new plastics recycling facility that underwent an upgrade to its wastewater treatment process, adding additional filtration. The study looked at the microplastics emissions prior to and after the upgrade. Based on the concentrations measured in the wastewater they estimated that up to 13% of the mass of plastic brought into the facility may have been discharged in the wastewater as microplastics prior to the filtration upgrade and, after the upgrade, this dropped to 6%. These two numbers 6% and 13% form the basis for the estimate of how much primary microplastics are being discharged from the recycling industry as a whole.\nAt this point we should pause consider the error bars on those numbers. The study gives a range for the total annual mass discharge in the wastewater, based on measured mass concentrations in the facility wastewater. The ratio of this mass out to the plastic taken in is the origin of the 6 - 13% range. However, I think it is deeply disingenuous to present these numbers without context as the study’s estimates span three orders of magnitude.\n\n\n\nestimate\nlow end (t/y)\nhigh end (t/y)\n\n\n\n\nbefore filter upgrade\n96\n2933\n\n\nafter filter upgrade\n4\n1366\n\n\n\nI think the take away from this is that far more data is needed to narrow these error bars. The low end estimates are still much larger than other studies for the whole life-cycle of plastic5 and the high end estimates are many orders of magnitude larger still. This study is only a single data point, but it is showing that the estimates used in other life cycle analysis may be far too small and that recycling is a much larger contributor to primary microplastics than has been accounted for.\n5 Ryberg, Laurent, and Hauschild, “Mapping of Global Plastics Value Chain and Plastics Losses to the Environment” page 56, estimates 0.005% loss;\nBoucher and Friot, “Primary Microplastics in the Oceans” page 37, estimates 0.00033 - 0.001% loss;\nThe low end post-upgrade estimate from Brown et al., “The Potential for a Plastic Recycling Facility to Release Microplastic Pollution and Possible Filtration Remediation Effectiveness” is 0.018%\n\nThe Total Amount of Primary Microplastics losses\nI think the ~3Mt/y is a relatively robust estimate, for the type of study Celia references, because it has been replicated6. However, this is the source of the most egregious and obvious mistake, and the one that prompted me to write this blog post in the first place. The studies referenced as the sources for the 3Mt/y number include recycling as a source in the estimate but do not estimate the losses from recycling to be anywhere near as high. Dividing these two numbers is simply a mathematically invalid operation.\n6 Ryberg, Laurent, and Hauschild, “Mapping of Global Plastics Value Chain and Plastics Losses to the Environment” estimates 3.1Mt/y;\nOECD, “Global Plastics Outlook” estimates 2.7Mt/y;\nBoucher and Friot, “Primary Microplastics in the Oceans” estimates 1.8 - 5Mt/yBefore I go any further, where do numbers like 3Mt/y come from? They are not from direct measurements of microplastics in the environment. They come from a life-cycle analysis that looks at the entire life of plastics and estimates rates of losses at the various steps along the path from the creation of virgin plastic to its ultimate fate. Adding all of these losses up gives the total estimated primary microplastics loss. This is why it is incorrect to simply ratio 2Mt/y over 3Mt/y: that would only work if the 2Mt/y was included in the total, and it isn’t.\nSupposing that we are going with 2Mt/y of primary microplastics from recycling, most studies (importantly the ones referenced by Celia) do not use a number anywhere near this high. In fact most assume it quite small and some take it to be negligible.7 The correct procedure would be to subtract the previous estimate for losses due to recycling from the total losses, and then add the new estimate of 2Mt/y, giving a corrected total. This would then be the denominator.\n7 Ryberg, Laurent, and Hauschild, “Mapping of Global Plastics Value Chain and Plastics Losses to the Environment,” 56.8 Ryberg, Laurent, and Hauschild, 54.Consider a UNEP study that estimates the total primary microplastics losses from the entire plastics value chain as 3.1Mt/y.8 Conveniently, this study assumed the losses due to recycling were negligible (i.e. zero). So based on this study’s estimates for all other sources of primary microplastics, and our estimate of 2Mt/y from the recycling industry alone, we would estimate a new total of 5.1Mt/y, of which 2/5.1 = 39% came from the recycling industry. So.. not 2/3rds.\nBut considering how wide the error bars are for the estimate of primary microplastics emissions, from that one plastics recycling facility, all we can really say is that recycling is somewhere between a small, but important, source of primary microplastics and the single largest source of primary microplastics. Which is important in the sense that it identifies that we may be missing a major source of primary microplastics, but it really does not live up to the hype in Celia’s article."
  },
  {
    "objectID": "posts/plastics-recycling-microplastics/index.html#the-climate-impacts-of-landfilling",
    "href": "posts/plastics-recycling-microplastics/index.html#the-climate-impacts-of-landfilling",
    "title": "Plastics Recycling and Microplastics",
    "section": "The Climate Impacts of Landfilling",
    "text": "The Climate Impacts of Landfilling\nCelia makes reference to the landfilling of municipal solid waste being a source of methane emissions as part of the argument for why recycling should be abandoned and plastic incinerated instead. Independent of the merits of recycling or incinerating, this is at best irrelevant. Plastic has a negligible methane generating potential when landfilled, a fact that is related to the primary concern with plastic waste in the environment: its environmental persistence. The methane emissions coming from the landfilling of municipal waste is from decomposing organic matter, not the plastic. In fact a recent meta-analysis9 shows that, if anything, the presence of non-biodegradable plastic reduces the methane emissions from anaerobic digestion as non-biodegradable plastics may leach toxins that prevent bacteria from decomposing organic matter. I wouldn’t take that to mean we should be landfilling plastic waste, as some climate mitigation strategy, merely that the methane emissions from doing so are irrelevant to the argument around what to do with plastic waste.\n9 Gao et al., “Comprehensive Meta-Analysis Reveals the Impact of Non-Biodegradable Plastic Pollution on Methane Production in Anaerobic Digestion”."
  },
  {
    "objectID": "posts/plastics-recycling-microplastics/index.html#conclusions-and-take-aways",
    "href": "posts/plastics-recycling-microplastics/index.html#conclusions-and-take-aways",
    "title": "Plastics Recycling and Microplastics",
    "section": "Conclusions and Take Aways",
    "text": "Conclusions and Take Aways\nThere is certainly a growing chorus of concern over the fate of plastics in the environment, and the environmental and health consequences of microplastics given their ubiquity. That alone should warrant a lot more study into the sources of microplastics. That the estimate that recycling accounts for 2/3rds of primary microplastics doesn’t hold up, due to rudimentary math mistakes, doesn’t invalidate the broader concern that recycling simply has not lived up to the promise and may in fact be worsening the microplastics problem. We don’t know that is the case, given the data cited, but I think the onus is on the recycling industry to show that they are, in fact, part of the solution and not making the problem worse.\nI am not going to comment on the relative merits of incineration, recycling, or advanced recycling other than to say few of the technical problems in this field are truly insurmountable. The real question always comes down to cost and how much we are willing to pay to achieve the environmental performance we want."
  },
  {
    "objectID": "posts/plastics-recycling-microplastics/index.html#references",
    "href": "posts/plastics-recycling-microplastics/index.html#references",
    "title": "Plastics Recycling and Microplastics",
    "section": "References",
    "text": "References\n\n\nBoucher, Julien, and Damien Friot. “Primary Microplastics in the Oceans.” Gland, CH: International Union for Conservation of Nature, 2017. https://doi.org/10.2305/IUCN.CH.2017.01.en.\n\n\nBrown, Erina, Anna MacDonald, Steve Allen, and Deonie Allen. “The Potential for a Plastic Recycling Facility to Release Microplastic Pollution and Possible Filtration Remediation Effectiveness.” Journal of Hazardous Materials Advances 10 (2023): 100309. https://doi.org/10.1016/j.hazadv.2023.100309.\n\n\nCelia, Frank. “Recycling Plastic Is a Dangerous Waste of Time.” Quillette, June 17, 2024. https://quillette.com/2024/06/17/recycling-plastic-is-a-dangerous-waste-of-time-microplastics-health/.\n\n\nGao, Zhenghui, Hang Qian, Tianyi Cui, Zongqiang Ren, and Xingjie Wang. “Comprehensive Meta-Analysis Reveals the Impact of Non-Biodegradable Plastic Pollution on Methane Production in Anaerobic Digestion.” Chemical Engineering Journal 484 (2024): 149703. https://doi.org/10.1016/j.cej.2024.149703.\n\n\nOECD. “Global Plastics Outlook.” Paris: OECD Publishing, 2022. https://doi.org/10.1787/de747aef-en.\n\n\nRyberg, Morten W., Alexis Laurent, and Michael Hauschild. “Mapping of Global Plastics Value Chain and Plastics Losses to the Environment.” Nairobi: United Nations Environment Programme, 2018. https://www.unep.org/resources/report/mapping-global-plastics-value-chain-and-plastics-losses-environment-particular/."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "GasDispersion.jl\n\n\nA julia package for dispersion modeling of chemical releases\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnitfulCorrelations.jl\n\n\nA julia module for using correlations with Unitful\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]